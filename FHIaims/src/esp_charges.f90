!****h* FHI-aims/esp_charges
!  NAME
!    esp_cahrges
!  SYNOPSIS

module esp_charges

!  PURPOSE
!  This module contains all subroutines to calculate esp charges. The charges 
!  are located at the atomic positions and are fitted to the potential 
!  generated by the density \phi_{ik}\phi_{jk}(r) or the total density.
!  AUTHOR
!  HISTORY
!    Development version, FHI-aims (2013).
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Develoment version, FHI-aims (2013).
!  SOURCE
  implicit none

  private
  logical, dimension(:), allocatable, public   :: flag_density_full 
  integer, dimension(:,:), allocatable, public :: esp_state
  integer, dimension(:), allocatable, public :: esp_spin
  integer, dimension(:), allocatable, public :: esp_kpoint
  real*8, dimension(:,:), allocatable, public  :: esp_vdw_radius
  integer, dimension(:), allocatable, public  :: esp_n_radius
  integer, dimension(:), allocatable, public  :: esp_km
  integer, dimension(:), allocatable, public  :: esp_rm
  real*8, dimension(:), allocatable, public  :: esp_R_c
  integer, dimension(:), allocatable, public  :: esp_pbc_method
  integer, dimension(:), allocatable, public  :: esp_grid
  real*8, dimension(:,:), allocatable, public  :: esp_atom_constraint
  integer, dimension(:,:), allocatable, public  :: esp_equal_grid_n  
  integer, dimension(:), allocatable, public  :: esp_output_cube
  logical, dimension(:), allocatable, public  :: esp_output_pot
  logical, dimension(:), allocatable, public  :: esp_use_dip_for_cube

  public :: get_transition_density_densmat
  public :: get_vxc_esp
  public :: multipole_expantion_rho
  public :: sum_up_potential
  public :: esp_fit
  public :: esp_radius
  public :: read_esp_parameters
  public :: allocate_esp_out
  public :: deallocate_esp_out
  public :: esp_state_minmax
  public :: output_esp_charges
  public :: get_transition_density
  public :: esp_fit_pbc
  public :: esp_fit_pbc_two
  public :: esp_fit_pbc_three
  public :: esp_output_cubefile
  public :: esp_collect_pot
  public :: esp_collect_pot_hu
  private :: evaluate_densmat_esp
  private :: evaluate_k_densmat_esp
  private :: accumulate_k_densmat_esp
  private :: evaluate_KS_density_densmat_esp
  private :: evaluate_KS_trafo_densmat_esp
  private :: evaluate_KS_trafo_densmat

  contains


!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/get_transition_density_densma
!  NAME
!    get_transition_density_densmat
!  SYNOPSIS
subroutine get_transition_density_densmat &
     ( KS_eigenvector, KS_eigenvector_complex, occ_numbers, partition_tab_std, &
       hartree_partition_tab_std,  basis_l_max, delta_rho_KS_std,  &
       density_matrix_sparse,density_matrix_sparse_two,state_one,state_two,&
       k_point_one,density_full )

!  PURPOSE
!
!  The subroutine obtains the transistion density using the density
!  matrix for the calculation of esp charges (see evaluate_KS_density_densmat).
!
!
!  USES

  use dimensions,only:n_basis_fns,n_species,n_full_points,n_k_points,&
      n_k_points_task,n_spin,n_states,n_basis,n_my_batches,&
      n_centers_basis_T,l_wave_max,n_periodic,n_centers_basis_integrals,&
      n_centers,n_centers_integrals,n_max_compute_atoms,n_max_compute_fns_dens,&
      n_max_compute_dens,n_max_batch_size
  use runtime_choices,only:spin_treatment,prune_basis_once,PM_none,&
      packed_matrix_format
  use grids,only:batch_of_points,batches
  use basis,only:basis_wave_ordered
  use mpi_tasks,only:mpi_wtime,mpi_comm_global,check_allocation
  use pbc_lists,only:inv_centers_basis_integrals,centers_basis_integrals
  use localorb_io,only:localorb_info
  use density_matrix_evaluation,only:evaluate_densmat
  use load_balancing,only:use_batch_permutation,batch_perm,&
      permute_point_array,permute_point_array_back
  implicit none

!  ARGUMENTS

  real*8,     dimension(n_basis, n_states, n_spin,n_k_points_task):: &
              KS_eigenvector
  complex*16, dimension(n_basis, n_states, n_spin,n_k_points_task):: &
              KS_eigenvector_complex
  real*8,     dimension(n_states, n_spin, n_k_points)             :: occ_numbers

  real*8, target, dimension(n_full_points)      :: partition_tab_std
  real*8, target, dimension(n_full_points)      :: hartree_partition_tab_std
  integer, dimension(n_species)                 :: basis_l_max
  real*8, target, intent(OUT) :: delta_rho_KS_std( n_spin,n_full_points)
  ! when this routine is called, density_matrix_sparse has either the dimension
  ! (n_hamiltonian_matrix_size) or (n_local_matrix_size)
  ! so we declare it here as a 1D assumed size array
  real*8,  dimension(*)                          :: density_matrix_sparse
  real*8,  dimension(*)                          :: density_matrix_sparse_two
  integer :: state_one,state_two,k_point_one
  logical :: density_full
! INPUTS
! o KS_eigenvector -- eigenvectors if real eigenvectors are in use
! o KS_eigenvector_complex -- eigenvectors is complex eigenvectors are in use
! o occ_numbers -- occupation numbers of states
! o partition_tab -- partition function values
! o hartree_partition_tab -- hartree potentials partition function values
! o basis_l_max -- maximum basis l value of the basis functions
! o density_matrix_sparse -- this is the works space for density matrix, it is 
!                            used if packed matrixes are in use.
! o density_matrix_sparse_two -- this is the works space for density matrix, 
!                                it is used if packed matrixes are in use. 
!                                upper/lower trinangle
! o state_one -- first state for transition density
! o state_two -- second state for transition density
! o k_point_one  -- k_point for transition density
! o density_full -- switch to select transition or full density
!
! OUTPUT
!   o delta_rho_KS -- calculated charge density (no delta)
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE





  real*8, dimension(:),     allocatable:: dist_tab
  real*8, dimension(:,:),   allocatable:: dist_tab_sq
  real*8, dimension(:),     allocatable:: i_r
  real*8, dimension(:,:,:), allocatable:: dir_tab
  real*8, dimension(:,:),   allocatable:: trigonom_tab

  real*8,dimension(:),  allocatable:: radial_wave
  real*8,dimension(:,:),allocatable:: wave

  integer,dimension(:),allocatable :: i_basis
  integer :: n_compute_c

  real*8 coord_current(3)

  integer :: n_compute_fns
  integer :: i_basis_fns(n_basis_fns*n_centers_integrals)
  integer :: i_basis_fns_inv(n_basis_fns,n_centers)
  integer :: i_atom_fns(n_basis_fns*n_centers_integrals)

  integer :: n_compute_atoms
  integer :: atom_index(n_centers_integrals)
  integer :: atom_index_inv(n_centers)

  integer :: spline_array_start(n_centers_integrals)
  integer :: spline_array_end(n_centers_integrals)

! VB - renewed index infrastructure starts here

  real*8 :: one_over_dist_tab(n_max_compute_atoms)

      ! indices for basis functions that are nonzero at current point

      integer :: rad_index(n_max_compute_atoms)
      integer :: wave_index(n_max_compute_fns_dens)
      integer :: l_index(n_max_compute_fns_dens)
      integer :: l_count(n_max_compute_fns_dens)
      integer :: fn_atom(n_max_compute_fns_dens)

      ! indices for known zero basis functions at current point
      integer :: n_zero_compute
      integer :: zero_index_point(n_max_compute_dens)

      ! active atoms in current batch
      integer :: n_batch_centers
      integer :: batch_center(n_centers_integrals)

  !     other local variables

  integer :: l_ylm_max
  integer :: n_points

  ! (n_basis, n_states, n_spin) on purpose
  ! beware: inside the subroutines (evaluate_KS_density_v1 for instance)
  ! dimension is (n_compute, max_occ_number)
  ! that's a trick to get a continuous data flow and not
  ! bad programming

  real*8 :: temp_rho(n_max_batch_size,n_spin)
  real*8,dimension(:,:),allocatable :: temp_rho_small

  integer, dimension(:,:), allocatable :: index_lm
  real*8,  dimension(:,:), allocatable :: ylm_tab

  real*8, dimension(:,:),  allocatable :: density_matrix
  real*8, dimension(:,:),  allocatable :: density_matrix_two
  real*8, dimension(:),  allocatable :: density_matrix_con
  real*8, dimension(:),  allocatable :: density_matrix_con_two
  real*8, dimension(:,:),  allocatable :: work
  complex*16, dimension(:,:),allocatable :: work_complex

  !     counters
  integer :: i_l
  integer :: i_m
  integer :: i_point

  integer :: i_full_points
  integer :: i_full_points_2
  integer :: i_full_points_3

  integer :: i_spin
  integer :: i_index
  integer :: i_my_batch
  integer :: info

  ! Load balancing stuff

  integer n_my_batches_work ! Number of batches actually used
  type (batch_of_points), pointer :: batches_work(:) ! Pointer to batches 
                                                     ! actually used

  ! Pointers to the actually used array
  real*8, pointer :: partition_tab(:)
  real*8, pointer :: hartree_partition_tab(:)
  real*8, pointer :: delta_rho_KS(:,:)

  integer i, j, i_off, n_bp
  integer, allocatable :: ins_idx(:)

  ! Timings for analyzing work imbalance
  real*8 time0, time_work, time_all



  !---------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing)
  ! or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    batches_work => batch_perm(n_bp)%batches
    partition_tab => batch_perm(n_bp)%partition_tab

    allocate(hartree_partition_tab(batch_perm(n_bp)%n_full_points))
    call permute_point_array(n_bp,1,hartree_partition_tab_std,&
         hartree_partition_tab)

    allocate(delta_rho_KS(batch_perm(n_bp)%n_full_points,n_spin))

    allocate(ins_idx(batch_perm(n_bp)%n_basis_local))

  else

    n_my_batches_work = n_my_batches
    batches_work => batches
    partition_tab => partition_tab_std
    hartree_partition_tab => hartree_partition_tab_std
    delta_rho_KS => delta_rho_KS_std

  endif

  !---------------------------------------------------------------------------

  ! initialize charge density convergence criterion

  delta_rho_KS = 0.d0



  if(packed_matrix_format == PM_none)then
     if(.not. allocated(density_matrix))then
        allocate(density_matrix(n_centers_basis_T, n_centers_basis_T),&
        stat=info)
        call check_allocation(info, 'density_matrix                ')
     end if
     if(.not.density_full.and.state_one.ne.state_two)then
       if(.not. allocated(density_matrix_two))then
        allocate(density_matrix_two(n_centers_basis_T, n_centers_basis_T),&
        stat=info)
        call check_allocation(info, 'density_matrix_two             ')
       end if
     else
       allocate(density_matrix_two(1,1))
     endif
  else
     ! Allocate dummy since otherways compiling with checking doesn't work
     allocate(density_matrix(1,1))
     allocate(density_matrix_two(1,1))
  end if

  ! --- start actual calculation

  time_work = 0
  time_all  = 0

  do i_spin = 1, n_spin

     ! --- density matrix construction/administration
     if(density_full)then
       call evaluate_densmat(KS_eigenvector,KS_eigenvector_complex,occ_numbers,&
            density_matrix, density_matrix_sparse, i_spin, .false.)
     else
       call evaluate_densmat_esp(KS_eigenvector, KS_eigenvector_complex, &
            occ_numbers,density_matrix, density_matrix_sparse, i_spin, &
            state_one, state_two,k_point_one, .false.)
       if(state_one.ne.state_two)then
         call evaluate_densmat_esp(KS_eigenvector, KS_eigenvector_complex, &
            occ_numbers,density_matrix_two, density_matrix_sparse_two, i_spin, &
            state_two, state_one,k_point_one, .false.)
       endif
     end if


     ! ---------------- end construct density matrix -------------------------

     if(.not. allocated(dist_tab))then
        allocate(dist_tab(n_centers_integrals),stat=info)
        call check_allocation(info, 'dist_tab                      ')
     end if
     if(.not. allocated(dist_tab_sq))  then
        allocate(dist_tab_sq(n_centers_integrals, n_max_batch_size),stat=info)
        call check_allocation(info, 'dist_tab_sq                   ')
     end if
     if(.not. allocated(i_r))then
        allocate(i_r(n_max_compute_atoms),stat=info)
        call check_allocation(info, 'i_r                           ')
     end if
     if(.not. allocated(dir_tab)) then
        allocate(dir_tab(3, n_centers_integrals, n_max_batch_size),stat=info)
        call check_allocation(info, 'dir_tab                       ')
     end if
     if(.not. allocated(trigonom_tab)) then
        allocate(trigonom_tab(4, n_max_compute_atoms),stat=info)
        call check_allocation(info, 'trigonom_tab                  ')
     end if
     if(.not. allocated(density_matrix_con))then
        allocate(density_matrix_con(n_max_compute_dens*n_max_compute_dens),&
                 stat=info)
        call check_allocation(info, 'density_matrix_con            ')
     end if
     if(.not.density_full.and.state_one.ne.state_two)then
       if(.not. allocated(density_matrix_con_two))then
        allocate(density_matrix_con_two(n_max_compute_dens*n_max_compute_dens),&
                 stat=info)
        call check_allocation(info, 'density_matrix_con_two            ')
       end if
     endif
     if(.not. allocated(work))then
        allocate(work(n_max_compute_dens, n_max_batch_size),stat=info)
        call check_allocation(info, 'work                          ')
     end if
     if(.not. allocated(i_basis))then
        allocate(i_basis(n_centers_basis_T),stat=info)
        call check_allocation(info, 'i_basis                       ')
     end if
     if(.not.allocated(radial_wave))then
        allocate(radial_wave(n_max_compute_fns_dens),stat=info)
        call check_allocation(info, 'radial_wave                   ')
     end if
     if(.not. allocated(wave))then
        allocate(wave(n_max_compute_dens, n_max_batch_size),stat=info)
        call check_allocation(info, 'wave                          ')
     end if

     l_ylm_max = l_wave_max

     if(.not. allocated( ylm_tab))then
        allocate( ylm_tab( (l_ylm_max+1)**2,n_max_compute_atoms),stat=info )
        call check_allocation(info, 'ylm_tab                       ')
     end if
     if(.not. allocated( index_lm))then
        allocate( index_lm( -l_ylm_max:l_ylm_max, 0:l_ylm_max),stat=info)
        call check_allocation(info, 'index_lm                      ')
      end if

     ! initialize index_lm
     i_index = 0
     do i_l = 0, l_ylm_max, 1
        do i_m = -i_l, i_l
           i_index = i_index + 1
           index_lm(i_m, i_l) = i_index
        enddo
     enddo

     call mpi_barrier(mpi_comm_global,info) ! Barrier is for correct timing!!!
     time0 = mpi_wtime()

     ! --------go over the grids and construc the density using density matrix

     i_basis_fns_inv = 0

     i_full_points = 0
     i_full_points_2 = 0
     i_full_points_3 = 0
     do i_my_batch = 1, n_my_batches_work, 1

           n_compute_c = 0
           i_basis = 0

           i_point = 0

           ! loop over one batch
           do i_index = 1, batches_work(i_my_batch)%size, 1

              i_full_points = i_full_points + 1

              if (max(partition_tab(i_full_points),&
                   hartree_partition_tab(i_full_points)).gt.0.d0) then

                 i_point = i_point+1

                 !     get current integration point coordinate
                 coord_current(:) = batches_work(i_my_batch) % points(i_index) &
                                    % coords(:)

                 if(n_periodic>0)then
                    call map_to_center_cell( coord_current)
                 end if

                 ! compute atom-centered coordinates of current integration 
                 ! point, as viewed from all atoms
                 call tab_atom_centered_coords_p0 &
                      ( coord_current,  &
                      dist_tab_sq(1,i_point),  &
                      dir_tab(1,1,i_point), &
                      n_centers_integrals, centers_basis_integrals )



                 !     determine which basis functions are relevant at current 
                 !     integration point,and tabulate their indices

                 !next, determine which basis functions u(r)/r*Y_lm(theta,phi)
                 ! are actually needed
                 if (.not.prune_basis_once) then
                   call prune_basis_p2 &
                   ( dist_tab_sq(1,i_point), &
                     n_compute_c, i_basis,  &
                     n_centers_basis_T, n_centers_basis_integrals, &
                     inv_centers_basis_integrals  )
                 end if
              end if
           enddo ! end loop over the angular shell

           if (prune_basis_once) then
              n_compute_c = batches_work(i_my_batch)%batch_n_compute
              i_basis(1:n_compute_c) = batches_work(i_my_batch)%batch_i_basis
           end if

           if(packed_matrix_format /= PM_none )then
              if(use_batch_permutation > 0) then
                do i=1,n_compute_c
                  ins_idx(i) = batch_perm(n_bp)%i_basis_glb_to_loc(i_basis(i))
                enddo
                density_matrix_con(1:n_compute_c*n_compute_c) = 0
                if(.not.density_full.and.state_one.ne.state_two)then
                  density_matrix_con_two(1:n_compute_c*n_compute_c) = 0
                endif
                do i=1,n_compute_c
                  i_off = (ins_idx(i)*(ins_idx(i)-1))/2
                  do j=1,i
                    density_matrix_con(j+(i-1)*n_compute_c) = &
                    density_matrix_sparse(ins_idx(j)+i_off)
                    if(.not.density_full.and.state_one.ne.state_two)then
                      density_matrix_con_two(j+(i-1)*n_compute_c) = &
                      density_matrix_sparse_two(ins_idx(j)+i_off)
                    endif
                  enddo
                enddo
              else
                call  prune_density_matrix_sparse(density_matrix_sparse, &
                      density_matrix_con, n_compute_c, i_basis)
                if(.not.density_full.and.state_one.ne.state_two)then
                  call  prune_density_matrix_sparse(density_matrix_sparse_two, &
                      density_matrix_con_two, n_compute_c, i_basis)
                endif
              endif
           else
              call  prune_density_matrix(density_matrix, density_matrix_con, &
                   n_compute_c, i_basis)
              if(.not.density_full.and.state_one.ne.state_two)then
                call  prune_density_matrix(density_matrix_two, density_matrix_con_two, &
                   n_compute_c, i_basis)
              endif
           end if


           ! from list of n_compute active basis functions in batch, collect 
           ! all atoms that are ever needed in batch.
           call collect_batch_centers_p2 &
           ( n_compute_c, i_basis, n_centers_basis_T, &
             n_centers_basis_integrals, inv_centers_basis_integrals, &
             n_batch_centers, batch_center &
           )

           n_points = i_point

           if (n_compute_c.gt.0) then

              ! Determine all radial functions, ylm functions and their 
              ! derivatives thatare best evaluated strictly locally at each 
              ! individual grid point.
              i_point = 0
              do i_index = 1, batches_work(i_my_batch)%size, 1

                 i_full_points_2 = i_full_points_2 + 1

                 if (max(partition_tab(i_full_points_2),&
                      hartree_partition_tab(i_full_points_2)).gt.0.d0) then

                    i_point = i_point+1
                    n_compute_atoms = 0
                    n_compute_fns = 0


                    ! All radial functions (i.e. u(r), u''(r)+l(l+2)/r^2, 
                    ! u'(r) if needed) Are stored in a compact spline array 
                    ! that can be accessed by spline_vector_waves, without any 
                    !copying and without doing any unnecessary operations.
                    ! The price is that the interface is no longer explicit in 
                    ! terms of physical objects. See shrink_fixed_basis() for 
                    ! details regarding the reorganized spline arrays.


                    call prune_radial_basis_p2 &
                    ( n_max_compute_atoms, n_max_compute_fns_dens, &
                       dist_tab_sq(1,i_point), dist_tab, dir_tab(1,1,i_point),&
                       n_compute_atoms, atom_index, atom_index_inv, &
                       n_compute_fns, i_basis_fns, i_basis_fns_inv, &
                       i_atom_fns, spline_array_start, spline_array_end, &
                       n_centers_basis_integrals, centers_basis_integrals, &
                       n_compute_c, i_basis, &
                       n_batch_centers, batch_center, &
                       one_over_dist_tab, rad_index, wave_index, l_index, &
                       l_count, &
                       fn_atom, n_zero_compute, zero_index_point &
                    )

                    ! Tabulate distances, unit vectors, and inverse logarithmic 
                    ! grid units for all atoms which are actually relevant
                    call tab_local_geometry_p2 &
                      ( n_compute_atoms, atom_index, &
                        dist_tab,  &
                        i_r &
                      )

                    ! Determine all needed radial functions from efficient 
                    ! splines

                    ! Now evaluate radial functions u(r) from the previously 
                    ! stored compressed spline arrays
                    call evaluate_radial_functions_p0 &
                         (   spline_array_start, spline_array_end, &
                         n_compute_atoms, n_compute_fns,  &
                         dist_tab, i_r(1), &
                         atom_index, i_basis_fns_inv, &
                         basis_wave_ordered, radial_wave(1), &
                         .false., n_compute_c, n_max_compute_fns_dens   &
                         )

                    ! compute trigonometric functions of spherical coordinate
                    ! angles of current integration point, viewed from all atoms

                       call tab_trigonom_p0 &
                            ( n_compute_atoms, dir_tab(1,1,i_point),  &
                            trigonom_tab(1,1) &
                            )

                         ! tabulate distance and Ylm's w.r.t. other atoms
                       call tab_wave_ylm_p0 &
                            ( n_compute_atoms, atom_index,  &
                            trigonom_tab(1,1), basis_l_max,  &
                            l_ylm_max, &
                            ylm_tab(1,1) )



                    ! tabulate total wave function value for each basis function 
                    ! in all cases -but only now are we sure that we have 
                    ! ylm_tab ...

                    ! tabulate total wave function value for each basis function
                    call evaluate_waves_p2  &
                    ( n_compute_c, n_compute_atoms, n_compute_fns, &
                      l_ylm_max, ylm_tab, one_over_dist_tab,   &
                      radial_wave, wave(1,i_point), &
                      rad_index, wave_index, l_index, l_count, fn_atom, &
                      n_zero_compute, zero_index_point &
                    )





                    ! reset i_basis_fns_inv
                    i_basis_fns_inv(:,atom_index(1:n_compute_atoms)) = 0

                 end if ! end if (partition_tab.gt.0)
              enddo ! end loop over a batch
              ! - all quantities that are evaluated pointwise are now known ...



              if (n_points.gt.0) then
                 ! Now perform all operations which are done across the entire 
                 ! integration shell at once.
                 ! New density is always evaluated
                 if (.not.density_full.and.state_one.ne.state_two)then
                     call evaluate_KS_density_densmat_esp &
                      (  n_points, wave(1,1), n_compute_c,   &
                      temp_rho(1,i_spin), n_max_compute_dens, &
                      density_matrix_con,&
                      density_matrix_con_two, work(1,1) )
                 else
                     call evaluate_KS_density_densmat &
                      (  n_points, wave(1,1), n_compute_c,   &
                      temp_rho(1,i_spin), n_max_compute_dens, &
                      n_centers_basis_T, density_matrix_con &
                      , work(1,1) )
                 endif
              end if ! (n_points>0)

              ! set electron density
              i_point = 0
              do i_index = 1, batches_work(i_my_batch)%size, 1

                 i_full_points_3 = i_full_points_3 + 1

                 if (max(partition_tab(i_full_points_3),&
                      hartree_partition_tab(i_full_points_3)).gt.0) then

                    i_point = i_point + 1

                    ! and set charge density locally

                    if (spin_treatment.eq.0) then

                       delta_rho_KS(1,i_full_points_3) =   &
                            delta_rho_KS( 1,i_full_points_3) &
                            + temp_rho(i_point,1)
                    elseif (spin_treatment.eq.1) then

                       ! Note that delta_rho_KS mixes the spin components of 
                       ! the density - but, in the dm
                       ! formalism, we do first an _outermost_ loop over 
                       ! spin up, and only then over spin down.
                       if(i_spin == 1)then

                          delta_rho_KS(i_full_points_3, 1) =  &
                               delta_rho_KS(i_full_points_3, 1)   &
                               +( temp_rho(i_point, 1) ) 


                          delta_rho_KS(i_full_points_3, 2) =   &
                               delta_rho_KS(i_full_points_3, 2)   &
                               +( temp_rho(i_point, 1) )  
                       else

                          delta_rho_KS(i_full_points_3, 1) =  &
                               delta_rho_KS(i_full_points_3, 1)   &
                               +( temp_rho(i_point, 2) )  

                          delta_rho_KS(i_full_points_3, 2) =   &
                               delta_rho_KS(i_full_points_3, 2)   &
                               -( temp_rho(i_point, 2) )  
                       end if
                    end if

                 endif
              end do

           else
               ! Even if n_compute .eq. 0 for the entire current batch of grid 
               ! points, we still need to make sure that the density _change_ 
               ! at this point is ( zero minus previous density ). This ensures 
               ! that. even for a zero KS density, a potentially non-zero 
               ! initialization density is subtracted to properly account for 
               ! the density change ....

               do i_index = 1, batches_work(i_my_batch)%size, 1

                  i_full_points_2 = i_full_points_2 + 1
                  i_full_points_3 = i_full_points_3 + 1

                  if (max(partition_tab(i_full_points_3),&
                        hartree_partition_tab(i_full_points_3)).gt.0.d0) then

                     ! only evaluate once both spin components are known
                     if (i_spin.eq.n_spin) then

                        ! local charge density

                        if (spin_treatment.eq.0) then

                           delta_rho_KS(i_full_points_3, 1) =   &
                                 delta_rho_KS(i_full_points_3, 1) 

                        elseif (spin_treatment.eq.1) then

                           delta_rho_KS(i_full_points_3, 1) =   &
                                 delta_rho_KS(i_full_points_3, 1) 

                           delta_rho_KS(i_full_points_3, 2) =   &
                                 delta_rho_KS(i_full_points_3, 2) 

                        end if
                     

                     end if ! (i_spin.eq.n_spin)

                  endif !(partition_tab)

               enddo ! (loop over all grid points, for empty batches)

           end if  ! end if (n_compute.gt.0)
        ! end if ! end distribution over threads
     end do ! end loop over grid batches

     ! Get work time and total time after barrier
     time_work = time_work + mpi_wtime()-time0
     call mpi_barrier(mpi_comm_global,info)
     time_all  = time_all  +mpi_wtime()-time0

     !--------------------------------- end go over the grids ------------------
  end do ! end i_spin


  if(use_batch_permutation > 0) then

    do i_spin = 1, n_spin
      call permute_point_array_back(n_bp,1,delta_rho_KS(:,i_spin),&
           delta_rho_KS_std(:,i_spin))
    enddo

    deallocate(hartree_partition_tab)

    deallocate(delta_rho_KS)

    deallocate(ins_idx)

  endif

  !---------- finally, deallocate stuff -----------------

  if (allocated( temp_rho_small       )) deallocate( temp_rho_small       )
  if (allocated( index_lm             )) deallocate( index_lm             )
  if (allocated( ylm_tab              )) deallocate( ylm_tab              )
  if (allocated( wave                 )) deallocate( wave                 )
  if (allocated( radial_wave          )) deallocate( radial_wave          )
  if (allocated( i_basis              )) deallocate( i_basis              )
  if (allocated( work                 )) deallocate( work                 )
  if (allocated( density_matrix_con   )) deallocate( density_matrix_con   )
  if (allocated( density_matrix_con_two)) deallocate( density_matrix_con_two)
  if (allocated( trigonom_tab         )) deallocate( trigonom_tab         )
  if (allocated( dir_tab              )) deallocate( dir_tab              )
  if (allocated( i_r                  )) deallocate( i_r                  )
  if (allocated( dist_tab_sq          )) deallocate( dist_tab_sq          )
  if (allocated( dist_tab             )) deallocate( dist_tab             )
  if (allocated( work_complex         )) deallocate( work_complex         )
  if (allocated( density_matrix       )) deallocate( density_matrix       )
  if (allocated( density_matrix_two   )) deallocate( density_matrix_two   )

end subroutine get_transition_density_densmat

!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/get_transition_density_densma
!  NAME
!    get_transition_density_densmat
!  SYNOPSIS
subroutine get_vxc_esp &
     ( KS_eigenvector, KS_eigenvector_complex, occ_numbers, partition_tab_std, &
       basis_l_max, delta_rho_KS_std,  &
       density_matrix_sparse,density_matrix_sparse_two,state_one,state_two,&
       k_point_one,density_full, output_select )

!  PURPOSE
!
!  The subroutine obtains the transistion density using the density
!  matrix for the calculation of esp charges (see evaluate_KS_density_densmat).
!
!
!  USES

  use dimensions,only:n_basis_fns,n_species,n_k_points,&
      n_k_points_task,n_spin,n_states,n_basis,&
      n_centers_basis_T,l_wave_max,n_periodic,n_centers_basis_integrals,&
      n_centers,n_centers_integrals
!      n_my_batches,n_full_points,n_max_batch_size
  use runtime_choices,only:spin_treatment,PM_none,&
      packed_matrix_format
  use esp_grids,only:n_full_points_esp,n_my_batches_esp,n_full_points_esp,&
                     n_max_batch_size_esp,batch_of_points_esp,batches_esp,&
                     n_max_compute_atoms_esp,n_max_compute_fns_dens_esp,&
                     n_max_compute_dens_esp
!  use grids,only:batch_of_points,batches
  use basis,only:basis_wave_ordered,basis_deriv_ordered
  use mpi_tasks,only:mpi_wtime,mpi_comm_global,check_allocation,myid
  use pbc_lists,only:inv_centers_basis_integrals,centers_basis_integrals
  use localorb_io,only:localorb_info
  use density_matrix_evaluation,only:evaluate_densmat
  use load_balancing,only:use_batch_permutation,batch_perm,&
      permute_point_array,permute_point_array_back
  use cartesian_ylm,only:n_max_cartesian,evaluate_cartesians,&
      evaluate_cartesian_hessian_and_gradient_terms
  use constants,only:pi
!libXC 
!      use xc_f90_types_m
!      use xc_f90_lib_m 
  implicit none

!  ARGUMENTS

  real*8,     dimension(n_basis, n_states, n_spin,n_k_points_task):: &
              KS_eigenvector
  complex*16, dimension(n_basis, n_states, n_spin,n_k_points_task):: &
              KS_eigenvector_complex
  real*8,     dimension(n_states, n_spin, n_k_points)             :: occ_numbers

  real*8, target, dimension(n_full_points_esp)      :: partition_tab_std
  integer, dimension(n_species)                 :: basis_l_max
  real*8, target, intent(OUT) :: delta_rho_KS_std( n_full_points_esp,n_spin)
  ! when this routine is called, density_matrix_sparse has either the dimension
  ! (n_hamiltonian_matrix_size) or (n_local_matrix_size)
  ! so we declare it here as a 1D assumed size array
  real*8,  dimension(*)                          :: density_matrix_sparse
  real*8,  dimension(*)                          :: density_matrix_sparse_two
  integer :: state_one,state_two,k_point_one
  logical :: density_full
  integer :: output_select
! INPUTS
! o KS_eigenvector -- eigenvectors if real eigenvectors are in use
! o KS_eigenvector_complex -- eigenvectors is complex eigenvectors are in use
! o occ_numbers -- occupation numbers of states
! o partition_tab -- partition function values
! o basis_l_max -- maximum basis l value of the basis functions
! o density_matrix_sparse -- this is the works space for density matrix, it is 
!                            used if packed matrixes are in use.
! o density_matrix_sparse_two -- this is the works space for density matrix, 
!                                it is used if packed matrixes are in use. 
!                                upper/lower trinangle
! o state_one -- first state for transition density
! o state_two -- second state for transition density
! o k_point_one  -- k_point for transition density
! o density_full -- switch to select transition or full density
!
! OUTPUT
!   o delta_rho_KS -- calculated charge density (no delta)
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE

!      TYPE(xc_f90_pointer_t) :: xc_func
!      TYPE(xc_f90_pointer_t) :: xc_info
     
 
      integer ::  func_id  ! here 1 is slater-x, 9 is pz-correlation
      real*8 zk(n_spin),sigma(n_spin),    &   
             vx(n_spin),fx(n_spin),kx(n_spin),  &
             vc(n_spin),fc(n_spin),kc(n_spin) 
     
      real*8 vsigma_x(n_spin), v2rho2_x(n_spin), & 
             v2rhosigma_x(n_spin), v2sigma2_x(n_spin)
      real*8 vsigma_c(n_spin), v2rho2_c(n_spin), & 
             v2rhosigma_c(n_spin), v2sigma2_c(n_spin), &
             v3rho3, v3rho2sigma, v3rhosigma2, v3sigma3


  integer :: n_full_points_work
  real*8, dimension(:,:),     allocatable:: dist_tab
  real*8, dimension(:,:),   allocatable:: dist_tab_sq
  real*8, dimension(:),     allocatable:: i_r
  real*8, dimension(:,:,:), allocatable:: dir_tab
  real*8, dimension(:,:),   allocatable:: trigonom_tab

  real*8,dimension(:),  allocatable:: radial_wave
  real*8,dimension(:)  ,allocatable:: radial_wave_deriv
  real*8,dimension(:),allocatable:: radial_wave_2nd_deriv
  real*8,dimension(:,:),allocatable:: wave
  real*8, dimension(:,:,:), allocatable :: gradient_basis_wave

  real*8, dimension(:,:,:), allocatable :: hessian_basis_wave
  real*8, dimension(:,:,:), allocatable :: sum_hessian
  real*8, dimension(:,:,:), allocatable :: cartesians
  real*8, dimension(:,:,:), allocatable :: sum_gradient

  integer,dimension(:),allocatable :: i_basis
  integer :: n_compute_c
  integer :: n_compute_a

  real*8 coord_current(3)

  integer :: n_compute_fns
  integer :: i_basis_fns(n_basis_fns*n_centers_integrals)
  integer :: i_basis_fns_inv(n_basis_fns,n_centers)
  integer :: i_atom_fns(n_basis_fns*n_centers_integrals)

  integer :: n_compute_atoms
  integer :: atom_index(n_centers_integrals)
  integer :: atom_index_inv(n_centers)

  integer :: spline_array_start(n_centers_integrals)
  integer :: spline_array_end(n_centers_integrals)

! VB - renewed index infrastructure starts here

  !     other local variables

  integer :: l_ylm_max
  integer :: n_points

  ! (n_basis, n_states, n_spin) on purpose
  ! beware: inside the subroutines (evaluate_KS_density_v1 for instance)
  ! dimension is (n_compute, max_occ_number)
  ! that's a trick to get a continuous data flow and not
  ! bad programming

  real*8, dimension(n_max_batch_size_esp,n_spin) :: temp_rho
  real*8, dimension(n_max_batch_size_esp,3,n_spin) :: temp_rho_gradient
  real*8, dimension(n_max_batch_size_esp,6,n_spin) :: temp_hessian
  real*8, dimension(n_spin) :: vxc
  real*8, dimension(n_spin) :: exc
  real*8, dimension(n_spin) :: sigmaxc
  real*8, dimension(n_spin) :: vsigma
  real*8,dimension(:,:),allocatable :: temp_rho_small

  integer, dimension(:,:), allocatable :: index_lm
  real*8,  dimension(:,:), allocatable :: ylm_tab

  real*8, dimension(:,:),  allocatable :: density_matrix
  real*8, dimension(:,:),  allocatable :: density_matrix_two
  real*8, dimension(:),  allocatable :: density_matrix_con
  real*8, dimension(:),  allocatable :: density_matrix_con_two
  real*8, dimension(:,:),  allocatable :: work
  complex*16, dimension(:,:),allocatable :: work_complex

  !     counters
  integer :: i_l
  integer :: i_m
  integer :: i_point

  integer :: i_full_points
  integer :: i_full_points_2
  integer :: i_full_points_3

  integer :: i_spin
  integer :: i_index
  integer :: i_my_batch
  integer :: info

  ! Load balancing stuff

  integer n_my_batches_work ! Number of batches actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                     ! actually used

  ! Pointers to the actually used array
  real*8, pointer :: partition_tab(:)
  real*8, pointer :: delta_rho_KS(:,:)

  integer i, j, i_off, n_bp
  integer, allocatable :: ins_idx(:)

  ! Timings for analyzing work imbalance
  real*8 time0, time_work, time_all


  real*8 :: abs_grad_rho, LAPLACIAN_rho, del_grad_rho, S, U, V, EX, RS,&
            ZET, G, fk, T, UU, VV, WW, ec, vcup, vcdn, H, DVCUP, DVCDN,&
            twoksg, rho, rho2, sk, v_ex,vsig,vex


  !---------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing)
  ! or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    !n_my_batches_work = batch_perm(n_bp)%n_my_batches
    !batches_work => batch_perm(n_bp)%batches
    partition_tab => batch_perm(n_bp)%partition_tab

   ! allocate(hartree_partition_tab(batch_perm(n_bp)%n_full_points))
   ! call permute_point_array(n_bp,1,hartree_partition_tab_std,&
   !      hartree_partition_tab)

    !allocate(delta_rho_KS(batch_perm(n_bp)%n_full_points,n_spin))

    !allocate(ins_idx(batch_perm(n_bp)%n_basis_local))

  else
    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    delta_rho_KS => delta_rho_KS_std

  endif

  !---------------------------------------------------------------------------

  ! initialize charge density convergence criterion

  delta_rho_KS = 0.d0


  if(packed_matrix_format == PM_none)then
     if(.not. allocated(density_matrix))then
        allocate(density_matrix(n_centers_basis_T, n_centers_basis_T),&
        stat=info)
        call check_allocation(info, 'density_matrix                ')
     end if
     if (spin_treatment.eq.1)then
       if(.not. allocated(density_matrix_two))then
        allocate(density_matrix_two(n_centers_basis_T, n_centers_basis_T),&
        stat=info)
        call check_allocation(info, 'density_matrix_two             ')
        end if
     endif
     if(.not.density_full.and.state_one.ne.state_two)then
       if(.not. allocated(density_matrix_two))then
        allocate(density_matrix_two(n_centers_basis_T, n_centers_basis_T),&
        stat=info)
        call check_allocation(info, 'density_matrix_two             ')
       end if
     else
       if(.not. allocated(density_matrix_two))then
         allocate(density_matrix_two(1,1))
       endif
     endif
  else
     ! Allocate dummy since otherways compiling with checking doesn't work
     allocate(density_matrix(1,1))
     allocate(density_matrix_two(1,1))
  end if

  ! --- start actual calculation
  time_work = 0
  time_all  = 0


     ! --- density matrix construction/administration
     if(density_full)then
       call evaluate_densmat(KS_eigenvector,KS_eigenvector_complex,occ_numbers,&
            density_matrix, density_matrix_sparse, 1, .false.)
       if (spin_treatment.eq.1)then
       call evaluate_densmat(KS_eigenvector,KS_eigenvector_complex,occ_numbers,&
            density_matrix_two, density_matrix_sparse_two, 2, .false.)      
       endif
     else
       call evaluate_densmat_esp(KS_eigenvector, KS_eigenvector_complex, &
            occ_numbers,density_matrix, density_matrix_sparse, 1, &
            state_one, state_two,k_point_one, .false.)
       if(state_one.ne.state_two)then
         call evaluate_densmat_esp(KS_eigenvector, KS_eigenvector_complex, &
            occ_numbers,density_matrix_two, density_matrix_sparse_two, 1, &
            state_two, state_one,k_point_one, .false.)
       endif
     end if

     ! ---------------- end construct density matrix -------------------------

     if(.not. allocated(dist_tab))then
        allocate(dist_tab(n_centers_integrals, n_max_batch_size_esp),stat=info)
        call check_allocation(info, 'dist_tab                      ')
     end if
     if(.not. allocated(dist_tab_sq))  then
        allocate(dist_tab_sq(n_centers_integrals, n_max_batch_size_esp),stat=info)
        call check_allocation(info, 'dist_tab_sq                   ')
     end if
     if(.not. allocated(i_r))then
        allocate(i_r(n_max_compute_atoms_esp),stat=info)
        call check_allocation(info, 'i_r                           ')
     end if
     if(.not. allocated(dir_tab)) then
        allocate(dir_tab(3, n_centers_integrals, n_max_batch_size_esp),stat=info)
        call check_allocation(info, 'dir_tab                       ')
     end if
     if(.not. allocated(trigonom_tab)) then
        allocate(trigonom_tab(4, n_max_compute_atoms_esp),stat=info)
        call check_allocation(info, 'trigonom_tab                  ')
     end if
     if(.not. allocated(density_matrix_con))then
        allocate(density_matrix_con(n_max_compute_dens_esp*n_max_compute_dens_esp),&
                 stat=info)
        call check_allocation(info, 'density_matrix_con            ')
     end if
     if(spin_treatment.eq.1.or.(.not.density_full.and.state_one.ne.state_two))then
       if(.not. allocated(density_matrix_con_two))then
        allocate(density_matrix_con_two(n_max_compute_dens_esp*n_max_compute_dens_esp),&
                 stat=info)
        call check_allocation(info, 'density_matrix_con_two            ')
       end if
     endif
     if(.not. allocated(work))then
        allocate(work(n_max_compute_dens_esp, n_max_batch_size_esp),stat=info)
        call check_allocation(info, 'work                          ')
     end if
     if(.not. allocated(i_basis))then
        allocate(i_basis(n_centers_basis_T),stat=info)
        call check_allocation(info, 'i_basis                       ')
     end if
     if(.not.allocated(radial_wave))then
        allocate(radial_wave(n_max_compute_fns_dens_esp),stat=info)
        call check_allocation(info, 'radial_wave                   ')
     end if
     if(.not.allocated(radial_wave_deriv))then
        allocate(radial_wave_deriv(n_max_compute_fns_dens_esp),stat=info)
        call check_allocation(info, 'radial_wave_deriv             ')
     endif
     if(.not. allocated(wave))then
        allocate(wave(n_max_compute_dens_esp, n_max_batch_size_esp),stat=info)
        call check_allocation(info, 'wave                          ')
     end if
     if(.not. allocated(gradient_basis_wave))then
       allocate (gradient_basis_wave(n_max_compute_dens_esp,3,n_max_batch_size_esp ),stat=info)
       call check_allocation(info, 'gradient_basis_wave           ')
     end if

     if (.not.allocated(cartesians)) then
          allocate(cartesians(n_max_cartesian, 0:l_wave_max,n_max_compute_atoms_esp ),stat=info)
          call check_allocation(info, 'cartesians                    ')
     end if
     if (.not.allocated(sum_gradient)) then
          allocate(sum_gradient(3, (l_wave_max+1) ** 2, n_max_compute_atoms_esp ),stat=info)
          call check_allocation(info, 'sum_gradient                  ')
     end if
     if (.not.allocated(sum_hessian)) then
          allocate(sum_hessian(6, (l_wave_max+1) ** 2, n_max_compute_atoms_esp ),stat=info)
          call check_allocation(info, 'sum_hessian                   ')
     end if
     if (.not.allocated(hessian_basis_wave)) then
          allocate(hessian_basis_wave(n_max_compute_dens_esp, 6, n_max_batch_size_esp),stat=info)
          call check_allocation(info, 'hessian_basis_wave            ')
     end if
     if(.not. allocated(radial_wave_2nd_deriv))then
          allocate(radial_wave_2nd_deriv(n_max_compute_fns_dens_esp),stat=info)
          call check_allocation(info, 'radial_wave_2nd_deriv         ')
     end if

     l_ylm_max = l_wave_max

     if(.not. allocated( ylm_tab))then
        allocate( ylm_tab( (l_ylm_max+1)**2,n_max_compute_atoms_esp),stat=info )
        call check_allocation(info, 'ylm_tab                       ')
     end if
     if(.not. allocated( index_lm))then
        allocate( index_lm( -l_ylm_max:l_ylm_max, 0:l_ylm_max),stat=info)
        call check_allocation(info, 'index_lm                      ')
      end if
!libXC
!     func_id=130 !PBE_Cor
!     call xc_f90_func_init(xc_func, xc_info, func_id, XC_UNPOLARIZED)

     ! initialize index_lm
     i_index = 0
     do i_l = 0, l_ylm_max, 1
        do i_m = -i_l, i_l
           i_index = i_index + 1
           index_lm(i_m, i_l) = i_index
        enddo
     enddo

     call mpi_barrier(mpi_comm_global,info) ! Barrier is for correct timing!!!
     time0 = mpi_wtime()

     ! --------go over the grids and construc the density using density matrix

     i_basis_fns_inv = 0

     i_full_points = 0
     i_full_points_2 = 0
     i_full_points_3 = 0
     do i_my_batch = 1, n_my_batches_work, 1

           n_compute_c = 0
           n_compute_a = 0
           i_basis = 0

           i_point = 0

           ! loop over one batch
           do i_index = 1, batches_work(i_my_batch)%size_esp, 1

              i_full_points = i_full_points + 1

              if ((partition_tab(i_full_points)).gt.0.d0) then

                 i_point = i_point+1

                 !     get current integration point coordinate
                 coord_current(:) = batches_work(i_my_batch) % points_esp(i_index) &
                                    % coords_esp(:)

                 if(n_periodic>0)then
                    call map_to_center_cell( coord_current(1:3))
                 end if

                 ! compute atom-centered coordinates of current integration 
                 ! point, as viewed from all atoms
                 call tab_atom_centered_coords_p0 &
                      ( coord_current,  &
                      dist_tab_sq(1,i_point),  &
                      dir_tab(1,1,i_point), &
                      n_centers_integrals, centers_basis_integrals )



                 !     determine which basis functions are relevant at current 
                 !     integration point,and tabulate their indices

                 !next, determine which basis functions u(r)/r*Y_lm(theta,phi)
                 ! are actually needed

                 call prune_basis_p0 &
                     ( dist_tab_sq(1,i_point), &
                       n_compute_a, n_compute_c, i_basis,  &
                       n_centers_basis_T, n_centers_integrals,&
                       inv_centers_basis_integrals  )
              end if
           enddo ! end loop over the angular shell


           ! from list of n_compute active basis functions in batch, collect 
           ! all atoms that are ever needed in batch.


           if(packed_matrix_format /= PM_none )then
              if(use_batch_permutation > 0) then
                do i=1,n_compute_c
                  ins_idx(i) = batch_perm(n_bp)%i_basis_glb_to_loc(i_basis(i))
                enddo
                density_matrix_con(1:n_compute_c*n_compute_c) = 0
                if(.not.density_full.and.state_one.ne.state_two)then
                  density_matrix_con_two(1:n_compute_c*n_compute_c) = 0
                endif
                do i=1,n_compute_c
                  i_off = (ins_idx(i)*(ins_idx(i)-1))/2
                  do j=1,i
                    density_matrix_con(j+(i-1)*n_compute_c) = &
                    density_matrix_sparse(ins_idx(j)+i_off)
                    if(.not.density_full.and.state_one.ne.state_two)then
                      density_matrix_con_two(j+(i-1)*n_compute_c) = &
                      density_matrix_sparse_two(ins_idx(j)+i_off)
                    endif
                  enddo
                enddo
              else
                call  prune_density_matrix_sparse(density_matrix_sparse, &
                      density_matrix_con, n_compute_c, i_basis)
                if (spin_treatment.eq.1)then
		  call  prune_density_matrix_sparse(density_matrix_sparse_two, &
			density_matrix_con_two, n_compute_c, i_basis)                
	        endif
                if(.not.density_full.and.state_one.ne.state_two)then
                  call  prune_density_matrix_sparse(density_matrix_sparse_two, &
                      density_matrix_con_two, n_compute_c, i_basis)
                endif
              endif
           else
              call  prune_density_matrix(density_matrix, density_matrix_con, &
                   n_compute_c, i_basis)
	      if (spin_treatment.eq.1)then
		  call  prune_density_matrix(density_matrix_two, &
			density_matrix_con_two, n_compute_c, i_basis)                
	      endif
              if(.not.density_full.and.state_one.ne.state_two)then
                call  prune_density_matrix(density_matrix_two, density_matrix_con_two, &
                   n_compute_c, i_basis)
              endif
           end if

           n_points = i_point

           if (n_compute_c.gt.0) then

              ! Determine all radial functions, ylm functions and their 
              ! derivatives thatare best evaluated strictly locally at each 
              ! individual grid point.
              i_point = 0
              do i_index = 1, batches_work(i_my_batch)%size_esp, 1

                 i_full_points_2 = i_full_points_2 + 1

                 if ((partition_tab(i_full_points_2)).gt.0.d0) then

                    i_point = i_point+1
                    n_compute_atoms = 0
                    n_compute_fns = 0
                    i_basis_fns_inv = 0
                    atom_index_inv = 0

                    ! All radial functions (i.e. u(r), u''(r)+l(l+2)/r^2, 
                    ! u'(r) if needed) Are stored in a compact spline array 
                    ! that can be accessed by spline_vector_waves, without any 
                    !copying and without doing any unnecessary operations.
                    ! The price is that the interface is no longer explicit in 
                    ! terms of physical objects. See shrink_fixed_basis() for 
                    ! details regarding the reorganized spline arrays.

                    call prune_radial_basis_p0 &
                        ( dist_tab_sq(1,i_point), &
                        dist_tab(1,i_point), &
                        dir_tab(1,1,i_point), &
                        n_compute_atoms, atom_index, atom_index_inv, &
                        n_compute_fns, i_basis_fns, i_basis_fns_inv, &
                        i_atom_fns, spline_array_start, spline_array_end, &
                        n_centers_integrals, centers_basis_integrals)


                    ! Tabulate distances, unit vectors, and inverse logarithmic 
                    ! grid units for all atoms which are actually relevant

                    call tab_local_geometry_p0 &
                        ( dist_tab_sq(1, i_point), n_compute_atoms, atom_index, &
                        dir_tab(1,1,i_point), dist_tab(1,i_point), i_r )

                    ! Determine all needed radial functions from efficient 
                    ! splines

                    ! compute trigonometric functions of spherical coordinate
                    ! angles of current integration point, viewed from all atoms

                    call tab_trigonom_p0 &
                         ( n_compute_atoms, dir_tab(1,1,i_point),  &
                           trigonom_tab(1,1) &
                           )

                    ! tabulate distance and Ylm's w.r.t. other atoms
                    call tab_wave_ylm_p0 &
                            ( n_compute_atoms, atom_index,  &
                            trigonom_tab(1,1), basis_l_max,  &
                            l_ylm_max, &
                            ylm_tab(1,1) )
                    ! Now evaluate radial functions u(r) from the previously 
                    ! stored compressed spline arrays
                    call evaluate_radial_functions_p0 &
                         (   spline_array_start, spline_array_end, &
                         n_compute_atoms, n_compute_fns,  &
                         dist_tab(1,i_point), i_r(1), &
                         atom_index, i_basis_fns_inv, &
                         basis_wave_ordered, radial_wave(1), &
                         .false., n_compute_c, n_max_compute_fns_dens_esp   &
                         )


                    ! tabulate total wave function value for each basis function 
                    ! in all cases -but only now are we sure that we have 
                    ! ylm_tab ...

                    ! tabulate total wave function value for each basis function
                   ! tabulate total wave function value for each basis function
                   call evaluate_waves_p0  &
                        ( l_ylm_max,   &
                        ylm_tab, dist_tab(1,i_point),   &
                        index_lm, n_compute_c,   &
                        i_basis, radial_wave(1),   &
                        wave(1,i_point), n_compute_atoms,   &
                        atom_index_inv, n_compute_fns,  &
                        i_basis_fns_inv,  n_max_compute_fns_dens_esp )

                   call evaluate_radial_functions_p0  &
                        ( spline_array_start, spline_array_end,  &
                        n_compute_atoms, n_compute_fns,   &
                        dist_tab(1,i_point), i_r,  &
                        atom_index, i_basis_fns_inv,  &
                        basis_deriv_ordered,   &
                        radial_wave_deriv(1), .true.,  &
                        n_compute_c, n_max_compute_fns_dens_esp )

                   call evaluate_radial_functions_deriv_p0 &
                        (spline_array_start, spline_array_end, &
                         n_compute_atoms, n_compute_fns,  &
                         dist_tab(1,i_point), i_r(1), &
                         atom_index, i_basis_fns_inv, &
                         basis_deriv_ordered,  &
                         radial_wave_2nd_deriv(1),  &
                         .true., n_max_compute_fns_dens_esp &
                         )
                      ! Basis functions, their gradients and hessians are needed.
                      ! They are evaluated on the basis of a cartesian expansion of the ylm-functions -
                      ! hence, if we do need all this, we get a lot of stuff directly that we'd have to
                      ! compute separately otherwise (see below)

                      ! first, corresponding cartesian terms x^l_x*y^l_y*z^l_z are evaluated
                      ! for the current integration point 
                      call evaluate_cartesians &
                           (dir_tab(1,1,i_point), basis_l_max, l_wave_max,  &
                           atom_index, n_compute_atoms, cartesians(1,0,1) )

                      ! further "ingredients" of the hessian are evaluated based on the cartesians
                      ! ylm's are evaluated on the fly
                      call evaluate_cartesian_hessian_and_gradient_terms &
                           (basis_l_max, l_wave_max, cartesians(1,0,1), &
                           atom_index, n_compute_atoms,  &
                           ylm_tab(1,1),  &
                           sum_gradient(1,1,1),  &
                           sum_hessian(1,1,1))

                      ! now, all hessians of the basis functions are evaluated based on the
                      ! "ingredients" (ylm, sum_two_gradient, sum_hessian)
                      ! (gradient and ylm-functions themselves would be for free, we have
                      ! to check the performance)
                   call evaluate_wave_gradient_cartesian_esp &
                        (dist_tab(1,i_point), i_r(1),  &
                         dir_tab(1,1,i_point), index_lm, l_wave_max,  &
                         n_compute_c, i_basis, atom_index_inv,  &
                         i_basis_fns_inv, radial_wave(1),  &
                         radial_wave_deriv(1), &
                         ylm_tab(1,1),  &
                         sum_gradient(1,1,1),  &
                         gradient_basis_wave(1,1,i_point),&
                         n_compute_atoms,n_max_compute_fns_dens_esp,&
                         n_max_compute_dens_esp)

                   call evaluate_wave_hessian_cartesian_esp &
                        (dist_tab(1,i_point), i_r(1),  &
                         dir_tab(1,1,i_point), index_lm, l_wave_max,  &
                         n_compute_c, i_basis, atom_index_inv,  &
                         i_basis_fns_inv, radial_wave(1),  &
                         radial_wave_deriv(1),  &
                         radial_wave_2nd_deriv(1),  &
                         ylm_tab(1,1),  &
                         sum_gradient(1,1,1),  &
                         sum_hessian(1,1,1), &
                         hessian_basis_wave(1,1,i_point), n_compute_atoms,&
                         n_max_compute_fns_dens_esp,n_max_compute_dens_esp)
                 end if ! end if (partition_tab.gt.0)
              enddo ! end loop over a batch
              ! - all quantities that are evaluated pointwise are now known ...
              if (n_points.gt.0) then
                 ! Now perform all operations which are done across the entire 
                 ! integration shell at once.
                 ! New density is always evaluated
                 if (.not.density_full.and.state_one.ne.state_two)then
                     call evaluate_KS_density_densmat_esp &
                      (  n_points, wave(1,1), n_compute_c,   &
                      temp_rho(1,1), n_max_compute_dens_esp, &
                      density_matrix_con,&
                      density_matrix_con_two, work(1,1) )
                     do i=1, 3, 1
                       call evaluate_KS_trafo_densmat_esp &
                        (  n_points, wave(1,1), gradient_basis_wave(1:n_max_compute_dens_esp,i,1:n_points), n_compute_c,   &
                        temp_rho_gradient(1,i,1), n_max_compute_dens_esp, &
                         density_matrix_con, &
                        density_matrix_con_two, work(1,1) )
                     enddo
                     do i=1, 6, 1
                       call evaluate_KS_trafo_densmat_esp &
                        (  n_points, wave(1,1), hessian_basis_wave(1,i,1), n_compute_c,   &
                        temp_hessian(1,i,1), n_max_compute_dens_esp, &
                        density_matrix_con, &
                        density_matrix_con_two, work(1,1) )
                     enddo
                 else
                     call evaluate_KS_density_densmat &
                      (  n_points, wave(1,1), n_compute_c,   &
                      temp_rho(1,1), n_max_compute_dens_esp, &
                      n_centers_basis_T, density_matrix_con &
                      , work(1,1) )
                     do i=1, 3, 1
                       call evaluate_KS_trafo_densmat &
                        (  n_points, wave(1,1), gradient_basis_wave(1:n_max_compute_dens_esp,i,1:n_points), n_compute_c,   &
                        temp_rho_gradient(1,i,1), n_max_compute_dens_esp, &
                        n_centers_basis_T, density_matrix_con &
                        , work(1,1) )
                     enddo
                     do i=1, 6, 1
                       call evaluate_KS_trafo_densmat &
                        (  n_points, wave(1,1), hessian_basis_wave(1:n_max_compute_dens_esp,i,1:n_points), n_compute_c,   &
                        temp_hessian(1,i,1), n_max_compute_dens_esp, &
                        n_centers_basis_T, density_matrix_con &
                        , work(1,1) )
                     enddo
                     if (spin_treatment.eq.1)then
		      call evaluate_KS_density_densmat &
			(  n_points, wave(1,1), n_compute_c,   &
			temp_rho(1,2), n_max_compute_dens_esp, &
			n_centers_basis_T, density_matrix_con_two &
			, work(1,1) )
		      do i=1, 3, 1
			call evaluate_KS_trafo_densmat &
			  (  n_points, wave(1,1), gradient_basis_wave(1:n_max_compute_dens_esp,i,1:n_points), n_compute_c,   &
			  temp_rho_gradient(1,i,2), n_max_compute_dens_esp, &
			  n_centers_basis_T, density_matrix_con_two &
			  , work(1,1) )
		      enddo
		      do i=1, 6, 1
			call evaluate_KS_trafo_densmat &
			  (  n_points, wave(1,1), hessian_basis_wave(1:n_max_compute_dens_esp,i,1:n_points), n_compute_c,   &
			  temp_hessian(1,i,2), n_max_compute_dens_esp, &
			  n_centers_basis_T, density_matrix_con_two &
			  , work(1,1) )
		      enddo    
		    endif
                  endif
              end if ! (n_points>0)       
              ! set electron density
              i_point = 0
              do i_index = 1, batches_work(i_my_batch)%size_esp, 1

                 i_full_points_3 = i_full_points_3 + 1

                 if (partition_tab(i_full_points_3).gt.0) then

                     i_point = i_point + 1

                    ! and set charge density locally
                     do i_spin = 1, n_spin

                       rho=temp_rho(i_point,i_spin)
                       rho2=temp_rho(i_point,i_spin)
                       abs_grad_rho=sqrt(temp_rho_gradient(i_point,1,i_spin)**2+&
                                         temp_rho_gradient(i_point,2,i_spin)**2+&
                                         temp_rho_gradient(i_point,3,i_spin)**2)
                       LAPLACIAN_rho=temp_hessian(i_point,1,i_spin) + &
                                     temp_hessian(i_point,4,i_spin) + &
                                     temp_hessian(i_point,6,i_spin)
                       if (abs_grad_rho.gt.1.e-18)then
                         del_grad_rho=((temp_rho_gradient(i_point,1,i_spin)*&
                                             temp_hessian(i_point,1,i_spin)+&
                                        temp_rho_gradient(i_point,2,i_spin)*&
                                             temp_hessian(i_point,2,i_spin)+&
                                        temp_rho_gradient(i_point,3,i_spin)*&
                                            temp_hessian(i_point,3,i_spin))*&
                                        temp_rho_gradient(i_point,1,i_spin)+&
                                       (temp_rho_gradient(i_point,1,i_spin)*&
                                             temp_hessian(i_point,2,i_spin)+&
                                        temp_rho_gradient(i_point,2,i_spin)*&
                                             temp_hessian(i_point,4,i_spin)+&
                                        temp_rho_gradient(i_point,3,i_spin)*&
                                            temp_hessian(i_point,5,i_spin))*&
                                        temp_rho_gradient(i_point,2,i_spin)+&
                                       (temp_rho_gradient(i_point,1,i_spin)*&
                                             temp_hessian(i_point,3,i_spin)+&
                                        temp_rho_gradient(i_point,2,i_spin)*&
                                             temp_hessian(i_point,5,i_spin)+&
                                        temp_rho_gradient(i_point,3,i_spin)*&
                                            temp_hessian(i_point,6,i_spin))*&
                                       temp_rho_gradient(i_point,3,i_spin))/&
                                      abs_grad_rho
                       else
                         del_grad_rho=0d0
                       endif
                       ! (GRAD rho)*GRAD(ABS(GRAD rho))
                       if((output_select.eq.3).or.(output_select.eq.2))then
			  fk=(3.0*(pi**2)*rho2)**(1./3.)
			  if (rho2.gt.1.e-15)then
			      S = abs_grad_rho/&
				  (2.0*fk*rho2)
			      U = del_grad_rho/&
				  ((rho2**2) * (2.0*fk)**3)
			      V = (LAPLACIAN_rho)/&
				  (rho2*(2.0*fk)**2)
!                             U = (4./3.)*S**3
!                             V = (4./3.)*S*S
			      call EXCHPBE(rho2,S,U,V,1,1,EX,v_ex)
                            delta_rho_KS(i_full_points_3,i_spin) =   &
                                      delta_rho_KS( i_full_points_3,i_spin) +  v_ex
!                            write(use_unit,*) 'vx',temp_rho(i_point,1),v_ex,ex
			  endif
                       endif
		       if(output_select.eq.5)then

			    delta_rho_KS(i_full_points_3,i_spin) =   &
			             delta_rho_KS( i_full_points_3,i_spin)  + temp_rho(i_point,i_spin)
!                        write(use_unit,*) 'vc',rho,vcup+DVCUP,vcdn+DVCDN,ec+h
		       endif    
                     enddo  
                     
		     if(output_select.eq.4.or.output_select.eq.2)then

		       if (spin_treatment.eq.0)then
			  if(rho.gt.1.e-18)then
			    
			    ZET =  0d0!(rhoup-rhodn)/rho
			    g = ((1.+zet)**(2./3.)+(1.-zet)**(2./3.))/2.
			    fk=(3.0*(pi**2)*rho)**(1./3.)
			    rs=(3./(4.*pi*rho))**(1./3.)
			    sk = sqrt((4.0*fk)/pi)
			    twoksg=2.d0*sk*g
			    t=abs_grad_rho/(twoksg*rho)
			    uu=del_grad_rho/(rho**2 * twoksg**3)
			    vv=LAPLACIAN_rho/(rho*twoksg**2)
			    WW= 0d0 !(GRAD rho)*(GRAD ZET)/(rho * (2*KS*G)**2
			    call CORPBE(RS,ZET,T,uu,vv,WW,1,1,ec,vcup,vcdn,H,DVCUP,DVCDN)
			    vcup = vcup + DVCUP
			    vcdn = vcdn + DVCDN
			    delta_rho_KS(i_full_points_3,1) =   &
			             delta_rho_KS( i_full_points_3,1)  + vcup
			  endif
!                        write(use_unit,*) 'vc',rho,vcup+DVCUP,vcdn+DVCDN,ec+h
                       else
			  rho=temp_rho(i_point,1)+temp_rho(i_point,2)
			  if(rho.gt.1.e-18)then
			  
			    abs_grad_rho = sqrt(sum(temp_rho_gradient(i_point,1:3,1) + temp_rho_gradient(i_point,1:3,2))**2)
                            LAPLACIAN_rho=temp_hessian(i_point,1,1) + &
                                     temp_hessian(i_point,4,1) + &
                                     temp_hessian(i_point,6,1) + &
                                     temp_hessian(i_point,1,2) + &
                                     temp_hessian(i_point,4,2) + &
                                     temp_hessian(i_point,6,2) 
			    del_grad_rho=((temp_rho_gradient(i_point,1,1)*&
                                             temp_hessian(i_point,1,1)+&
                                        temp_rho_gradient(i_point,2,1)*&
                                             temp_hessian(i_point,2,1)+&
                                        temp_rho_gradient(i_point,3,1)*&
                                            temp_hessian(i_point,3,1))*&
                                        temp_rho_gradient(i_point,1,1)+&
                                       (temp_rho_gradient(i_point,1,1)*&
                                             temp_hessian(i_point,2,1)+&
                                        temp_rho_gradient(i_point,2,1)*&
                                             temp_hessian(i_point,4,1)+&
                                        temp_rho_gradient(i_point,3,1)*&
                                            temp_hessian(i_point,5,1))*&
                                        temp_rho_gradient(i_point,2,1)+&
                                       (temp_rho_gradient(i_point,1,1)*&
                                             temp_hessian(i_point,3,1)+&
                                        temp_rho_gradient(i_point,2,1)*&
                                             temp_hessian(i_point,5,1)+&
                                        temp_rho_gradient(i_point,3,1)*&
                                            temp_hessian(i_point,6,1))*&
                                       temp_rho_gradient(i_point,3,1))/&
                                      abs_grad_rho + &
                                      ((temp_rho_gradient(i_point,1,2)*&
                                             temp_hessian(i_point,1,2)+&
                                        temp_rho_gradient(i_point,2,2)*&
                                             temp_hessian(i_point,2,2)+&
                                        temp_rho_gradient(i_point,3,2)*&
                                            temp_hessian(i_point,3,2))*&
                                        temp_rho_gradient(i_point,1,2)+&
                                       (temp_rho_gradient(i_point,1,2)*&
                                             temp_hessian(i_point,2,2)+&
                                        temp_rho_gradient(i_point,2,2)*&
                                             temp_hessian(i_point,4,2)+&
                                        temp_rho_gradient(i_point,3,2)*&
                                            temp_hessian(i_point,5,2))*&
                                        temp_rho_gradient(i_point,2,2)+&
                                       (temp_rho_gradient(i_point,1,2)*&
                                             temp_hessian(i_point,3,2)+&
                                        temp_rho_gradient(i_point,2,2)*&
                                             temp_hessian(i_point,5,2)+&
                                        temp_rho_gradient(i_point,3,2)*&
                                            temp_hessian(i_point,6,2))*&
                                       temp_rho_gradient(i_point,3,2))/&
                                      abs_grad_rho
			    ZET =  (temp_rho(i_point,1)-temp_rho(i_point,2))/rho
			    g = ((1.+zet)**(2./3.)+(1.-zet)**(2./3.))/2.
			    fk=(3.0*(pi**2)*rho)**(1./3.)
			    rs=(3./(4.*pi*rho))**(1./3.)
			    sk = sqrt((4.0*fk)/pi)
			    twoksg=2.d0*sk*g
			    t=abs_grad_rho/(twoksg*rho)
			  
			    uu=del_grad_rho/(rho**2 * twoksg**3)
			    vv=LAPLACIAN_rho/(rho*twoksg**2)
			    WW= (abs(sum(temp_rho_gradient(i_point,1:3,1)**2))-&
	                         abs(sum(temp_rho_gradient(i_point,1:3,2)**2))-&
	                         zet*abs(sum((temp_rho_gradient(i_point,1:3,1)+&
				 temp_rho_gradient(i_point,1:3,2))**2)))/&
			         ((rho * 2*sk*g)**2)
			    call CORPBE(RS,ZET,T,uu,vv,WW,1,1,ec,vcup,vcdn,H,DVCUP,DVCDN)
			    vcup = vcup + DVCUP
			    vcdn = vcdn + DVCDN
			    delta_rho_KS(i_full_points_3,1) =   &
			             delta_rho_KS( i_full_points_3,1)  + vcup
			    delta_rho_KS(i_full_points_3,2) =   &
			             delta_rho_KS( i_full_points_3,2)  + vcdn
			  endif
			endif
			
!libXC
!                      abs_grad_rho=sqrt(temp_rho_gradient(i_point,1,i_spin)**2+&
!                                         temp_rho_gradient(i_point,2,i_spin)**2+&
!                                         temp_rho_gradient(i_point,3,i_spin)**2)                   
!                      sigmaxc(i_spin)=abs_grad_rho**2
!                      call xc_f90_lda_exc_vxc(xc_func, 1, temp_rho(i_point,i_spin),&
!                                         exc(i_spin), vxc(i_spin))
!                      call xc_f90_gga_exc_vxc(xc_func, 1, temp_rho(i_point,i_spin),&
!                               sigmaxc(i_spin), exc(i_spin), vxc(i_spin), vsigma(i_spin))
!                       write(use_unit,*) 'vlibxc', temp_rho(i_point,1),vxc(i_spin),exc(i_spin)
!                      delta_rho_KS(i_full_points_3,i_spin) =   &
!		             delta_rho_KS( i_full_points_3,i_spin) +vxc(i_spin)   

!                    elseif (spin_treatment.eq.1) then
!		      write(info_str,*) 'Error: The evaluation of the XC-Potential is not implemented for the spin polarized case'
!                      call aims_stop_coll(info_str)
                    

                    end if

		 endif
		 
              end do
           else
               ! Even if n_compute .eq. 0 for the entire current batch of grid 
               ! points, we still need to make sure that the density _change_ 
               ! at this point is ( zero minus previous density ). This ensures 
               ! that. even for a zero KS density, a potentially non-zero 
               ! initialization density is subtracted to properly account for 
               ! the density change ....

               do i_index = 1, batches_work(i_my_batch)%size_esp, 1

                  i_full_points_2 = i_full_points_2 + 1
                  i_full_points_3 = i_full_points_3 + 1

                  if (partition_tab(i_full_points_3).gt.0.d0) then

                     ! only evaluate once both spin components are known
                     if (i_spin.eq.n_spin) then

                        ! local charge density

                        if (spin_treatment.eq.0) then

                           delta_rho_KS(i_full_points_3, 1) =   &
                                 delta_rho_KS(i_full_points_3, 1) 

                        elseif (spin_treatment.eq.1) then

                           delta_rho_KS(i_full_points_3, 1) =   &
                                 delta_rho_KS(i_full_points_3, 1) 

                           delta_rho_KS(i_full_points_3, 2) =   &
                                 delta_rho_KS(i_full_points_3, 2) 

                        end if
                     

                     end if ! (i_spin.eq.n_spin)

                  endif !(partition_tab)

               enddo ! (loop over all grid points, for empty batches)

           end if  ! end if (n_compute.gt.0)
        ! end if ! end distribution over threads
     end do ! end loop over grid batches

     ! Get work time and total time after barrier
     time_work = time_work + mpi_wtime()-time0
     call mpi_barrier(mpi_comm_global,info)
     time_all  = time_all  +mpi_wtime()-time0

     !--------------------------------- end go over the grids ------------------


  if(use_batch_permutation > 0) then

    do i_spin = 1, n_spin
      call permute_point_array_back(n_bp,1,delta_rho_KS(:,i_spin),&
           delta_rho_KS_std(:,i_spin))
    enddo


    deallocate(delta_rho_KS)

    deallocate(ins_idx)

  endif

  !---------- finally, deallocate stuff -----------------

  if (allocated( temp_rho_small       )) deallocate( temp_rho_small       )
  if (allocated( index_lm             )) deallocate( index_lm             )
  if (allocated( ylm_tab              )) deallocate( ylm_tab              )
  if (allocated( wave                 )) deallocate( wave                 )
  if (allocated( radial_wave          )) deallocate( radial_wave          )
  if (allocated( i_basis              )) deallocate( i_basis              )
  if (allocated( work                 )) deallocate( work                 )
  if (allocated( density_matrix_con   )) deallocate( density_matrix_con   )
  if (allocated( density_matrix_con_two)) deallocate( density_matrix_con_two)
  if (allocated( trigonom_tab         )) deallocate( trigonom_tab         )
  if (allocated( dir_tab              )) deallocate( dir_tab              )
  if (allocated( i_r                  )) deallocate( i_r                  )
  if (allocated( dist_tab_sq          )) deallocate( dist_tab_sq          )
  if (allocated( dist_tab             )) deallocate( dist_tab             )
  if (allocated( work_complex         )) deallocate( work_complex         )
  if (allocated( density_matrix       )) deallocate( density_matrix       )
  if (allocated( density_matrix_two   )) deallocate( density_matrix_two   )

end subroutine get_vxc_esp
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/multipole_expantion_rho
!  NAME
!    multipole_expantion_rho
!  SYNOPSIS
subroutine multipole_expantion_rho &
     ( partition_tab, rho, &
     delta_v_hartree_part_at_zero, &
     delta_v_hartree_deriv_l0_at_zero, &
     multipole_moments, multipole_radius_sq, &
     l_hartree_max_far_distance, &
     outer_potential_radius, free_rho_superpos, density_full,output_esp,&
    i_spin_esp  )

!  PURPOSE
!  computes the partitioned transition density to
!  calculate the partitioned hartree potential for esp charges. Calculates 
!  multipole expansion from partitioned transition density.
!
!  The subroutine also calls integrate_hartree_log_grid to calculate Hartree 
!  potential in the form of multipole expansion.
!
!  USES

  use dimensions,only:n_max_radial,l_pot_max,n_atoms,n_spin,n_full_points,&
      n_hartree_grid,use_distributed_spline_storage,n_my_batches,n_max_spline
  use runtime_choices,only:compensate_multipole_errors,force_hartree_log_grid,&
      communication_type,shmem_comm,output_level
  use grids,only:batches,r_angular,local_ylm_tab,lebedev_grid_index,&
      n_radial,r_radial
  use geometry,only:species
  use species_data,only:l_hartree
  use mpi_tasks,only:myid,n_tasks,check_allocation,aims_stop
  use synchronize_mpi_basic,only:sync_vector,sync_integer_vector,&
      sync_real_number
  use localorb_io,only:localorb_info,OL_norm,OL_high,use_unit
  use constants,only:hartree,pi4_inv,pi4
  use hartree_potential_storage,only:rho_multipole,rho_multipole_index,&
      original_multipole_moments,i_rho_multipole_atoms,n_rho_multipole_atoms,&
      compensation_norm,compensation_radius,total_compensated_charge,&
      get_rho_multipole_spl,get_multipole_moments_on_original_grid
  implicit none

!  ARGUMENTS

  real*8, dimension(n_full_points)                 :: partition_tab  
  real*8, dimension(n_spin, n_full_points)         :: rho
  real*8, dimension(n_atoms)                       :: &
                                                    delta_v_hartree_part_at_zero
  real*8, dimension(3,n_atoms)                     :: &
                                                delta_v_hartree_deriv_l0_at_zero
  real*8, dimension( ( l_pot_max + 1)**2, n_atoms) :: multipole_moments
  real*8, dimension(n_atoms)                       :: multipole_radius_sq
  integer, dimension(n_atoms)                      :: l_hartree_max_far_distance
  real*8, dimension(0:l_pot_max, n_atoms)          :: outer_potential_radius
  real*8, dimension(n_full_points)                 :: free_rho_superpos
  logical                                          :: density_full
  logical                                          :: output_esp
  integer                                          :: i_spin_esp
!  INPUTS
!   o  partition_tab -- values of partition function
!   o  rho -- electron density
!   o  free_rho_superpos --superposition of free atom electron densities
!   o  density_full -- switch between full and transition density
!   o  output_esp -- trigger output
!   o  i_spin_esp -- spin channel
!
!  OUTPUT
!   o  delta_v_hartree_part_at_zero --  multipole expansion of the Hartree 
!                                       potential at origin of atoms
!   o  delta_v_hartree_deriv_l0_at_zero -- derivates of  multipole expansion 
!                                   of the Hartree potential at origin of atoms
!   o  multipole_moments -- multipole moments
!   o  multipole_radius_sq -- (radius of multipole moments)**2
!   o  l_hartree_max_far_distance -- maximum l for far distance Hartree 
!                                    potential (periodic systems)
!   o  outer_potential_radius -- outer radius of multipole expansion of 
!                                Hartree potential, which needs spline. 
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE





!  rho_multipole is the angular integral over
!  Y_lm(Omega) * partition_fn(at,r,Omega) * delta_rho(r,Omega),
!  Delley 1990, Eq. (11) 
!  i.e. rho_multipole = rho_multipole(r) . For convenience


  real*8 dir_tab(3)
  real*8 temp_rho_new
  real*8 multipole_radius
  real*8 ylm_tab((l_pot_max+1)**2)
  real*8, allocatable :: current_rho_multipole_spl(:,:,:), &
                                           current_rho_multipole(:,:)

  integer l_h_dim(n_atoms)
  real*8 :: drho

!  counters

  integer i_atom
  integer i_index
  integer current_atom, current_radial, current_angular
  integer i_full_points
  integer i_spin
  integer i_my_batch
  integer i_atom_index

  integer i_l, i_m

  ! for shmem
  real*8, allocatable :: delta_v_hartree_part_spl(:,:,:)
  integer num_shm_procs, my_shm_id, n_bytes

  character*100 :: info_str



!  begin work
  if(output_esp)then
    write(info_str,'(2X,A)') " "
    call localorb_info( info_str, use_unit,'(A)', OL_norm )
    write(info_str,'(2X,A,A)') "Evaluating partitioned Hartree potential by multipole expansion."
    call localorb_info( info_str, use_unit,'(A)', OL_norm )
  endif

  allocate(current_rho_multipole((l_pot_max+1)**2, n_max_radial+2),stat=i_index)
  call check_allocation(i_index, 'current_rho_multipole')

  allocate(current_rho_multipole_spl((l_pot_max+1)**2, n_max_spline, &
           n_max_radial+2),stat=i_index)
  call check_allocation(i_index, 'current_rho_multipole_spl')


  do i_atom = 1, n_atoms, 1
    l_h_dim(i_atom) = (l_hartree(species(i_atom))+1)**2
  end do

  rho_multipole = 0.0d0
  i_full_points = 0

  do i_my_batch = 1, n_my_batches, 1
     
    do i_index = 1, batches(i_my_batch)%size, 1
           
      current_atom = batches(i_my_batch) % points(i_index) % index_atom
      current_radial = batches(i_my_batch) % points(i_index) % index_radial
      current_angular = batches(i_my_batch) % points(i_index) % index_angular

      ! run for the first time through the integration grid to tabulate 
      ! integral_1_zero_r and thus calculate integral_zero_infinity
      !
      ! meanwhile tabulate first greens-function solution of partitioned 
      ! potential and difference charge density
      
      i_full_points = i_full_points + 1

      ! execute only if partition_tab.gt.0 here, i.e. if the integration point
      ! makes sense
      !!!!! This is NOT the usual integration partition tab
      !!!!! This is the HARTREE partition tab. 
      !!!!! Thus this criterion is not the same as in all other integrations.
      !!!!! We must fix that one day.
      if (partition_tab(i_full_points).gt.0.d0) then

        ! Check if the atom has been correctly detected as mine
        i_atom_index = rho_multipole_index(current_atom)
        if(i_atom_index<=0) then
          print '(2(a,i5))','ID ',myid,' INTERNAL ERROR update_hartree_potential_p1 - need atom! Atom: ',current_atom
          call aims_stop
        endif

        ! compute atom-centered coordinates of current integration point,
        ! BUT HERE ONLY FOR PRESENT ATOM!
        dir_tab(:) = r_angular( : , current_angular, current_radial, &
                                species(current_atom))

        ylm_tab (1:l_h_dim(current_atom)) = &
             local_ylm_tab(1:l_h_dim(current_atom),current_angular, &
             lebedev_grid_index(current_radial,species(current_atom)))

        ! calculate contribution to the angular parts of the two integrals which
        ! are equal 

        ! notice that the radial and angular integration weights are already 
        ! part of  partition_tab; we need not multiply them in separately.

        ! consider only difference density for hartree potential

        temp_rho_new = 0.d0


        if (density_full)then
          do i_spin = 1, n_spin, 1
             temp_rho_new = temp_rho_new + rho(i_spin, i_full_points)
          enddo
          temp_rho_new = temp_rho_new - pi4_inv * &
                                            free_rho_superpos(i_full_points)
        else
          temp_rho_new = temp_rho_new + rho(i_spin_esp, i_full_points)
        endif

        ! implied loop over all (l,m) from (0,0) to (l_hartree,l_hartree)
        rho_multipole(1:l_h_dim(current_atom), current_radial+1, i_atom_index)=&
        rho_multipole(1:l_h_dim(current_atom), current_radial+1, i_atom_index)+&
        ylm_tab(1:l_h_dim(current_atom)) * partition_tab(i_full_points) * &
        temp_rho_new 
      end if
    end do !    end loop over a batch
  end do !     end loop over batches
  ! Add all rho_multipole contributions

  do i_atom = 1, n_atoms

    i_atom_index = rho_multipole_index(i_atom)
    if(i_atom_index > 0) then
      current_rho_multipole(:,:) = rho_multipole(:,:,i_atom_index)
    else
      current_rho_multipole(:,:) = 0.
    endif

    call sync_vector(current_rho_multipole, ((l_pot_max+1)**2)*(n_max_radial+2))

    if(i_atom_index > 0) then
      rho_multipole(:,:,i_atom_index) = current_rho_multipole(:,:)
    endif

  enddo

  ! Set boundaries on rho_multipole

  do i_atom_index = 1, n_rho_multipole_atoms

    i_atom = i_rho_multipole_atoms(i_atom_index)

    ! At Infinity (n_radial+2)
    ! enforce zero charge at infinity explicitly by a trick:
    ! (used in splines later on)
    rho_multipole(1:l_h_dim(i_atom), n_radial(species(i_atom))+2, i_atom_index)&
     = 0.d0

    ! At zero
    drho = (rho_multipole(1,2,i_atom_index) - rho_multipole(1,3,i_atom_index)) &
    &     /    (r_radial(1,species(i_atom)) - r_radial(2,species(i_atom)))

    rho_multipole(1,1,i_atom_index) = rho_multipole(1,2,i_atom_index) &
                                      - drho * r_radial(1,species(i_atom))
    rho_multipole(2:l_h_dim(i_atom),1,i_atom_index) = 0.d0

  enddo

  ! Calculate output variables
  ! the variables must be set to 0 since they are sync'd at the end
  delta_v_hartree_part_at_zero(:) = 0
  multipole_moments(:,:) = 0
  multipole_radius_sq(:) = 0
  l_hartree_max_far_distance(:) = 0
  outer_potential_radius(:,:) = 0

  original_multipole_moments(:,:) = 0.d0
  if (compensate_multipole_errors) then
     compensation_norm(:)   = 0.d0
     compensation_radius(:) = 0.d0
     total_compensated_charge = 0.d0
  end if

  do i_atom = 1, n_atoms

    if (mod(i_atom-1,n_tasks) == myid) then

      if (compensate_multipole_errors) then
         call get_multipole_moments_on_original_grid(i_atom)
      end if

      call get_rho_multipole_spl(current_rho_multipole_spl, i_atom)

      if (force_hartree_log_grid) then

        call integrate_hartree_log_grid &
                (i_atom, current_rho_multipole_spl, &
                delta_v_hartree_part_at_zero(i_atom), &
                delta_v_hartree_deriv_l0_at_zero(1:3,i_atom), &
                multipole_moments(1,i_atom), &
                multipole_radius, &
                l_hartree_max_far_distance(i_atom), &
                outer_potential_radius (0:l_pot_max, i_atom) )

        multipole_radius_sq(i_atom) = multipole_radius**2           

      else
           
        write(use_unit,*) 'Only log integrals are supported'
        stop

      end if

!     end distribution over threads
    end if

!     end loop over atoms 
  end do

  ! synchronize results

  call sync_vector(delta_v_hartree_part_at_zero,n_atoms)
  call sync_vector(delta_v_hartree_deriv_l0_at_zero,3*n_atoms)
  call sync_vector(multipole_moments,(l_pot_max+1)**2*n_atoms)
  call sync_vector(multipole_radius_sq,n_atoms)
  call sync_integer_vector(l_hartree_max_far_distance,n_atoms)
  call sync_vector(outer_potential_radius,(l_pot_max+1)*n_atoms)

  if (compensate_multipole_errors) then
     call sync_vector(original_multipole_moments,(l_pot_max+1)**2*n_atoms)

     call sync_vector(compensation_norm,n_atoms)
     call sync_vector(compensation_radius,n_atoms)
     call sync_real_number(total_compensated_charge)

     write(info_str,'(2X,A,E14.6)') "| Original multipole sum: apparent total charge = ", &
       - sum( original_multipole_moments(1,1:n_atoms)) * sqrt(pi4)
     call localorb_info(info_str,use_unit,'(A)', OL_norm )

     write(info_str,'(2X,A,E14.6)') "| Sum of charges compensated after spline to logarithmic grids = ", &
       - total_compensated_charge * sqrt(pi4)
     call localorb_info(info_str,use_unit,'(A)', OL_norm )
  end if

  if(communication_type.eq.shmem_comm) then

    if(use_distributed_spline_storage) then
      ! We need complete rho_multipole array (well, not really complete ...)
      print *,'use_distributed_spline_storage and shmem communication can not be used together'
      call aims_stop
    endif

    call aims_shm_n_procs(num_shm_procs)
    call aims_shm_myid(my_shm_id)

    allocate(delta_v_hartree_part_spl((l_pot_max+1)**2, 2, n_hartree_grid))

    ! Calculate spline coefficients and put them into shared memory

    n_bytes = (l_pot_max+1)**2 * 2 * n_hartree_grid * 8
    do i_atom = 1,n_atoms
      if( mod(i_atom-1,num_shm_procs) == my_shm_id) then
        call get_rho_multipole_spl(current_rho_multipole_spl, i_atom)
        call integrate_delta_v_hartree( &
             current_rho_multipole_spl, &
             delta_v_hartree_part_spl, 2, i_atom)
        call aims_shm_put(delta_v_hartree_part_spl, (i_atom-1)*n_bytes, n_bytes)
      endif
    enddo

    deallocate(delta_v_hartree_part_spl)

  endif
  !multipole_moments(1,1:n_atoms) = multipole_moments(1,1:n_atoms)-1.0/sqrt(pi4)
  !Now that we have all multipole components (after synchronisation of all 
  !MPI threads), output a checksum of all extrapolated point charges in the 
  !Hartree potential

  if(output_esp)then
     write(info_str,'(2X,A)') "| "
     call localorb_info(info_str,use_unit,'(A)')
     write(info_str,'(2X,A)') "| Multipole moments used in the electrostatic potential : "
     call localorb_info(info_str,use_unit,'(A)')
     write(info_str,'(2X,A)') "| What we write is -moment*sqrt(pi/4) , which gives charges in e etc."
     call localorb_info(info_str,use_unit,'(A)')
     do i_atom = 1, n_atoms, 1
        write(info_str,'(2X,A)') "| "
        call localorb_info(info_str,use_unit,'(A)')
        write(info_str,'(2X,A,1X,I8,A)') '| Atom ',i_atom, ":   l   m          moment"
        call localorb_info(info_str,use_unit,'(A)')
        i_index = 0
        do i_l = 0, l_hartree(species(i_atom)), 1
           do i_m = -i_l, i_l, 1
              i_index = i_index+1
              write(info_str,'(2X,A,16X,I3,1X,I3,1X,F15.8)') '| ',i_l,i_m,-multipole_moments(i_index,i_atom)*sqrt(pi4)
              call localorb_info(info_str,use_unit,'(A)')
           end do
        end do
     end do
     write(info_str,'(2X,A)') " "
     call localorb_info(info_str,use_unit,'(A)')
  end if


  deallocate(current_rho_multipole)
  deallocate(current_rho_multipole_spl)

end subroutine multipole_expantion_rho
!******
!---------------------------------------------------------------------------
!****s* esp_cahrges/sum_up_potential
!  NAME
!    sum_up_potential
!  SYNOPSIS
subroutine sum_up_potential &
     ( delta_v_hartree_part_at_zero, multipole_moments, &
     partition_tab_std, potential_std, multipole_radius_sq, &
     l_hartree_max_far_distance,  &
     outer_potential_radius, free_hartree_superpos_std, density_full,&
     output_esp,use_dip_for_cube)

!  PURPOSE
!    The subroutine sums up the hartree potential from the multipole components
!    which have been calculated before hand. The actual summation is done here.
!
!  USES

  use dimensions,only:n_max_spline,n_max_radial,n_hartree_grid,n_periodic,&
      n_occ_atoms,n_atoms,l_pot_max,n_centers_hartree_potential
  use constants,only:hartree
  use runtime_choices,only:analytic_potential_average,&
      use_hartree_non_periodic_ewald,&
      use_dipole_correction,calculate_work_function,dipole_correction_method,&
      force_new_functional,communication_type,shmem_comm,&
      analytic_potential_average,&
      vacuum_z_level,out_vacuum_potential,out_vacuum_potential_z_grid,&
      out_vacuum_potential_z1,out_vacuum_potential_z2,&
      out_vacuum_potential_x_grid,&
      out_vacuum_potential_y_grid
  use geometry,only:empty,species,coords,cell_volume
  use species_data,only:species_pseudoized,l_hartree
  use grids,only:n_grid,n_radial,r_grid_min!!!!!!!!!!
  use spline,only:spline_vector_v2
  use mpi_tasks,only:myid,mpi_wtime,n_tasks,mpi_comm_global,check_allocation
  use mpi_utilities,only:task_list
  use synchronize_mpi_basic,only:sync_real_number
  use pbc_lists,only:centers_hartree_potential,center_to_atom,species_center
  use synchronize_mpi,only:sync_average_potential
  use localorb_io,only:localorb_info,OL_norm
  use hartree_non_periodic_ewald,only:&
      interpolate_extended_hartree_non_periodic_ewald,&
      calculate_extended_hartree_non_periodic_ewald
  use hartree_potential_real_p0,only:hartree_potential_real_coeff,&
      update_outer_radius_l,far_distance_hartree_fp_periodic_single_atom,&
      far_distance_hartree_fp_cluster_single_atom_p2,&
      far_distance_real_hartree_potential_single_atom_p2,&
      far_distance_real_hartree_potential_single_atom
  use hartree_potential_recip,only:update_hartree_potential_recip,&
      evaluate_hartree_recip_coef,&
      evaluate_potential_at_vacuum,evaluate_dipole_correction_from_dipole,&
      evaluate_dipole_correction
  use load_balancing,only:get_batch_weights,batch_perm,use_batch_permutation,&
      permute_point_array_back,set_batch_weights
  use esp_grids
  ! rho_multipole from hartree_potential_storage conflicts with the local 
  ! variable here, so just only use get_rho_multipole_spl() from this module.
  ! Maybe the local variable name should be changed ...
  use hartree_potential_storage, only : get_rho_multipole_spl

  implicit none

!  ARGUMENTS


  real*8, dimension(n_atoms)                     :: delta_v_hartree_part_at_zero
  real*8, dimension( ( l_pot_max+1)**2, n_atoms) :: multipole_moments

  real*8, target, dimension(n_full_points_esp)        :: partition_tab_std
  real*8, target, dimension(n_full_points_esp)        :: potential_std

  real*8, dimension(n_atoms)              :: multipole_radius_sq
  integer, dimension( n_atoms)            :: l_hartree_max_far_distance
  real*8, dimension(0:l_pot_max, n_atoms) :: outer_potential_radius
  real*8, target, dimension(n_full_points_esp)      :: free_hartree_superpos_std
  logical                                 :: density_full
  logical                                 :: output_esp
  logical                                 :: use_dip_for_cube
!  INPUTS
! o delta_v_hartree_part_at_zero -- Hartree potential at origin of the atoms 
!                                   from different multipole components
! o delta_v_hartree_deriv_l0_at_zero -- Derivative of Hartree potential at 
!                                      origin of the atoms
! o multipole_moments -- mutlpole moments of the Hartree potential
! o partition_tab_std -- values of partition function
! o rho_std -- electron density
! o free_hartree_superpos_std -- superposition of the free atoms Hartree 
!                                potential
! o multipole_radius_sq -- outer radius of multipole components
! o l_hartree_max_far_distance -- maximum l-components of for the far distance 
!   Hartree potential (periodic systems)
! o density_full -- switch between full and transition density
! o output_esp -- trigger output
!
!  OUTPUT
! o potential_std -- Hartree potential
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE

  ! work arrays
  real*8, allocatable :: delta_v_hartree(:)
  !real*8, allocatable :: v_hartree_free(:)
  real*8, allocatable :: rho_multipole(:)

  !  local variables

  integer index_lm(-l_pot_max:l_pot_max, 0:l_pot_max )

  real*8 coord_current(3)
  real*8 dist_tab_sq
  real*8 dist_tab_in
  real*8 dist_tab_out
  real*8 dir_tab(3)
  real*8 dir_tab_in(3)
  real*8 trigonom_tab(4)
  real*8 i_r
  real*8 i_r_log
  real*8 ylm_tab((l_pot_max+1)**2)

  !     for spline_vector
  real*8, dimension((l_pot_max+1)**2) :: delta_v_hartree_multipole_component
  integer l_h_dim

  integer, parameter :: n_coeff_hartree = 2 ! Number of spline coeffs for 
                                            ! current_delta_v_hart_part_spl

  real*8, dimension(:,:,:), allocatable :: current_rho_multipole_spl
  real*8, dimension(:,:,:), allocatable :: current_delta_v_hart_part_spl

  real*8, dimension(:),allocatable :: adap_outer_radius_sq

!
  real*8 delta_v_hartree_aux
  !real*8 v_hartree_free_aux

  ! In periodic systems (only), this will be the correct potential zero for 
  ! the electrostatic potential 
  ! The average electrostatic potential from the previous iteration will be 
  ! saved, as the current "chemical potential" (the Fermi level) was shifted by 
  ! the average electrostatic potential from the previous iteration.
  real*8 :: average_delta_v_hartree_real
  real*8, save :: previous_average_delta_v_hartree_real = 0.d0

  ! Optionally, we can compute the average electrostatic potential of the 
  ! real-space part (i.e., the actual electrostatic potential plus the Ewald 
  ! compensating potential) analytically based on the logarithmic grids of the 
  ! free atoms. The following variables are used for this purpose.
  logical, dimension(:), allocatable :: have_atom_average_es_pot
  real*8,  dimension(:), allocatable :: atom_average_es_pot
  

  integer :: current_spl_atom
  integer :: current_center
  integer :: atom_of_splines

  integer :: l_atom_max

  !  counters

  integer i_atom_2
  integer i_center
  integer i_batch
  integer i_l
  integer i_m
  integer i_coord
  integer i_index
  integer i_lm

  integer :: i_full_points

  !  external functions
  real*8, external :: ddot
  character*100 :: info_str

  integer :: info


  real*8:: out_vacuum_potential_z, dip_gradient, dip_origin, dip_lenght, &
           dip_coord_current
  integer:: i_z

   integer n_bytes

  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) 
                                              ! Pointer to batches actually used

  ! Pointers to the actually used array

  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)
  real*8, pointer :: free_hartree_superpos(:)

  integer n_bp

  ! Timing
  real*8, allocatable :: batch_times(:)
  real*8 time_start

  ! Timings for analyzing work imbalance
  real*8 time0, time_work, time_all

  if (output_esp) then
    if(use_batch_permutation > 0) then
      call localorb_info("Summing up the Hartree potential with load balancing.", use_unit,'(2X,A)', OL_norm )
    else
      call localorb_info("Summing up the Hartree potential.", use_unit,'(2X,A)', OL_norm )
    endif
  endif
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

   ! batches_work => batch_perm(n_bp)%batches
    partition_tab => batch_perm(n_bp)%partition_tab

    !allocate(rho(n_spin,n_full_points_work))
    !call permute_point_array(n_bp,n_spin,rho_std,rho)

    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
    free_hartree_superpos => free_hartree_superpos_std
  endif

  if(get_batch_weights) then
    allocate(batch_times(n_my_batches_work))
    batch_times(:) = 0
  endif

  !-----------------------------------------------------------------------------

  allocate(delta_v_hartree(n_full_points_work),stat=info)
  call check_allocation(info, 'delta_v_hartree')

  !allocate(v_hartree_free(n_full_points_work),stat=info)
  !call check_allocation(info, 'v_hartree_free')

  allocate(rho_multipole(n_full_points_work),stat=info)
  call check_allocation(info, 'rho_multipole')
  
  allocate(adap_outer_radius_sq(n_atoms),stat=info)
  call check_allocation(info, 'adap_outer_radius_sq          ')

  if (.not.allocated(current_rho_multipole_spl)) then
     allocate(current_rho_multipole_spl &
          ((l_pot_max+1)**2, n_max_spline, n_max_radial+2),stat=info)
     call check_allocation(info, 'current_rho_multipole_spl     ')

  end if

  if (.not.allocated(current_delta_v_hart_part_spl)) then
     allocate(current_delta_v_hart_part_spl &
          ((l_pot_max+1)**2, n_coeff_hartree, n_hartree_grid),stat=info)
     call check_allocation(info, 'current_delta_v_hart_part_spl ')
     current_delta_v_hart_part_spl = 0.d0
  end if

  if (analytic_potential_average) then
     allocate(have_atom_average_es_pot(n_atoms))
     allocate(atom_average_es_pot(n_atoms))
     have_atom_average_es_pot = .false.
     atom_average_es_pot = 0.d0
  end if

  i_index = 0
  do i_l = 0, l_pot_max, 1
     do i_m = -i_l, i_l
        i_index = i_index + 1
        index_lm(i_m, i_l) = i_index
     enddo
  enddo


  call hartree_potential_real_coeff(index_lm, multipole_moments, &
       l_hartree_max_far_distance, n_centers_hartree_potential )

  if ( n_periodic > 0 .or. use_hartree_non_periodic_ewald ) then
     call update_outer_radius_l( outer_potential_radius, multipole_moments, &
                                 multipole_radius_sq, & 
                                 l_hartree_max_far_distance, index_lm)

     if ( .not. use_hartree_non_periodic_ewald ) then
        do i_atom_2 = 1, n_atoms
           adap_outer_radius_sq(i_atom_2) = maxval(&
                                             outer_potential_radius(:,i_atom_2))
        end do
     else
        adap_outer_radius_sq(:) = multipole_radius_sq(:)
     end if

  else
     adap_outer_radius_sq = 1e8    
  end if



  if(n_periodic > 0)then
     call evaluate_hartree_recip_coef( index_lm, multipole_moments, &
                                       l_hartree_max_far_distance , .false. )
  else if (use_hartree_non_periodic_ewald) then
     call calculate_extended_hartree_non_periodic_ewald( &
          l_hartree_max_far_distance, multipole_radius_sq, adap_outer_radius_sq)
  end if

  ! Initialize potential (which is an output variable and runs over all grid 
  ! points
  potential   = 0.d0

  
  dip_gradient = 0
  dip_origin = 0
  dip_lenght = 0

  if(n_periodic > 0  )then
     
     if((use_dipole_correction .or. calculate_work_function).and.&
         use_dip_for_cube)then
        if(dipole_correction_method=='potential') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via potential gradient'
             endif
             call evaluate_dipole_correction & 
            ( previous_average_delta_v_hartree_real, dip_gradient, dip_origin, &
              dip_lenght,.true. )
        elseif(dipole_correction_method=='dipole') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via slab dipole moment'
             endif
           call evaluate_dipole_correction_from_dipole &
           (previous_average_delta_v_hartree_real, dip_gradient,dip_origin,&
            dip_lenght,.true.)
        endif
     end if
  end if
 

  ! There are two separate loops over integration points:
  ! 1) A loop over the integration grid to tabulate the multipole components and 
  !    densities at each grid point one by one
  ! 2) A second loop over the grid to integrate over products 
  !    rho_multipole(r)*v_multipole(r)

  ! initialize the physical quantities that are tabulated over the entire 
  ! integration grid
  rho_multipole    = 0.d0
  delta_v_hartree  = 0.d0
  !v_hartree_free   = 0.d0

  ! The following averages are only needed in periodic systems
  average_delta_v_hartree_real = 0.d0


  call mpi_barrier(mpi_comm_global,info) ! Barrier is for correct timing!!!
  time0 = mpi_wtime()

    ! First loop over the grid: We run over the Hartree potential center by 
    ! center, adding the Hartreecontribution of that atom at each point of 
    ! the integration grid

    atom_of_splines = 0
    do i_center = 1, n_centers_hartree_potential, 1

      current_center   = centers_hartree_potential(i_center)
      current_spl_atom = center_to_atom(current_center)

      if (n_periodic > 0 .or. force_new_functional)then
        ! in this case, use multipole_spline to compute
        ! Hartree potential components ON each individual atomic nucleus, 
        ! before doing anything else.

        if (mod(current_spl_atom-1,n_tasks) == myid) then
          ! for this case rho_multipole for current_spl_atom is on myid

          do i_atom_2 = 1, n_atoms,1


            ! reinitialize the physical quantities of interest
            delta_v_hartree_aux = 0.d0


            ! get current integration point coordinate
            do i_coord = 1, 3, 1
              coord_current(i_coord) = coords(i_coord, i_atom_2)
            end do

            ! Tabulate distances and directions to all atoms -
            ! including relative positions on logarithmic and
            ! radial integration grids.

            call tab_single_atom_centered_coords_p0 &
                 ( current_center, coord_current,  &
                 dist_tab_sq,  &
                 dir_tab )

            if (i_atom_2.eq. current_center ) then
              dist_tab_sq = r_grid_min(species(i_atom_2))**2 + 1e-15
            end if

            ! At each integration point, the Hartree potential components coming 
            ! from different atoms are split into two groups:
            ! Any components coming from atoms close by are evaluated by 
            ! explicit numerical splines; far away atoms are simply represented 
            ! by an analytical long-distance multipole potential.
            if (dist_tab_sq.lt.multipole_radius_sq(current_spl_atom)) then
               ! begin with everything related to n_atoms_in ...

               if(current_spl_atom /= atom_of_splines) then
                 call get_rho_multipole_spl(current_rho_multipole_spl, &
                      current_spl_atom)

                 if(communication_type.eq.shmem_comm) then
                   n_bytes = (l_pot_max+1)**2 * n_coeff_hartree * &
                              n_hartree_grid * 8
                   call aims_shm_get(current_delta_v_hart_part_spl, &
                        (current_spl_atom-1)*n_bytes, n_bytes)
                 else
                   call integrate_delta_v_hartree( current_rho_multipole_spl, &
                        current_delta_v_hart_part_spl, &
                        n_coeff_hartree, current_spl_atom )
                 endif
                 atom_of_splines = current_spl_atom
                 ! This is the first time that we have the actual splined 
                 ! Hartree potential components that will be used below. If we
                 ! are to compute the average electrostatic potential
                 ! analytically (based on the actual splines on the logarithmic
                 ! grid) we must do it here.
                 if (analytic_potential_average) then
                    if (.not.have_atom_average_es_pot(current_spl_atom)) then
                       call integrate_average_atom_potential &
                          (current_delta_v_hart_part_spl,n_coeff_hartree, &
                           multipole_radius_sq(current_spl_atom),&
                           adap_outer_radius_sq(current_spl_atom), &
                           multipole_moments(1,current_spl_atom), &
                           atom_average_es_pot(current_spl_atom),&
                           current_spl_atom )
                       have_atom_average_es_pot(current_spl_atom) = .true.
                    end if
                 end if
               endif


               call tab_single_atom_centered_coords_radial_log_p0 &
                    ( current_center, dist_tab_sq, dir_tab,  &
                    dist_tab_in, i_r, i_r_log, dir_tab_in )

               ! for all inner atoms, we need ylm functions and their gradients 
               ! explicitly
               call tab_single_trigonom_p0(dir_tab_in, trigonom_tab)

               ! Now loop over all inner atoms (those which are close enough
               ! to the current integration point so we need explicit numerical 
               ! splines) partitioned
               !     hartree potentials need to be summed up
               !     according to Delley (eq. 12c)

               l_h_dim = (l_hartree(species_center(current_center))+1)**2

               ! obtain spline-interpolated values of the multipole components
               ! of the partitioned Hartree potential, splined on the 
               ! logarithmic integration grid

               call spline_vector_v2 &
                    ( i_r_log, &
                    current_delta_v_hart_part_spl, &
                    (l_pot_max+1)**2, n_coeff_hartree, n_hartree_grid, &
                    n_grid(species_center(current_center)), &
                    l_h_dim, &
                    delta_v_hartree_multipole_component)

               if (i_atom_2.eq. current_center ) then
                  delta_v_hartree_multipole_component(1) =  &
                  delta_v_hartree_part_at_zero(current_spl_atom)

                  do i_lm = 2, l_h_dim


                     delta_v_hartree_multipole_component(i_lm) = 2* &
                     current_delta_v_hart_part_spl(i_lm,1,1) &
                          - current_delta_v_hart_part_spl(i_lm,1,2)

                  end do
               end if

               ! sum up the Hartree potential contribution from the present 
               ! i_atom_in
               delta_v_hartree_aux = delta_v_hartree_aux + &
                    ddot ( l_h_dim, delta_v_hartree_multipole_component, 1, &
                    ylm_tab, 1 )
               

               if ( n_periodic > 0 .or. use_hartree_non_periodic_ewald ) then
                  call far_distance_hartree_Fp_periodic_single_atom &
                       (current_spl_atom, i_center, &
                       dist_tab_in, l_hartree_max_far_distance, .true., &
                       .false., multipole_radius_sq(current_spl_atom),  &
                       sqrt( adap_outer_radius_sq(current_spl_atom) )   )
               end if


            else if (dist_tab_sq.lt. adap_outer_radius_sq(current_spl_atom) )&
                                                                           then


! VB: FIXME - is this correct for the cluster case? any dist_tab_sq should be 
!             the right one for the cluster case!

               ! Tabulate distances only for outer part
               dist_tab_out = sqrt(dist_tab_sq)

               if ( n_periodic ==0 .and. .not. use_hartree_non_periodic_ewald )&
                                                                            then


                  ! Recursively tabulate the radial behavior of all multipole 
                  ! components These radial functions are a private variable in
                  ! module hartree_potential_real.f90, and are reused there 
                  ! later in subroutine 
                  ! far_distance_hartree_potential_real_single_atom
                  call far_distance_hartree_Fp_cluster_single_atom_p2 &
                       ( dist_tab_out, &
                       l_hartree_max_far_distance( current_spl_atom), .false. )

                  call far_distance_real_hartree_potential_single_atom_p2 &
                       ( current_spl_atom, delta_v_hartree_aux, &
                       l_hartree_max_far_distance(current_spl_atom), &
                       coord_current )



               else
                  ! Now sum up the potential contributions from all far-field 
                  ! atoms (analytical multipole potentials only ...)
                  call far_distance_hartree_Fp_periodic_single_atom &
                       (current_spl_atom, i_center, &
                       dist_tab_out, l_hartree_max_far_distance, .false. ,&
                       .false., multipole_radius_sq(current_spl_atom),    &
                       sqrt( adap_outer_radius_sq(current_spl_atom) )     )

               end if
            end if ! multipole radius


            if (dist_tab_sq.lt. max(adap_outer_radius_sq(current_spl_atom), &
                 multipole_radius_sq(current_spl_atom)) )then


               if ( n_periodic > 0 .or. use_hartree_non_periodic_ewald ) then


                  ! Now sum up the far distance parts of the Hartree potential
                  call far_distance_real_hartree_potential_single_atom &
                       ( current_center, i_center, delta_v_hartree_aux, &
                       l_hartree_max_far_distance, coord_current )
               end if
            end if

          end do ! loop over atoms
        end if ! parallelization over current_spl_atom
      end if ! (((n_periodic > 0) .or. force_new_functional) .and. i_iter == 1 )

      !
      ! Now follows the real work: Summing the multipole potential, density, 
      ! and their derivatives on the integration grid
      !

      ! Reset grid counter for current Hartree potential center
      i_full_points = 0

      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the entire
          ! grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          if (partition_tab(i_full_points).gt.0.d0) then

            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
            coords_esp(:)
            ! Tabulate distances and directions to a single atom -
            ! including relative positions on logarithmic and
            ! radial integration grids.

            call tab_single_atom_centered_coords_p0 &
                 ( current_center, &
                 coord_current,  &
                 dist_tab_sq,  &
                 dir_tab )

            ! VB: For uniformity, determine the maximum angular momentum
            ! required for the present atom  right here

            l_atom_max = l_hartree(species(current_spl_atom))
            do while ( (outer_potential_radius(l_atom_max, current_spl_atom) &
                       .lt. dist_tab_sq ) .and. (l_atom_max.gt.0) ) 
              l_atom_max = l_atom_max - 1
            enddo

            ! At each integration point, the Hartree potential components coming 
            ! fromdifferent atoms are split into two groups:
            ! Any components coming from atoms close by are evaluated by 
            ! explicit numerical splines; far away atoms are simply represented 
            ! by an analytical long-distance multipole potential.

            ! We treat first the atoms close by
            if (dist_tab_sq.lt.multipole_radius_sq(current_spl_atom) ) then
              if(current_spl_atom /= atom_of_splines) then
                call get_rho_multipole_spl(current_rho_multipole_spl, &
                     current_spl_atom)

                if(communication_type.eq.shmem_comm) then
                  n_bytes = (l_pot_max+1)**2 * n_coeff_hartree * &
                  n_hartree_grid * 8
                  call aims_shm_get(current_delta_v_hart_part_spl, &
                       (current_spl_atom-1)*n_bytes, n_bytes)
                else
                  call integrate_delta_v_hartree( current_rho_multipole_spl, &
                       current_delta_v_hart_part_spl, &
                       n_coeff_hartree, current_spl_atom )
                endif
                atom_of_splines = current_spl_atom
              endif


              call tab_single_atom_centered_coords_radial_log_p0 &
                   ( current_center, dist_tab_sq, dir_tab,  &
                   dist_tab_in, i_r, i_r_log, dir_tab_in )


              ! for an inner atom we need ylm functions and their gradients 
              ! explicitly

              call tab_single_trigonom_p0(dir_tab_in, trigonom_tab)

              call tab_single_wave_ylm_p2 &
                     ( trigonom_tab, l_atom_max,  &
                     l_pot_max, ylm_tab)

              ! For an inner atoms (those which are close enough
              ! to the current integration point so we need explicit numerical 
              ! splines) partitioned hartree potentials need to be summed up
              ! according to Delley (eq. 12c)
              l_h_dim = (l_atom_max + 1)**2

              ! obtain spline-interpolated values of the multipole components
              ! of the partitioned Hartree potential, splined on the logarithmic
              ! integration grid
              call spline_vector_v2 &
                   ( i_r_log, &
                   current_delta_v_hart_part_spl, &
                   (l_pot_max+1)**2, n_coeff_hartree, n_hartree_grid, &
                   n_grid(species_center(current_center)), &
                   l_h_dim, &
                   delta_v_hartree_multipole_component)

              ! sum up the Hartree potential contribution from the present inner
              ! atom
              delta_v_hartree_aux = &
                       ddot ( l_h_dim, delta_v_hartree_multipole_component, 1, &
                       ylm_tab, 1 )

              delta_v_hartree(i_full_points) = &
                       delta_v_hartree(i_full_points) + delta_v_hartree_aux

              ! far-distance treatment for then center i_center for the periodic
              ! system
              if ( n_periodic > 0 .or. use_hartree_non_periodic_ewald ) then

                !  VB: FIXME!!!!! 
                !      I kept the original call here because I was not sure I 
                !      was doing the completely right thing. We should cut the 
                !      periodic part down to l_atom_max as well below,
                !      but that requires a change to the periodic infrastructure 
                !      first!
                !
                call far_distance_hartree_Fp_periodic_single_atom &
                      (current_spl_atom, i_center, &
                      dist_tab_in, l_hartree_max_far_distance, .true., .false.,&
                      multipole_radius_sq(current_spl_atom),                   &
                      sqrt( adap_outer_radius_sq(current_spl_atom) )           )

                ! VB: Paula - this is what the function call should look like 
                !             but the function needs to be changed!
                !             call far_distance_hartree_Fp_periodic_single_atom&
                !                        (i_center, &
                !                     dist_tab_in, l_atom_max, .true., .false. )
              end if


            else if (dist_tab_sq .lt. adap_outer_radius_sq(current_spl_atom)) &
                                                                            then
              ! the current center is in the far-distance part-----------------

              ! Tabulate distance for the outer part
              dist_tab_out = sqrt(dist_tab_sq)

              ! Now sum up the potential contributions from a far-field atom
              ! (analytical multipole potentials only ...)
              if ( n_periodic == 0 .and. .not. use_hartree_non_periodic_ewald )&
                                                                            then

                ! Recursively tabulate the radial behavior of all multipole 
                ! components These radial functions are a private variable in 
                ! module hartree_potential_real.f90, and are reused there later 
                ! in subroutine far_distance_hartree_potential_real_single_atom
                call far_distance_hartree_Fp_cluster_single_atom_p2 &
                     ( dist_tab_out, &
                     l_atom_max, .false. )

                  call far_distance_real_hartree_potential_single_atom_p2 &
                         ( i_center, delta_v_hartree(i_full_points), &
                         l_atom_max, coord_current )

              else ! Periodic system or non-periodic Ewald method


                call far_distance_hartree_Fp_periodic_single_atom &
                     (current_spl_atom, i_center, &
                     dist_tab_out, l_hartree_max_far_distance, .false.,.false.,&
                     multipole_radius_sq(current_spl_atom),                    &
                     sqrt( adap_outer_radius_sq(current_spl_atom) )            )

                ! Note: 'far_distance_real_hartree_potential_single_atom' is 
                ! called in the periodic systems later, because then we have Fp 
                ! in inner and outer multipole radius.

              end if  ! n_periodic == 0 .and. .not. 
                      ! use_hartree_non_periodic_ewald

            end if  ! end if for separating current_center either 
                    ! far-distance or near part


            ! Now sum up the far distance part of the Hartree potential if 
            ! Ewald's decomposition is performed. In the opposite case, this 
            ! was already done.
            if ( n_periodic > 0 .or. use_hartree_non_periodic_ewald ) then


              if (dist_tab_sq.lt. &
                   max(adap_outer_radius_sq(current_spl_atom), &
                   multipole_radius_sq(current_spl_atom))) then

                ! Far distance analytic real-space part of the potential

                ! VB: FIXME: l_atom_max not implemented here although it 
                !            should be!!

                  call far_distance_real_hartree_potential_single_atom &
                          ( current_center, i_center, &
                            delta_v_hartree(i_full_points), &
                            l_hartree_max_far_distance, coord_current )

              end if

            end if ! end periodic system



            ! Force calculations in second iteration

    
          end if ! end if (partition_tab.gt.0.d0)
        end do  ! end loop over points in a batch
        if(get_batch_weights) batch_times(i_batch) = batch_times(i_batch) + &
           mpi_wtime() - time_start
      end do ! end loop over batches
    end do  ! end loop over source atoms

      ! next, we integrate the actual energy and force quantities

      i_full_points = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1
          if (partition_tab(i_full_points).gt.0.d0) then

            ! the reciprocal space contribution needs to be treated separately
            ! from the centers above
            if (n_periodic.gt.0) then
              ! get current integration point coordinate
              coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                                 coords_esp(:)

              ! before the reciprocal-space component is added, average over 
              ! the real-space part of  delta_v_hartree
              average_delta_v_hartree_real = average_delta_v_hartree_real + &
              delta_v_hartree(i_full_points) * partition_tab(i_full_points)

              call update_hartree_potential_recip &
                      ( coord_current, delta_v_hartree(i_full_points) )
              

              if( use_dipole_correction.and.use_dip_for_cube)then

                i_m = int(floor((coord_current(3) - vacuum_z_level)/dip_lenght))
                dip_coord_current = coord_current(3) - i_m * dip_lenght
                  
                if( dip_coord_current <  vacuum_z_level)then
                  dip_coord_current = dip_coord_current + dip_lenght
                end if
      
                delta_v_hartree(i_full_points) = delta_v_hartree(i_full_points)&
                       -  (dip_coord_current-dip_origin) * dip_gradient

              end if

            else if ( use_hartree_non_periodic_ewald ) then
              call interpolate_extended_hartree_non_periodic_ewald(            &
                              coord_current(:),  &
                              delta_v_hartree(i_full_points)                   )
            end if ! n_periodic > 0

            if (density_full) then
              potential(i_full_points) = &
                 delta_v_hartree(i_full_points) + &
                 free_hartree_superpos(i_full_points)
            else
              potential(i_full_points) = &
                 delta_v_hartree(i_full_points)
            endif
            !v_hartree_free(i_full_points) = &
            !     v_hartree_free(i_full_points) + &
            !     free_hartree_superpos(i_full_points)

          end if ! if (partition_tab(i_full_points).gt.0)

        end do ! end loop over a batch
        if(get_batch_weights) batch_times(i_batch) = batch_times(i_batch) + &
           mpi_wtime() - time_start
      end do ! end loop over batches



  ! Get work time and total time after barrier
  time_work = mpi_wtime()-time0
  call mpi_barrier(mpi_comm_global,info)
  time_all = mpi_wtime()-time0
  call sync_real_number(time_work)
  call sync_real_number(time_all)
  if (output_esp) then
    write(info_str,'(a,2(f12.3,a))') '  Time summed over all CPUs for potential: real work ', &
     time_work,' s, elapsed ',time_all,' s'
    call localorb_info(info_str, use_unit, "(A)", OL_norm)
  endif


  if(get_batch_weights) then
    call set_batch_weights(n_bp,batch_times)
    deallocate(batch_times)
  endif

  if(n_periodic > 0  )then
     ! The Fourier part of the potential - in this loop, we no longer compute 
     ! anything on the grid, but rather we compute the reciprocal-space part of 
     ! the potential on each nucleus, and  add this to the energy of the nuclei 
     ! in the Hartree potential of the electrons.

     do i_atom_2 = 1, n_occ_atoms, 1
        if (myid.eq.task_list(i_atom_2)) then

           ! reinitialize the physical quantities of interest
           delta_v_hartree_aux = 0.d0

           ! get current nucleus coordinate
           coord_current(:) = coords(:, i_atom_2)

           delta_v_hartree_aux     = 0.0d0

           call update_hartree_potential_recip( coord_current, &
                                                delta_v_hartree_aux )

           if( use_dipole_correction.and.use_dip_for_cube)then

              i_m = int(floor((coord_current(3) - vacuum_z_level)/dip_lenght))
              coord_current(3) = coord_current(3) - i_m * dip_lenght
              
              if( coord_current(3) <  vacuum_z_level)then
                 coord_current(3) = coord_current(3) + dip_lenght
              end if
              
              delta_v_hartree_aux = delta_v_hartree_aux -  &
                                    (coord_current(3)-dip_origin) * dip_gradient

           end if
               

        end if
     end do

  else if (use_hartree_non_periodic_ewald) then

     do i_atom_2 = 1, n_occ_atoms
        if ( myid == task_list(i_atom_2) ) then

           ! Set the following variable to zero because the potential is 
           ! _added_ to it.
           delta_v_hartree_aux = 0

           ! get current nucleus coordinate
           coord_current(:) = coords(:, i_atom_2)

           call interpolate_extended_hartree_non_periodic_ewald( coord_current,&
                delta_v_hartree_aux )

        end if
     end do

  end if  ! n_periodic > 0


  ! And finally, for periodic systems, we must remember to shift the potential
  ! zero to the average real-space potential in periodic systems, also for 
  ! energy-related quantities
  if (n_periodic.gt.0) then

      call sync_average_potential ( average_delta_v_hartree_real )

      average_delta_v_hartree_real = average_delta_v_hartree_real/cell_volume
      previous_average_delta_v_hartree_real = average_delta_v_hartree_real
      if (output_esp) then
        write (info_str,'(2X,A,1X,F15.8,A)')  &
           "| Average real-space part of the electrostatic potential :",   &
           average_delta_v_hartree_real*hartree, " eV"
        call localorb_info ( info_str, use_unit,'(A)', OL_norm )
      endif

      ! If the average electrostatic potential is really computed analytically,
      ! we replace the above value as follows:
      if (analytic_potential_average) then
         average_delta_v_hartree_real = 0.d0
         do i_atom_2 = 1,n_atoms,1
            average_delta_v_hartree_real = &
            average_delta_v_hartree_real + atom_average_es_pot(i_atom_2)
         enddo
         average_delta_v_hartree_real = average_delta_v_hartree_real/cell_volume
         previous_average_delta_v_hartree_real = average_delta_v_hartree_real
         call sync_average_potential ( average_delta_v_hartree_real )
         if (output_esp) then
           write (info_str,'(2X,A,1X,F15.8,A)')  &
           "| Analytical average real-space  electrostatic potential :",   &
           average_delta_v_hartree_real*hartree, " eV"
           call localorb_info ( info_str, use_unit,'(A)', OL_norm )
         endif
      end if
      if(density_full)then
        potential(:) = potential(:) - average_delta_v_hartree_real
      endif


       ! If requested, we can now write the average electrostatic potential 
       ! in the vacuum region of a slab calculation
       ! Notice that only the reciprocal-space part and the average real-space 
       ! potential contribute here.
       if( out_vacuum_potential)then

         write(info_str,*) ' | Potential at the vacuum (eV):'
         call localorb_info(info_str,use_unit,'(A)')    

         do i_z = 1, out_vacuum_potential_z_grid, 1

            if(out_vacuum_potential_z_grid==1) then
               out_vacuum_potential_z = out_vacuum_potential_z1
            else
              out_vacuum_potential_z = out_vacuum_potential_z1 + (i_z-1)*&
              (out_vacuum_potential_z2 - out_vacuum_potential_z1) &
              /dble(out_vacuum_potential_z_grid-1)
            endif

            call evaluate_potential_at_vacuum( out_vacuum_potential_z,  &
                 out_vacuum_potential_x_grid, &
                 out_vacuum_potential_y_grid, dip_gradient,  &
                 dip_origin, dip_lenght, average_delta_v_hartree_real)

         end do

       end if
  end if

  !! ! CC DEBUG: Restore original multipole moments 
  !! if ( AS_stress_on .and. AS_flag_read_mm_moments) then
  !!   multipole_moments = AS_mm_save
  !! end if

  if(use_batch_permutation > 0) then

    call permute_point_array_back(n_bp,1,potential,potential_std)
    deallocate(potential)


    !deallocate(rho)

  endif


  if(allocated(delta_v_hartree)) deallocate(delta_v_hartree)
  if(allocated(adap_outer_radius_sq)) deallocate(adap_outer_radius_sq)
  !if(allocated(v_hartree_free))  deallocate(v_hartree_free)
  if(allocated(rho_multipole))   deallocate(rho_multipole)

  if (allocated(current_rho_multipole_spl)) then
     deallocate(current_rho_multipole_spl)
  end if
  if (allocated(current_delta_v_hart_part_spl)) then
     deallocate(current_delta_v_hart_part_spl)
  end if

  if (allocated(have_atom_average_es_pot)) then
     deallocate(have_atom_average_es_pot)
  end if
  if (allocated(atom_average_es_pot)) then
     deallocate(atom_average_es_pot)
  end if


end subroutine sum_up_potential
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_fit
!  NAME
!    esp_fit
!  SYNOPSIS
subroutine esp_fit(potential_std,partition_tab_std,esp_charges,esp_dipole,&
                   density_full,output_esp,out_pot)

!  PURPOSE
!    Fits the esp charges centered at the atoms to the Hartree potential using
!    a least squares fit
!
!  USES
  use species_data,only:species_name
  use dimensions,only:n_atoms
  use runtime_choices,only:out_esp_full,charge,flag_esp_restraint,esp_restraint_strength,esp_restr_reference
  use load_balancing,only:use_batch_permutation,batch_perm,get_batch_weights
  use geometry,only:species,coords
  use mpi_tasks,only:myid,mpi_wtime,check_allocation
  use synchronize_mpi_basic,only:sync_vector,sync_matrix,sync_real_number,&
                                 sync_integer
  use localorb_io,only:OL_norm,localorb_info
  use esp_grids
  use physics,only:multipole_moments,hirshfeld_charge
  use constants,only:sqrt_pi
!  ARGUMENTS
!  INPUTS
!   o potential_std -- Hartree potential
!   o partition_tab_std -- partition tab
!   o output_esp -- trigger output
!  OUTPUT
!   o esp_charges -- esp charges fitted to potential
!   o esp_dipole --  dipole moment calculated from fitted charges
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none

  real*8, target, dimension(n_full_points_esp),INTENT(IN):: potential_std
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: partition_tab_std
  real*8, dimension(n_atoms),INTENT(OUT)        ::  esp_charges
  real*8, dimension(3) ,INTENT(OUT)       ::  esp_dipole
  logical,INTENT(IN)                                 :: density_full
  logical,INTENT(IN)                                 :: output_esp
  logical,INTENT(IN)                                 :: out_pot
  integer :: info
  character*100 :: info_str

  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                         ! actually used
  ! Timing
  real*8, allocatable :: batch_times(:)
  real*8 time_start
  ! Pointers to the actually used array

  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)

  integer n_bp

  real*8,  dimension(:,:), allocatable :: a
  real*8,  dimension(:), allocatable :: work
  real*8,  dimension(:), allocatable :: b
  real*8  :: di
  integer :: i_atom, j_atom
  real*8  :: total_diff, diff, Vol, pot_sum
  real*8  :: coord_current(3)
  real*8  :: qfact
  !  counters

  integer i_batch
  integer i_index
  integer :: i_full_points
  integer :: i_points
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

    partition_tab => batch_perm(n_bp)%partition_tab
    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
  endif

  if(get_batch_weights) then
    allocate(batch_times(n_my_batches_work))
    batch_times(:) = 0
  endif

  !ESP-charge fitting statrts here
  allocate(a(n_atoms+1,n_atoms+1),stat=info)
  call check_allocation(info, 'a')
  allocate(b(n_atoms+1),stat=info)
  call check_allocation(info, 'b')
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      a = 0
      b = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            do i_atom = 1, n_atoms, 1
                di=sqrt((coord_current(1)-coords(1,i_atom))**2+&
                        (coord_current(2)-coords(2,i_atom))**2+&
                        (coord_current(3)-coords(3,i_atom))**2)
!HO probably sign error in potential
                b(i_atom) = b(i_atom)-(1.0/di)*potential(i_full_points)
              do j_atom = 1, n_atoms, 1
                a(i_atom,j_atom) = a(i_atom,j_atom) + &
                                   1.0/(di*&
                   sqrt((coord_current(1)-coords(1,j_atom))**2+&
                        (coord_current(2)-coords(2,j_atom))**2+&
                        (coord_current(3)-coords(3,j_atom))**2))
              enddo
            enddo
        enddo
      enddo
            
      call sync_vector(b,n_atoms+1)
      call sync_matrix(a,n_atoms+1 ,n_atoms+1 )
      a(n_atoms+1,1:n_atoms) = 1.0d0
      a(1:n_atoms,n_atoms+1) = 1.0d0
      b(n_atoms+1) = charge

!HO rESP charges according to method one of JCP 97, 10269 with restraint strength a=1/2
      qfact=-sqrt_pi*2.0d0
      if(esp_restr_reference==2) call hirshfeld_analysis()
      if(flag_esp_restraint) then
        do i_atom = 1, n_atoms
          a(i_atom,i_atom) = a(i_atom,i_atom) + esp_restraint_strength
          if(esp_restr_reference==1) then
            b(i_atom) = b(i_atom) + qfact*esp_restraint_strength*multipole_moments(1,i_atom)
          else if(esp_restr_reference==2) then
            b(i_atom) = b(i_atom) + esp_restraint_strength*hirshfeld_charge(i_atom)
          endif
        enddo
      endif

      allocate(work(n_atoms+1),stat=info)
      call check_allocation(info, 'work')
      call dgesv(n_atoms+1, 1, a, n_atoms+1, work, b, n_atoms+1 ,info)
      if (myid==0.and.output_esp)then
            write(info_str,'(2X,A)') "| "
	    call localorb_info(info_str,use_unit,'(A)')
	    if(flag_esp_restraint) then
			write(info_str,'(2X,A)') "| rESP charges fitted to the electrostatic potential : "
		else
			write(info_str,'(2X,A)') "| ESP charges fitted to the electrostatic potential : "
		endif
	    call localorb_info(info_str,use_unit,'(A)')
	    do i_atom = 1, n_atoms, 1
		write(info_str,'(2X,A)') "| "
		call localorb_info(info_str,use_unit,'(A)')
		write(info_str,'(2X,A,1X,I8,A,A,A,F15.8)') '| Atom ', i_atom&
                   , ": ",trim(species_name(species(i_atom))),", ESP charge: ",&
                   b(i_atom)
		call localorb_info(info_str,use_unit,'(A)')
	    end do
	    write(info_str,'(2X,A)') " "
	    call localorb_info(info_str,use_unit,'(A)')
      endif

      !Calculate RMS for fitted charges
      total_diff = 0
      i_full_points = 0
      i_points = 0
      pot_sum = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the entire 
          ! grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
            i_points = i_points + 1
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            diff = 0d0
            do i_atom = 1, n_atoms, 1
                di=sqrt((coord_current(1)-coords(1,i_atom))**2+&
                        (coord_current(2)-coords(2,i_atom))**2+&
                        (coord_current(3)-coords(3,i_atom))**2)
                diff = diff + b(i_atom)/di
            enddo
              pot_sum = pot_sum + potential(i_full_points)**2
!HO probably sign error in potential
              total_diff =  total_diff + (-potential(i_full_points)-diff)**2
            if (out_pot) then
                   potential(i_full_points) = diff
            endif

        enddo
      enddo
      call sync_real_number(total_diff)
      call sync_real_number(pot_sum)
      do i_atom = 1, n_atoms, 1
       esp_charges(i_atom) = b(i_atom)
      enddo
      if ((.not.density_full.and.output_esp).or.out_esp_full) then
        esp_dipole = 0
        do i_atom = 1, n_atoms, 1
          esp_dipole(1)= esp_dipole(1) - b(i_atom)*coords(1,i_atom)
          esp_dipole(2)= esp_dipole(2) - b(i_atom)*coords(2,i_atom)
          esp_dipole(3)= esp_dipole(3) - b(i_atom)*coords(3,i_atom)
        enddo
        write(info_str,'(2X,A,1X,F15.8,F15.8,F15.8)') "Dipole matrix element: "&
                                   , esp_dipole(1), esp_dipole(2), esp_dipole(3)
        call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
        call sync_integer(i_points)
        write(info_str,'(2X,A,1X,F15.8)') "RRMS: ", sqrt(total_diff)/ &
                                                           (pot_sum)
        call localorb_info(info_str,use_unit,'(A)')
        ! Write one last line to bound output
        write(info_str,*) ' '
        call localorb_info(info_str,use_unit,'(A)',OL_norm)
      endif
  if(allocated(a)) deallocate(a)
  if(allocated(b)) deallocate(b)
  if(allocated(work)) deallocate(work)

end subroutine esp_fit
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_radius
!  NAME
!    esp_radius
!  SYNOPSIS
subroutine esp_radius(radius_esp_min, radius_esp_max, esp_min, esp_max)
!  PURPOSE
!   Generate the points where the potential will be evaluated. 
!    vdw-Radius of atoms +radius < points < vdw-Radius of atoms +radius2
!  USES
  use constants,only:bohr
  use species_data,only:species_z
  use geometry,only:species
  use dimensions,only:n_species
  use localorb_io,only:use_unit
!  ARGUMENTS
!  INPUTS
!   o esp_min -- factor for max radius used for ESP fitting
!   o esp_max -- factor for min radius used for ESP fitting
!  OUTPUT
!   o radius_esp_min -- esp_min*vdw_radius
!   o radius_esp_max -- esp_max*vdw_radius
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none
  real*8,  dimension(n_species) :: radius_esp_min
  real*8,  dimension(n_species) :: radius_esp_max
  real*8 :: esp_min
  real*8 :: esp_max
  real*8, dimension(:),       allocatable :: Atom_vdw
  integer :: i_species, info

  if(.not. allocated( Atom_vdw))then
     allocate( Atom_vdw(103),stat=info)
     if(info/=0)then
        write(use_unit,*)'Error in allocation: Atom_vdw'
        stop
     end if
  end if
  ! vdw radius for all atoms, numbers have been taken from wikipedia
  Atom_vdw(  1) = 1.10/bohr
  Atom_vdw(  2) = 1.40/bohr
  Atom_vdw(  3) = 1.82/bohr
  Atom_vdw(  4) = 1.53/bohr
  Atom_vdw(  5) = 1.92/bohr
  Atom_vdw(  6) = 1.70/bohr
  Atom_vdw(  7) = 1.55/bohr
  Atom_vdw(  8) = 1.52/bohr
  Atom_vdw(  9) = 1.47/bohr
  Atom_vdw( 10) = 1.54/bohr
  Atom_vdw( 11) = 2.27/bohr
  Atom_vdw( 12) = 1.73/bohr
  Atom_vdw( 13) = 1.84/bohr
  Atom_vdw( 14) = 2.10/bohr
  Atom_vdw( 15) = 1.80/bohr
  Atom_vdw( 16) = 1.80/bohr
  Atom_vdw( 17) = 1.75/bohr
  Atom_vdw( 18) = 1.88/bohr
  Atom_vdw( 19) = 2.75/bohr
  Atom_vdw( 20) = 2.31/bohr
  Atom_vdw( 21) = 1.00
  Atom_vdw( 22) = 1.00
  Atom_vdw( 23) = 1.00
  Atom_vdw( 24) = 1.00
  Atom_vdw( 25) = 1.00
  Atom_vdw( 26) = 1.00
  Atom_vdw( 27) = 1.00
  Atom_vdw( 28) = 1.63/bohr
  Atom_vdw( 29) = 1.40/bohr
  Atom_vdw( 30) = 1.39/bohr
  Atom_vdw( 31) = 1.87/bohr
  Atom_vdw( 32) = 2.11/bohr
  Atom_vdw( 33) = 1.85/bohr
  Atom_vdw( 34) = 1.90/bohr
  Atom_vdw( 35) = 1.85/bohr
  Atom_vdw( 36) = 2.02/bohr
  Atom_vdw( 37) = 3.03/bohr
  Atom_vdw( 38) = 2.49/bohr
  Atom_vdw( 39) = 1.00
  Atom_vdw( 40) = 1.00
  Atom_vdw( 41) = 1.00
  Atom_vdw( 42) = 1.00
  Atom_vdw( 43) = 1.00
  Atom_vdw( 44) = 1.00
  Atom_vdw( 45) = 1.00
  Atom_vdw( 46) = 1.63/bohr
  Atom_vdw( 47) = 1.72/bohr
  Atom_vdw( 48) = 1.58/bohr
  Atom_vdw( 49) = 1.93/bohr
  Atom_vdw( 50) = 2.17/bohr
  Atom_vdw( 51) = 2.06/bohr
  Atom_vdw( 52) = 2.06/bohr
  Atom_vdw( 53) = 1.98/bohr
  Atom_vdw( 54) = 2.16/bohr
  Atom_vdw( 55) = 3.43/bohr
  Atom_vdw( 56) = 2.68/bohr
  Atom_vdw( 57) = 1.00
  Atom_vdw( 58) = 1.00
  Atom_vdw( 59) = 1.00
  Atom_vdw( 60) = 1.00
  Atom_vdw( 61) = 1.00
  Atom_vdw( 62) = 1.00
  Atom_vdw( 63) = 1.00
  Atom_vdw( 64) = 1.00
  Atom_vdw( 65) = 1.00
  Atom_vdw( 66) = 1.00
  Atom_vdw( 67) = 1.00
  Atom_vdw( 68) = 1.00
  Atom_vdw( 69) = 1.00
  Atom_vdw( 70) = 1.00
  Atom_vdw( 71) = 1.00
  Atom_vdw( 72) = 1.00
  Atom_vdw( 73) = 1.00
  Atom_vdw( 74) = 1.00
  Atom_vdw( 75) = 1.00
  Atom_vdw( 76) = 1.00
  Atom_vdw( 77) = 1.00
  Atom_vdw( 78) = 1.75/bohr
  Atom_vdw( 79) = 1.66/bohr
  Atom_vdw( 80) = 1.55/bohr
  Atom_vdw( 81) = 1.96/bohr
  Atom_vdw( 82) = 2.02/bohr
  Atom_vdw( 83) = 2.07/bohr
  Atom_vdw( 84) = 1.97/bohr
  Atom_vdw( 85) = 2.02/bohr
  Atom_vdw( 86) = 2.20/bohr
  Atom_vdw( 87) = 3.48/bohr
  Atom_vdw( 88) = 2.83/bohr
  Atom_vdw( 89) = 1.00
  Atom_vdw( 90) = 1.00
  Atom_vdw( 91) = 1.00
  Atom_vdw( 92) = 1.86/bohr
  Atom_vdw( 93) = 1.00
  Atom_vdw( 94) = 1.00
  Atom_vdw( 95) = 1.00
  Atom_vdw( 96) = 1.00
  Atom_vdw( 97) = 1.00
  Atom_vdw( 98) = 1.00
  Atom_vdw( 99) = 1.00
  Atom_vdw(100) = 1.00
  Atom_vdw(101) = 1.00
  Atom_vdw(102) = 1.00
  Atom_vdw(103) = 1.00

  do i_species = 1, n_species, 1
    radius_esp_min(i_species) = &
       Atom_vdw(int(species_z(i_species)))*esp_min
    radius_esp_max(i_species) = &
       Atom_vdw(int(species_z(i_species)))*esp_max
  enddo



  if (allocated(Atom_vdw)) then
     deallocate(Atom_vdw)
  end if

end subroutine esp_radius
!******	
!---------------------------------------------------------------------------
!****s* esp_charges/read_esp_parameters
!  NAME
!    read_esp_parameters
!  SYNOPSIS
subroutine read_esp_parameters ( i_esp )
!  PURPOSE
!    Subroutine read_esp_parameters reads the geometry parameters of an output
!    esp for requested esp output number i_esp
!  USES
        use dimensions,only:n_spin,n_periodic
        use runtime_choices,only:n_k_points_xyz,flag_esp_restraint,esp_restraint_strength,esp_restr_reference
        use localorb_io,only:use_unit, localorb_info
        use mpi_tasks,only:myid,aims_stop
        use constants,only:bohr
        implicit none

! ARGUMENTS

        integer :: i_esp
        character*200 :: info_str
!  INPUTS
!    o i_esp - number of the esp file 
!  OUTPUT
!    none
!  AUTHOR
!    FHI-aims team.
!  HISTORY
!    Release version, FHI-aims (2008).
!  SEE ALSO
!    FHI-aims CPC publication (in copyright notice above)
!  SOURCE



!  local variables

!  desc_str : line identifier in control.in
!  i_code   : return status flag from read operation.

        character*20 desc_str
        character*132 inputline
        integer i_code

       character(*), parameter :: func = 'read_esp_parameters'


!  begin work
!set defaults


      flag_density_full(i_esp) = .true.

      esp_state(1,i_esp)=1
      esp_state(2,i_esp)=1
      esp_spin(i_esp)=1
      esp_kpoint(i_esp)=1
      if (n_periodic.ne.0)then
        esp_vdw_radius(1,i_esp)=1.0
        esp_vdw_radius(2,i_esp)=2.0
      else
        esp_vdw_radius(1,i_esp)=3.0
        esp_vdw_radius(2,i_esp)=8.0
      endif
      esp_n_radius(i_esp)=5
      esp_rm(i_esp)=7
      esp_km(i_esp)=7
      esp_R_c(i_esp)=20.0
      esp_pbc_method(i_esp)=2
      esp_grid(i_esp) = 3
      esp_equal_grid_n(1,i_esp) = 10
      esp_equal_grid_n(2,i_esp) = 10
      esp_equal_grid_n(3,i_esp) = 10
      esp_output_cube(i_esp) = 0
      esp_output_pot(i_esp) = .False.
      esp_use_dip_for_cube(i_esp) = .False.
      if(myid.eq.0)then
        write(use_unit,'(2X,A)') "| Esp charge output options: "
      endif
!       read esp information

      lineloop: do

         read(7,'(A)',iostat=i_code) inputline
         if(i_code<0) then
            backspace(7)
            exit lineloop        ! end of file reached
         end if
         if(i_code>0) then
            write(use_unit,'(2X,A)') &
               "Error reading file 'control.in'..."
            stop
         end if

         read (inputline,*,iostat=i_code) desc_str

         if (i_code.ne.0) cycle lineloop

         if (desc_str(1:1) == "#") cycle lineloop

         if (desc_str.eq.'esp') then

            read(inputline,*,end=88,err=99) desc_str, desc_str
            if (desc_str.eq.'state') then

               read(inputline,*,end=88,err=99) desc_str, desc_str, &
                       esp_state(1,i_esp), esp_state(2,i_esp)
               if(myid.eq.0)then
                 write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| Transistion Eigenstate i -> j: ", &
                       esp_state(1,i_esp), esp_state(2,i_esp)
               endif
               flag_density_full(i_esp) = .false.
            else if (desc_str.eq.'restraint') then
               flag_esp_restraint=.true.
               read(inputline,*,end=881,err=99) desc_str, desc_str, esp_restraint_strength, desc_str
               if (desc_str.eq.'monopole') then
                  esp_restr_reference = 1
               else if (desc_str.eq.'hirshfeld') then
                  esp_restr_reference = 2
               endif
881            continue
               if(myid.eq.0)then
                  write(use_unit,'(2X,A)') &
                       "| Using rESP charges "
               endif
            else if (desc_str.eq.'kpoint') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_kpoint(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| Transition Eigenstate k_i: ", &
                       esp_kpoint(i_esp)
              endif
                  flag_density_full(i_esp) = .false.
            else if (desc_str.eq.'spin') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                       esp_spin(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| Transition Eigenstate spin_i: ", &
                       esp_spin(i_esp)
              endif
                  flag_density_full(i_esp) = .false.
            else if (desc_str.eq.'radius') then
               read(inputline,*,end=88,err=99) desc_str, desc_str, &
                    esp_vdw_radius(1,i_esp), esp_vdw_radius(2,i_esp)
              if(myid.eq.0)then
               write(use_unit,'(2X,A,1X,F15.8,1X,F15.8)') &
                    "| Esp radis (inner/outer x vdw-radius): ", &
                    esp_vdw_radius(1,i_esp), esp_vdw_radius(2,i_esp)
               endif
            else if (desc_str.eq.'n_radius') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_n_radius(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| Number of radial shells: ", &
                       esp_n_radius(i_esp)
              endif
            else if (desc_str.eq.'rm') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_rm(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| Number of real space periodic replica: ", &
                       esp_rm(i_esp)
              endif
            else if (desc_str.eq.'km') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_km(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| Number of reciprocal periodic replica: ", &
                       esp_km(i_esp)
              endif
            else if (desc_str.eq.'R_c') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_R_c(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,F15.8)') &
                       "| Real space cut off: ", &
                       esp_R_c(i_esp)
              endif
            else if (desc_str.eq.'pbc_method') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_pbc_method(i_esp)
              if(myid.eq.0)then
                  write(use_unit,'(2X,A,1X,I4.4,1X,I4.4)') &
                       "| PBC method: ", &
                       esp_pbc_method(i_esp)
              endif
            else if (desc_str.eq.'grid') then
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_grid(i_esp)
                  if (esp_grid(i_esp).eq.1) then
                    if(myid.eq.0)then
                     write(use_unit,'(2X,A)') &
                       "| ESP Grid method: 1, radial grid"
                    endif
                  elseif(esp_grid(i_esp).eq.2) then
                    if(myid.eq.0)then
                     write(use_unit,'(2X,A)') &
                       "| ESP Grid method: 2, logarithmic radial grid"
                    endif
                  elseif(esp_grid(i_esp).eq.3) then
                    if(myid.eq.0)then
                     write(use_unit,'(2X,A)') &
                       "| ESP Grid method: 3, cubic grid"
                    endif
                  elseif(esp_grid(i_esp).eq.4) then
                    if(myid.eq.0)then
                     write(use_unit,'(2X,A)') &
                       "| ESP Grid method: 4, cubic grid from radii"
                    endif
                  endif
            else if (desc_str.eq.'equal_grid_n ') then
                  if (esp_grid(i_esp)<3)then
                        esp_grid(i_esp) = 3
                  endif
                  if(esp_grid(i_esp).eq.3) then
                    if(myid.eq.0)then
                     write(use_unit,'(2X,A)') &
                       "| ESP Grid method: 3, cubic grid"
                    endif
                  endif
               read(inputline,*,end=88,err=99) desc_str, desc_str, &
                       esp_equal_grid_n(1,i_esp), esp_equal_grid_n(2,i_esp), &
                            esp_equal_grid_n(3,i_esp)
              if(myid.eq.0)then
               write(use_unit,'(2X,A,1X,I4.4,1X,I4.4,1X,I4.4)') &
                       "| Number of points in x, ,y, z: ", &
                       esp_equal_grid_n(1,i_esp), esp_equal_grid_n(2,i_esp), &
                            esp_equal_grid_n(3,i_esp)
              endif
            else if (desc_str.eq.'output_cube') then
                  if (esp_grid(i_esp)<3)then
                        esp_grid(i_esp) = 3
                  endif
                  read(inputline,*,end=88,err=99) desc_str, desc_str, &
                      esp_output_cube(i_esp)
                 if(myid.eq.0)then
                  write(use_unit,'(2X,A)') &
                       "| Cubic grid will be used " 
                  if(esp_output_cube(i_esp)==1)then
                    write(use_unit,'(2X,A)') &
                       "| Hartree potential will be written to file"
                  elseif(esp_output_cube(i_esp)==2)then
                     write(use_unit,'(2X,A)') &
                       "| XC-potential will be written to file"
                  elseif(esp_output_cube(i_esp)==3)then
                     write(use_unit,'(2X,A)') &
                       "| X-potential will be written to file"
                  elseif(esp_output_cube(i_esp)==4)then
                     write(use_unit,'(2X,A)') &
                       "| C-potential will be written to file"
                  elseif(esp_output_cube(i_esp)==5)then
                     write(use_unit,'(2X,A)') &
                       "| Density will be written to file"
                  elseif(esp_output_cube(i_esp)==6)then
                     write(use_unit,'(2X,A)') &
                       "| Hartree potential with voxel coordinates will be written to file"
                  else
                     esp_output_cube(i_esp)=1
                     write(use_unit,'(2X,A)') &
                       "| Unknown selection defaulting to Hartree potential"
                  endif
                endif
            else if (desc_str.eq.'output_fit') then
                  if (esp_grid(i_esp)<3)then
                        esp_grid(i_esp) = 3
                  endif
                      esp_output_pot(i_esp) = .true.
                if(myid.eq.0)then
                  write(use_unit,'(2X,A)') &
                       "| Cubic grid will be used " 
                  write(use_unit,'(2X,A)') &
                       "| Fitted Potential will be written to file"
                endif
            else if (desc_str.eq.'use_dip_for_cube') then
                      esp_use_dip_for_cube(i_esp) = .true.
                if(myid.eq.0)then
                  write(use_unit,'(2X,A)') &
                       "| Dipole correction for potential output"
                endif
            else
              write(use_unit,'(1X,A,A,A)') &
              "* Unknown esp specification ", desc_str, &
              ". Please correct."
              stop

            end if

          else
!           must have reached the end of the esp description.

            backspace(7)
            exit lineloop

          end if

        end do lineloop

!       Nonsense checks
       if (myid.eq.0) then
        info_str = "  |"
        if(esp_state(1,i_esp).lt.0.or.esp_state(2,i_esp).lt.0) then
          info_str ='Negative state requested.'
          call aims_stop(info_str,func)
        elseif(esp_state(1,i_esp).eq.0.or.esp_state(2,i_esp).eq.0) then
          info_str = 'k-point 0 requested.'
          call aims_stop(info_str,func)
         endif
   
        if(esp_spin(i_esp).gt.2.or.esp_spin(i_esp).lt.1) then
             info_str = 'Esp charges with illegal spin requested.' 
             call aims_stop(info_str,func)
        endif
        if(esp_spin(i_esp).gt.n_spin) then
          info_str ='Esp charges: Spin = 2 not possible for spin=none.' 
          call aims_stop(info_str,func)
        endif
        if(esp_n_radius(i_esp).lt.1) then
          info_str ='Esp charges: Atleast one radial shell needed.' 
          call aims_stop(info_str,func)
        endif

     ! the following nonsense check should really be here, but
     ! apparently, the number of k-points is not known yet. 
        if(esp_kpoint(i_esp).gt.product(n_k_points_xyz)) then
            write(unit=info_str,fmt=('(A,I0,A,I0,A,I0,A)')) &
             'Esp cahrges output at k-point ', esp_kpoint(i_esp),  &
             ' requested, but there are only ', product(n_k_points_xyz),  &
             ' k-points specified. Please correct.' 
            call aims_stop(info_str,func)
        elseif(esp_kpoint(i_esp).lt.0) then
          info_str ='Cubefiles: Negative k-point requested.'
          call aims_stop(info_str,func)
        elseif(esp_kpoint(i_esp).eq.0) then
          info_str = 'k-point 0 requested.'
          call aims_stop(info_str,func)
         endif
      endif
      



             call localorb_info(info_str)

        return

      88 continue
         write(use_unit,'(A)') "Syntax error reading 'output esp' block from 'control.in' (missing arguments)"
         write(use_unit,'(A)') "in line: '"//trim(inputline)//"'"
         stop

      99 continue
         write(use_unit,'(A)') "Syntax error reading 'output esp' block from 'control.in'"
         write(use_unit,'(A)') "in line: '"//trim(inputline)//"'"
         stop

end subroutine read_esp_parameters
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/allocate_esp_out
!  NAME
!    allocate_esp_out
!  SYNOPSIS
subroutine allocate_esp_out ( )

! PURPOSE
!  Subroutine allocate_esp_out allocates the variables needed for esp output.
!  This allocation is done once at the beginning of the code, since the input 
!  is read from file control.in
! USES
        use dimensions,only:n_esp,use_esp,n_atoms,esp_constraint
!  AUTHOR
!    FHI-aims team.
!  HISTORY
!    Release version, FHI-aims (2008).
!  INPUTS
!    none
!  OUTPUT
!    none
!  SEE ALSO
!    FHI-aims CPC publication (in copyright notice above)
!  SOURCE
        implicit none

!       begin work
!       check here in case we introduce other plottable output later ...
        if (use_esp) then
          allocate ( flag_density_full(n_esp) )
          allocate ( esp_state(2,n_esp) )
          allocate ( esp_spin(n_esp))
          allocate ( esp_kpoint(n_esp))
          allocate ( esp_vdw_radius(2,n_esp))
          allocate ( esp_n_radius(n_esp))
          allocate ( esp_km(n_esp))
          allocate ( esp_rm(n_esp))
          allocate ( esp_R_c(n_esp))
          allocate ( esp_pbc_method(n_esp))
          allocate ( esp_grid(n_esp))
          allocate ( esp_equal_grid_n(3,n_esp))
          allocate ( esp_output_cube(n_esp))
          allocate ( esp_output_pot(n_esp))
          allocate ( esp_use_dip_for_cube(n_esp))
          if(esp_constraint.eq.1)then
            allocate (esp_atom_constraint(3,n_atoms))
            esp_atom_constraint = 0
          endif
          if(esp_constraint.eq.2)then
            allocate (esp_atom_constraint(2,n_atoms))
            esp_atom_constraint = 0
          endif
        end if

        end subroutine allocate_esp_out
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/deallocate_esp_out
!  NAME
!    deallocate_esp_out
!  SYNOPSIS
subroutine deallocate_esp_out ( )

! PURPOSE
!  Subroutinede allocate_plot deallocates the variables needed for esp output.
! USES
!  AUTHOR
!    FHI-aims team.
!  HISTORY
!    Release version, FHI-aims (2008).
!  INPUTS
!    none
!  OUTPUT
!    none
!  SEE ALSO
!    FHI-aims CPC publication (in copyright notice above)
!  SOURCE
        implicit none

!       begin work

!       check here in case we introduce other plottable output later ...
if (allocated( flag_density_full       )) deallocate( flag_density_full       )
if (allocated( esp_state       )) deallocate( esp_state       )
if (allocated( esp_spin       )) deallocate( esp_spin       )
if (allocated( esp_kpoint       )) deallocate( esp_kpoint       )
if (allocated( esp_vdw_radius       )) deallocate( esp_vdw_radius       )
if (allocated( esp_n_radius       )) deallocate( esp_n_radius       )
if (allocated( esp_km       )) deallocate( esp_km       )
if (allocated( esp_rm       )) deallocate( esp_rm       )
if (allocated( esp_R_c       )) deallocate( esp_R_c       )
if (allocated( esp_pbc_method       )) deallocate( esp_pbc_method       )
if (allocated( esp_grid      )) deallocate( esp_grid       )
if (allocated( esp_atom_constraint     )) deallocate( esp_atom_constraint)
if (allocated( esp_equal_grid_n     )) deallocate( esp_equal_grid_n)
if (allocated( esp_output_cube     )) deallocate( esp_output_cube)
if (allocated( esp_output_pot     )) deallocate( esp_output_pot)
if (allocated( esp_use_dip_for_cube     )) deallocate( esp_use_dip_for_cube)
end subroutine deallocate_esp_out
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_state_minmax
!  NAME
!    esp_state_minmax
!  SYNOPSIS
subroutine esp_state_minmax(KS_eigen, n_state_min_out, n_state_max_out)

  !  PURPOSE
  !   Get the highest and lowest states that are smaller/bigger than E_max/E_min 
  !
  ! USES
    use dimensions,only:n_states,n_spin,n_k_points
    use runtime_choices,only:esp_Emin,esp_Emax
    use constants,only:hartree
    implicit none
    real*8 , dimension(n_states, n_spin, n_k_points), INTENT(IN) ::     KS_eigen
    integer, INTENT(OUT) :: n_state_min_out
    integer, INTENT(OUT) :: n_state_max_out


    real*8 :: Emin_ha
    real*8 :: Emax_ha
    integer :: i, i_k_point, i_spin
    integer, allocatable :: n_state_min_k(:,:)
    integer, allocatable :: n_state_max_k(:,:)
    allocate(n_state_min_k(n_k_points,n_spin))
    allocate(n_state_max_k(n_k_points,n_spin))
    n_state_min_k = 1 
    n_state_max_k = 0 
    Emin_ha=esp_Emin/hartree
    Emax_ha=esp_Emax/hartree
    do i_k_point = 1, n_k_points, 1
      do i_spin = 1, n_spin, 1
	    do i = 1, n_states, 1
	      if (KS_eigen(i,i_spin,i_k_point)<=Emin_ha) then
	        n_state_min_k(i_k_point,i_spin)=&
                                               n_state_min_k(i_k_point,i_spin)+1
	      endif
	      if (KS_eigen(i,i_spin,i_k_point)<=Emax_ha) then
	        n_state_max_k(i_k_point,i_spin)=&
                                               n_state_max_k(i_k_point,i_spin)+1
	      endif
	    enddo
      enddo
      n_state_min_out = min(minval(n_state_min_k(:,1), n_k_points), &
		    minval(n_state_min_k(:,n_spin), n_k_points))
      n_state_max_out = max(maxval(n_state_max_k(:,1), n_k_points), &
		    maxval(n_state_max_k(:,n_spin), n_k_points))
    enddo ! k-point loop
    if (n_state_min_out>=n_states) then
       n_state_min_out=n_states-1
    endif
    if (n_state_max_out>n_states) then
       n_state_max_out=n_states
    endif
    if (n_state_max_out==0) then
       n_state_max_out=2
    endif
    if (n_state_min_out>=n_state_max_out) then
       n_state_min_out=n_state_max_out-1
    endif
    deallocate(n_state_min_k)
    deallocate(n_state_max_k)
end subroutine esp_state_minmax
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/output_esp_charges
!  NAME
!    output_esp_charges
!  SYNOPSIS
subroutine output_esp_charges(esp_charge_in,esp_moment_in, &
                         KS_Eigenvalue, k_point, n_state_min_in, n_state_max_in)

  !  PURPOSE
  !   Write esp charges and esp dipole moment (x,y,z) to file for one k-point
  !
  ! USES

  use pbc_lists,only:k_point_list
  use dimensions,only:n_atoms,n_k_points,n_states,n_spin
  use constants,only:hartree
  implicit none

  !  ARGUMENTS
  integer, INTENT(IN) :: n_state_min_in
  integer, INTENT(IN) :: n_state_max_in
  real*8, dimension(n_atoms,(((n_state_max_in-n_state_min_in+1)+1)*&
       (n_state_max_in-n_state_min_in+1)/2)),&
       INTENT(IN) :: esp_charge_in
  real*8, dimension(3,(((n_state_max_in-n_state_min_in+1)+1)*&
       (n_state_max_in-n_state_min_in+1)/2)),&
       INTENT(IN) :: esp_moment_in
  integer, INTENT(IN) :: k_point
  real*8 , dimension(n_states, n_spin, n_k_points), INTENT(IN) :: KS_eigenvalue
  ! Write dipelement to file
  !  INPUTS
  !    o esp_charge_in -- esp charges
  !    o esp_moment_in -- esp dipole moment
  !    o k_point -- k-point wanted to be calculated 
  !    o KS_Eigenvalue
  !    o n_state_min_in/n_state_max_in Maximum/Minimum state concidered
  !  OUTPUT
  !    o file esp_moment_k_point.dat
  !  AUTHOR
  !    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
  !  SEE ALSO
  !    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
  !    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
  !    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
  !     Computer Physics Communications (2008), submitted.
  !  COPYRIGHT
  !   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
  !   e.V. Please note that any use of the "FHI-aims-Software" is subject to
  !   the terms and conditions of the respective license agreement."
  !  HISTORY
  !    Release version, FHI-aims (2008).
  ! SOURCE

  integer:: n_state, m_state, i_spin, j_spin, num
  CHARACTER(len=5) :: value
  CHARACTER(len=1) :: valuek1
  CHARACTER(len=2) :: valuek2
  CHARACTER(len=3) :: valuek3
  CHARACTER(len=4) :: valuek4
  CHARACTER(len=5) :: valuek5
  CHARACTER(len=55) :: fmt
  CHARACTER(len=29) :: name
  CHARACTER(len=29) :: name2
  real*8 :: PP

  write(value,'(I3)') 4+n_atoms
  fmt = '(I5, I5, F10.4, I5, I5, F10.4, ' // value // 'ES14.4)'
  if (k_point<=9) then
     write(valuek1,'(I1)') k_point
     name = 'esp_element_k_'//trim(valuek1)//'.dat'
     name2 = 'esp_element_k_'//trim(valuek1)
  elseif (k_point<=99) then
     write(valuek2,'(I2)') k_point
     name = 'esp_element_k_'//trim(valuek2)//'.dat'
     name2 = 'esp_element_k_'//trim(valuek2)
  elseif (k_point<=999) then
     write(valuek3,'(I3)') k_point
     name = 'esp_element_k_'//trim(valuek3)//'.dat'
     name2 = 'esp_element_k_'//trim(valuek3)
  elseif (k_point<=9999) then
     write(valuek4,'(I4)') k_point
     name = 'esp_element_k_'//trim(valuek4)//'.dat'
     name2 = 'esp_element_k_'//trim(valuek4)
  elseif (k_point<=99999) then
     write(valuek5,'(I5)') k_point
     name = 'esp_element_k_'//trim(valuek5)//'.dat'
     name2 = 'esp_element_k_'//trim(valuek5)
  endif

  open(unit=9, file=name,ACTION='WRITE')
     WRITE (9,120) k_point, k_point_list(k_point,1:3)
     120 FORMAT ('# K-point:',I5,' at', 3F10.6)
     WRITE (9,121)
     121 FORMAT ('# KS state i, spin i, KS energy i [eV], KS state j, &
                 &spin j, KS energy j &
                 &[eV], p^x_ij,  p^y_ij, &
                 &p^z_ij, (p^x_ij^2+p^y_ij^2+p^z_ij^2)/3, &
                 &q_(i_atom)')
     num = 0
     do i_spin = 1, n_spin
       do j_spin = i_spin, n_spin
	  do n_state = n_state_min_in,n_state_max_in
	    do m_state = n_state, n_state_max_in
		num = num + 1
		PP = dble((esp_moment_in(1,num)*(esp_moment_in(1,num))+&
			  esp_moment_in(2,num)*(esp_moment_in(2,num))+&
			  esp_moment_in(3,num)*(esp_moment_in(3,num)))) 
		WRITE (9,fmt) n_state, i_spin,&
                              KS_eigenvalue(n_state, i_spin, k_point)*hartree, &
	                      m_state, j_spin, &
                              KS_eigenvalue(m_state, j_spin, k_point)*hartree, &
			   (esp_moment_in(1,num)),&
			  (esp_moment_in(2,num)),&
		      (esp_moment_in(3,num)),&
			      (PP)/3., esp_charge_in(1:n_atoms,num)
	    end do
	  end do
       enddo
     enddo
  close(unit=9)

end subroutine output_esp_charges
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/evaluate_densmat_esp
!  NAME
!    evaluate_densmat_esp
!  SYNOPSIS
  subroutine evaluate_densmat_esp &
  ( KS_eigenvector, KS_eigenvector_complex, occ_numbers,  &
    density_matrix, density_matrix_sparse, i_spin, state_one,state_two,&
    k_point_one,force_packed &
  ) 
    !  PURPOSE
    !    Evaluates the transition density matrix for esp charges
    !  USES
  use dimensions,only:n_hamiltonian_matrix_size,n_centers_basis_T,&
      n_k_points,n_k_points_task,n_spin,n_states,n_basis
  use runtime_choices,only:real_eigenvectors,PM_none,packed_matrix_format
  use mpi_tasks,only:n_tasks,myid,check_allocation
  use synchronize_mpi,only:sync_density_matrix_sparse,sync_density_matrix
  use localorb_io,only:localorb_info,use_unit,OL_norm
  use load_balancing,only:use_batch_permutation,batch_perm
    implicit none

    !  ARGUMENTS

    real*8,     dimension(n_basis, n_states, n_spin,n_k_points_task) :: &
                KS_eigenvector
    complex*16, dimension(n_basis, n_states, n_spin,n_k_points_task) :: &
                KS_eigenvector_complex
    real*8, dimension(n_states, n_spin, n_k_points), intent(IN) :: occ_numbers
    integer, intent(IN) :: i_spin, state_one, state_two, k_point_one
    real*8, dimension(n_centers_basis_T,n_centers_basis_T), intent(OUT) :: &
            density_matrix
    ! when this routine is called, density_matrix_sparse has either the 
    ! dimension (n_hamiltonian_matrix_size) or (n_local_matrix_size)
    ! so we declare it here as a 1D assumed size array
    real*8, intent(OUT) :: density_matrix_sparse(*)
    logical, intent(IN) :: force_packed

    !  INPUTS
    !   o KS_eigenvector -- Kohn-Sham eigenvectors real format
    !   o KS_eigenvector_complex -- Kohn-Sham eigenvectors complex format
    !   o occ_numbers -- occupation of states
    !   o i_spin -- spin index
    !   o state_one -- first state for transisiton matrix
    !   o state_two -- second state for transisiton matrix
    !   o k_point_one -- k_point for transisiton matrix
    !   o force_packed -- use packed storage (Lapack-type in ..._sparse) even if
    !                   packed_matrix_format is PM_none
    !
    !  OUTPUT
    !   o density_matrix -- density matrix if non-packed matrix is in use
    !   o density_matrix_sparse -- density matrix if packed matrix is in use
    ! 
    !  AUTHOR
    !    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
    !  SEE ALSO
    !    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
    !    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
    !    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
    !    Computer Physics Communications (2008), submitted.
    !  COPYRIGHT
    !   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
    !   e.V. Please note that any use of the "FHI-aims-Software" is subject to
    !   the terms and conditions of the respective license agreement."
    !  HISTORY
    !    Release version, FHI-aims (2008).
    !  SOURCE
    !

    !     other local variables
    real*8, dimension(:,:), allocatable :: kdm
    complex*16, dimension(:,:), allocatable:: kdm_complex
    integer :: i_k_point, i_k, n
    integer :: info
    character(*), parameter :: func = 'evaluate_densmat'


    call localorb_info("Evaluating density matrix", use_unit,'(2X,A)', OL_norm )

    if(packed_matrix_format /= PM_none)then
       if(use_batch_permutation > 0) then
          n = batch_perm(use_batch_permutation)%n_local_matrix_size
          density_matrix_sparse(1:n) = 0.d0     
       else
          density_matrix_sparse(1:n_hamiltonian_matrix_size) = 0.d0     
       endif
    else if (force_packed) then
       density_matrix_sparse(1:n_hamiltonian_matrix_size) = 0.d0     
    else
       density_matrix = 0.d0
    end if


       !------------ construct density matrix ----------------------------------

       if (real_eigenvectors) then
          allocate(kdm(n_basis, n_basis), stat=info)
          call check_allocation(info, 'evaluate_densmat:kdm')
          ! dummy for the real case
          allocate(kdm_complex(1, 1), stat=info)
          call check_allocation(info, 'evaluate_densmat:kdm_complex')
       else
          allocate(kdm_complex(n_basis, n_basis), stat=info)
          call check_allocation(info, 'evaluate_densmat:kdm_complex')
          ! dummy for the complex case
          allocate(kdm(1, 1), stat=info)
          call check_allocation(info, 'evaluate_densmat:kdm')
       end if

       ! JW: If there is need for optimization, we could just calculate
       ! those density matrix elements which are actually needed.
       ! This would scale O(N^2) instead O(N^3).

       i_k = 0
       do i_k_point = 1, n_k_points,1
          if (myid.eq.  MOD(i_k_point, n_tasks) .and. myid <= n_k_points ) then
             i_k = i_k + 1
             if(i_k_point.eq.k_point_one)then
		call evaluate_k_densmat_esp(kdm, kdm_complex, &
		&                   KS_eigenvector, KS_eigenvector_complex, &
		&                   i_spin, i_k,k_point_one, state_one, &
                                    state_two,occ_numbers)

		call accumulate_k_densmat_esp(density_matrix_sparse, &
                                          density_matrix, force_packed, &
		&                         kdm, kdm_complex, i_k_point)
             endif
          end if !myid == mod(k...
       end do ! i_k_point

       if(packed_matrix_format /= PM_none .or. force_packed) then
          call sync_density_matrix_sparse(density_matrix_sparse)
       else
          call sync_density_matrix(density_matrix)
       end if

       if (allocated(kdm)) deallocate(kdm)
       if (allocated(kdm_complex)) deallocate(kdm_complex)


  end subroutine evaluate_densmat_esp
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/evaluate_k_densmat_esp
!  NAME
!    evaluate_k_densmat_esp
!  SYNOPSIS
  subroutine evaluate_k_densmat_esp(kdensmat, kdensmat_complex, &
  &                             KS_eigenvector, KS_eigenvector_complex, &
  &                             i_spin, i_k, k_point_one, state_one,&
                                state_two,occ_numbers)

    !  PURPOSE
    !    Calculate the k-dependend trnasition density matrix
    !    for one k and one spin only for real_eigenvectors.
    !  USES
  use dimensions,only:n_k_points,n_k_points_task,n_spin,n_states,n_basis
  use runtime_choices,only:real_eigenvectors
    implicit none

    !  ARGUMENTS

    real*8, intent(OUT) :: kdensmat(n_basis, n_basis)
    complex*16, intent(OUT) :: kdensmat_complex(n_basis, n_basis)
    real*8, intent(IN) :: &
                      KS_eigenvector(n_basis, n_states, n_spin, n_k_points_task)
    complex*16, intent(IN) :: &
              KS_eigenvector_complex(n_basis, n_states, n_spin, n_k_points_task)
    integer, intent(IN) :: i_spin, i_k, k_point_one, state_one, state_two
    real*8, intent(IN) :: occ_numbers(n_states, n_spin, n_k_points)
    !  INPUTS
    !    o occ_numbers -- occupation numbers
    !    o KS_eigenvector, KS_eigenvector_complex -- eigencoefficients
    !    o i_spin -- spin component
    !    o i_k -- node-local k-point index
    !    o state_one -- first state for transition matrix
    !    o state_two -- second state for transition matrix
    !  OUTPUTS
    !    o kdensmat, kdensmat_complex -- density matrix of this k-point
    !  AUTHOR
    !    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
    !  SEE ALSO
    !    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
    !    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
    !    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
    !     Computer Physics Communications (2008), submitted.
    !  COPYRIGHT
    !   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
    !   e.V. Please note that any use of the "FHI-aims-Software" is subject to
    !   the terms and conditions of the respective license agreement."
    !  HISTORY
    !    Release version, FHI-aims (2010).
    !  SOURCE

    character(*), parameter :: func = 'evaluate_k_densmat'
    real*8 :: occu
!integer :: i_bas,i_bas2
    if (real_eigenvectors) then
       kdensmat = 0.d0
    else
       kdensmat_complex = (0.d0, 0.d0)
    end if
    occu =  occ_numbers(state_one, i_spin,  k_point_one)

       if (real_eigenvectors) then
          call dgemm('N','C',n_basis,n_basis,1,occu, &
          &         KS_eigenvector(1:n_basis, state_one, i_spin, i_k), &
                    n_basis, (KS_eigenvector(1:n_basis, state_two,&
                    i_spin, i_k)),n_basis,0.d0,&
          &         kdensmat, n_basis)
       else
          call zgemm('N','C',n_basis,n_basis,1,dcmplx(occu), &
          &         KS_eigenvector_complex(1:n_basis, state_one, i_spin, i_k), &
                    n_basis, KS_eigenvector_complex(1:n_basis, state_two,&
                    i_spin, i_k),n_basis,(0.d0,0.d0),&
          &         kdensmat_complex, n_basis)
       endif

!do i_bas = 1, n_basis, 1
!do i_bas2 = 1, n_basis, 1
!kdensmat_complex(i_bas,i_bas2)=(KS_eigenvector_complex(i_bas, state_one,&
!i_spin, i_k))*dconjg(KS_eigenvector_complex(i_bas2, state_two,i_spin, i_k))
!enddo
!enddo
  end subroutine evaluate_k_densmat_esp
  !******
  !-----------------------------------------------------------------------------
  !****s* esp_charges/accumulate_k_densmat_esp
  !  NAME
  !    accumulate_k_densmat_esp
  !  SYNOPSIS
  subroutine accumulate_k_densmat_esp(density_matrix_sparse, density_matrix, &
                                  force_packed, &
  &                               kdm, kdm_complex, i_k_point)

    !  PURPOSE
    !    Accumulate the k-dependent density matrices to the real-space 
    !    representation.
    !  USES
  use dimensions,only:n_k_points,n_k_points_task,n_spin,n_states,n_basis,&
      n_centers_basis_T,n_hamiltonian_matrix_size,&
      n_centers_basis_T
  use mpi_tasks,only:aims_stop
  use pbc_lists,only:index_hamiltonian,k_phase,center_to_cell,Cbasis_to_center,&
      Cbasis_to_basis,column_index_hamiltonian,n_cells_in_hamiltonian
  use runtime_choices,only:real_eigenvectors,PM_none,&
      PM_index,packed_matrix_format
    implicit none

    !  ARGUMENTS

    real*8, intent(INOUT) :: density_matrix_sparse(n_hamiltonian_matrix_size)
    real*8, intent(INOUT) :: &
                            density_matrix(n_centers_basis_T, n_centers_basis_T)
    logical, intent(IN) :: force_packed
    real*8, intent(IN) :: kdm(n_basis, n_basis)
    complex*16, intent(IN) :: kdm_complex(n_basis, n_basis)
    integer, intent(IN) :: i_k_point

    !  INPUTS
    !    o density_matrix{_sparse} -- real-space density matrix
    !    o force_packed -- use (lapack-)packed density_matrix even for PM_none
    !    o kdm{_cmplx} -- k-dependent density matrix for i_k_point
    !    o i_k_point -- corresponding k_point
    !  OUTPUTS
    !    o density_matrix{_sparse} -- real-space density matrix
    !  AUTHOR
    !    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
    !  HISTORY
    !    Release version, FHI-aims (2010).
    !  SOURCE

    ! Add these matrix elements with their phases to the corresponding
    ! real space matrix.

    real*8 :: add
    integer :: i_bas1, i_bas2, i_basT1, i_basT2
    integer :: i_cell1, i_cell2
    integer :: i_cell
    integer :: i_index
    complex*16 :: conj_phase
    character(*), parameter :: func = 'accumulate_k_densmat'

    select case(packed_matrix_format)
    case(PM_index)
       do i_bas2 = 1, n_basis
          do i_cell = 1,n_cells_in_hamiltonian-1
             conj_phase = conjg(k_phase(i_cell,i_k_point))
             if (index_hamiltonian(1,i_cell, i_bas2) > 0) then
                do i_index = index_hamiltonian(1, i_cell, i_bas2), &
                &            index_hamiltonian(2, i_cell, i_bas2)
                   i_bas1 =  column_index_hamiltonian(i_index)

                   if (real_eigenvectors) then
                      density_matrix_sparse(i_index) &
                      & = density_matrix_sparse(i_index) &
                      & + kdm(i_bas1, i_bas2) * dble(conj_phase)
                   else
                      density_matrix_sparse(i_index) &
                      & = density_matrix_sparse(i_index) &
                      & + dble(kdm_complex(i_bas1, i_bas2) * conj_phase)
                   end if

                end do
             end if
          end do
       end do

    case(PM_none)

       i_index = 0
       do i_basT2 = 1,  n_centers_basis_T
          i_bas2 = Cbasis_to_basis(i_basT2)
          i_cell2 = center_to_cell(Cbasis_to_center(i_basT2))
          do i_basT1 = 1, n_centers_basis_T
             if (force_packed .and. i_basT1 > i_basT2) cycle
             i_bas1 = Cbasis_to_basis(i_basT1)
             i_cell1 = center_to_cell(Cbasis_to_center(i_basT1))
             i_index = i_index + 1

             if (real_eigenvectors) then
                add = kdm(i_bas1, i_bas2) &
                &     * dble(k_phase(i_cell1, i_k_point)) * &
                        dble(k_phase(i_cell2, i_k_point))
             else
                add = dble(kdm_complex(i_bas1, i_bas2) &
                &     * dconjg(k_phase(i_cell1, i_k_point)) * &
                      k_phase(i_cell2, i_k_point))
             end if
             if (force_packed) then
                density_matrix_sparse(i_index) = &
                density_matrix_sparse(i_index) + add
             else
                density_matrix(i_basT1, i_basT2) = &
                density_matrix(i_basT1, i_basT2) + add
             end if
          end do
       end do


    case default

       call aims_stop('Invalid packing type', func)

    end select

  end subroutine accumulate_k_densmat_esp
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/evaluate_KS_density_densmat_esp
!  NAME
!    evaluate_KS_density_densmat_esp
!  SYNOPSIS
subroutine evaluate_KS_density_densmat_esp &
     ( n_points, wave, n_compute, &
     rho, n_basis_compute, &
     density_matrix,density_matrix_two, work & 
     )

!  PURPOSE
!  Subroutine evaluates Khan Sham electron density using density matrix 
!  formalism. Here use dgemm instead of dsymm
!
!  USES

!  use dimensions
  implicit none

!  ARGUMENTS

  integer :: n_points
  integer :: n_basis_compute
  real*8, dimension(n_basis_compute, n_points) :: wave

  integer :: n_compute

  real*8, dimension(n_compute,n_compute) :: density_matrix
  real*8, dimension(n_compute,n_compute) :: density_matrix_two
  real*8 :: rho(n_points)


! INPUTS
! o n_points -- number of grid points
! o n_basis_compute -- maximum number of relevant basis functions
! o wave -- basis functions
! o n_compute -- number of relevant basis functions
! o max_occ_number -- maximum number of states with non-zero occupation
! o density_matrix -- density matrix components for relevant basis function
! o density_matrix_two -- density matrix components for relevant basis 
!                         function; upper/lower triangle
!
! OUTPUT
! o rho -- electron density
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE





  !  local variables

  real*8, external :: ddot

  !     counters

  integer :: i_compute1
  integer :: i_point

  real*8:: work(n_compute, n_points)

  !  begin work
 work = 0.0
!Safeguard if nothing to do:
  if (n_compute.eq.0) then
     rho(1:n_points) = 0.0
     return
  else

     density_matrix = density_matrix + transpose(density_matrix_two)
     do i_compute1 = 1, n_compute
       density_matrix(i_compute1,i_compute1) = &
           density_matrix(i_compute1,i_compute1)*0.5
     enddo

    call dgemm( 'N', 'N', n_compute, n_points, n_compute, 1.0d0, &
                density_matrix , n_compute, wave, n_basis_compute, 0.0d0, &
                work, n_compute )
!       do i_compute1 = 1, n_compute
!       do i_point = 1, n_points,1
!         do i_compute2 = 1, n_compute
!           if (i_compute1.le.i_compute2)then
!             work(i_compute1,i_point)=work(i_compute1,i_point)+&
!             density_matrix(i_compute1,i_compute2)*wave(i_compute2,i_point)
!           else
!             work(i_compute1,i_point)=work(i_compute1,i_point)+&
!             density_matrix_two(i_compute2,i_compute1)*wave(i_compute2,i_point)
!           endif
!         end do
!       end do
!       end do

     do i_point = 1, n_points,1

        rho(i_point) = dot_product(work(1:n_compute,i_point), &
                                   wave(1:n_compute,i_point))

    end do
  endif

  !  end work

end subroutine evaluate_KS_density_densmat_esp
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/evaluate_KS_density_densmat_esp
!  NAME
!    evaluate_KS_density_densmat_esp
!  SYNOPSIS
subroutine evaluate_KS_trafo_densmat_esp &
     ( n_points, wave, grad_wave, n_compute, &
     rho, n_basis_compute, &
     density_matrix,density_matrix_two, work & 
     )

!  PURPOSE
!  Subroutine evaluates Khan Sham electron density using density matrix 
!  formalism. Here use dgemm instead of dsymm
!
!  USES

!  use dimensions
  implicit none

!  ARGUMENTS

  integer :: n_points
  integer :: n_basis_compute
  real*8, dimension(n_basis_compute, n_points) :: wave
  real*8, dimension(n_basis_compute, n_points) :: grad_wave

  integer :: n_compute

  real*8, dimension(n_compute,n_compute) :: density_matrix
  real*8, dimension(n_compute,n_compute) :: density_matrix_two
  real*8 :: rho(n_points)


! INPUTS
! o n_points -- number of grid points
! o n_basis_compute -- maximum number of relevant basis functions
! o wave -- basis functions
! o grad_wave -- basis functions/ gradient / hessian
! o n_compute -- number of relevant basis functions
! o max_occ_number -- maximum number of states with non-zero occupation
! o density_matrix -- density matrix components for relevant basis function
! o density_matrix_two -- density matrix components for relevant basis 
!                         function; upper/lower triangle
!
! OUTPUT
! o rho -- electron density
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE





  !  local variables

  real*8, external :: ddot

  !     counters

  integer :: i_compute1
  integer :: i_point

  real*8:: work(n_compute, n_points)

  !  begin work
 work = 0.0
!Safeguard if nothing to do:
  if (n_compute.eq.0) then
     rho(1:n_points) = 0.0
     return
  else

     density_matrix = density_matrix + transpose(density_matrix_two)
     do i_compute1 = 1, n_compute
       density_matrix(i_compute1,i_compute1) = &
           density_matrix(i_compute1,i_compute1)*0.5
     enddo

    call dgemm( 'N', 'N', n_compute, n_points, n_compute, 1.0d0, &
                density_matrix , n_compute, grad_wave, n_basis_compute, 0.0d0, &
                work, n_compute )
!       do i_compute1 = 1, n_compute
!       do i_point = 1, n_points,1
!         do i_compute2 = 1, n_compute
!           if (i_compute1.le.i_compute2)then
!             work(i_compute1,i_point)=work(i_compute1,i_point)+&
!             density_matrix(i_compute1,i_compute2)*wave(i_compute2,i_point)
!           else
!             work(i_compute1,i_point)=work(i_compute1,i_point)+&
!             density_matrix_two(i_compute2,i_compute1)*wave(i_compute2,i_point)
!           endif
!         end do
!       end do
!       end do

     do i_point = 1, n_points,1

        rho(i_point) = ddot(n_compute, wave(1:n_compute,i_point), 1,&
                                   work(1:n_compute,i_point), 1)

    end do
  endif

  !  end work

end subroutine evaluate_KS_trafo_densmat_esp
!****s* FHI-aims/evaluate_KS_trafo_densmat
!  NAME
!   evaluate_KS_trafo_densmat
!  SYNOPSIS

subroutine evaluate_KS_trafo_densmat &
     ( n_points, wave, grad_wave, n_compute, &
     rho, n_basis_compute, n_max_basis_T, &
     density_matrix, work & 
     )

!  PURPOSE
!  Subroutine evaluates Khan Sham basis transformation using density matrix formalism.
!
!  USES

  use dimensions
  implicit none

!  ARGUMENTS

  integer :: n_points
  integer :: n_basis_compute
  real*8, dimension(n_basis_compute, n_points) :: wave
  real*8, dimension(n_basis_compute, n_points) :: grad_wave
  integer :: n_compute

  integer :: n_max_basis_T
  real*8, dimension(n_compute,n_compute) :: density_matrix

  real*8 :: rho(n_points)


! INPUTS
! o n_points -- number of grid points
! o n_basis_compute -- maximum number of relevant basis functions
! o wave -- basis functions
! o grad_wave -- basis functions/ gradient / hessian
! o n_compute -- number of relevant basis functions
! o n_max_basis_T -- total number of basis functions
! o max_occ_number -- maximum number of states with non-zero occupation
! o density_matrix -- density matrix components for relevant basis function
!
! OUTPUT
! o rho -- electron density
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE





  !  local variables

  real*8, external :: ddot

  !     counters

  integer :: i_point

  real*8:: work(n_compute, n_points)

  !  begin work

!Safeguard if nothing to do:
  if (n_compute.eq.0) then
     rho(1:n_points) = 0.0
     return
  else
     call dsymm('L','U', n_compute, n_points, 2.0d0,  &
          density_matrix, n_compute, grad_wave, n_basis_compute, &
         0.0d0, work, n_compute)

     do i_point = 1, n_points,1

        rho(i_point) = &
           ddot(n_compute, wave(1:n_compute,i_point),1,work(1:n_compute,i_point),1)
    end do
  endif

  !  end work

end subroutine evaluate_KS_trafo_densmat
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/get_transition_density
!  NAME
!    get_transition_density
!  SYNOPSIS

subroutine get_transition_density &
     ( KS_eigenvector, KS_eigenvector_complex, occ_numbers, partition_tab,  &
     hartree_partition_tab, basis_l_max, &
     delta_rho_KS, &
      state_one, state_two, &
     i_k_point_one, i_k_point_two,density_full &
     )

  !  PURPOSE
  !  Subroutine get_transition_density (modified update_density_and_forces)   
  !  obtains the KS transition or total density from the eigenvectors
  !  (and occupation numbers); cluster calculation case .
  !
  !
  !  USES

  use dimensions,only:n_max_compute_atoms,n_centers_integrals,&
      n_max_compute_fns_dens,n_centers,n_centers_basis_integrals,&
      n_basis,n_states,n_spin,n_k_points,n_full_points,n_species,&
      n_basis_fns,n_periodic,n_my_batches,use_density_gradient,&
      l_wave_max,n_centers_basis_T,n_max_batch_size,n_max_compute_dens
  use runtime_choices,only:real_eigenvectors,n_k_points_group,prune_basis_once,&
      spin_treatment
  use grids,only:batches
  use mpi_tasks,only:n_tasks,myid,MPI_REAL8,mpi_comm_global,MPI_DOUBLE_PRECISION
  use pbc_lists,only:k_phase,center_to_cell,Cbasis_to_center,&
      Cbasis_to_basis,inv_centers_basis_integrals,centers_basis_integrals
  use localorb_io,only:localorb_info,use_unit
  use basis,only:basis_wave_ordered
  use synchronize_mpi_basic,only:sync_vector
  implicit none

  !  ARGUMENTS


  !  input 

  real*8,     dimension(n_basis, n_states, n_spin,n_k_points), intent(IN) :: &
                                                                KS_eigenvector
  complex*16, dimension(n_basis, n_states, n_spin,n_k_points), intent(IN) :: &
                                                        KS_eigenvector_complex

  real*8, dimension(n_states, n_spin, n_k_points), intent(IN) :: occ_numbers    

  real*8, dimension(n_full_points), intent(IN) ::  partition_tab
  real*8, dimension(n_full_points), intent(IN) ::  hartree_partition_tab

  integer, intent(IN) :: basis_l_max (n_species)

  !  output

  real*8, intent(OUT) :: delta_rho_KS(n_spin,n_full_points)

  integer, intent(IN) :: state_one
  integer, intent(IN) :: state_two
  integer, intent(IN) :: i_k_point_one
  integer, intent(IN) :: i_k_point_two
  logical, intent(IN) :: density_full
  !  INPUTS
  !   o KS_eigenvector -- Kohn-Sham eigenvectors (real format)
  !   o KS_eigenvector_complex -- Kohn-Sham eigenvectors (complex format)
  !   o partition_tab -- values of partition function
  !   o hartree_partition_tab -- values of partition function used in multipole 
  !                                   expansion of charges in Hartree potential.
  !   o basis_l_max -- maximum l of basis functions
  !   o state_one -- initial state
  !   o state_two -- final state
  !   o i_k_point_one -- initial k-point
  !   o i_k_point_two -- final k-point
  !   o density_full -- calculate transition or full density
  !   o occ_numbers -- occupation numbers
  !   
  !  OUTPUT
  !   o delta_rho_KS -- electron density, 
  !
  !  AUTHOR
  !    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
  !  SEE ALSO
  !    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
  !    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
  !    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
  !    Computer Physics Communications (2008), submitted.
  !  COPYRIGHT
  !   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
  !   e.V. Please note that any use of the "FHI-aims-Software" is subject to
  !   the terms and conditions of the respective license agreement."
  !  HISTORY
  !    Release version, FHI-aims (2008).
  !  SOURCE
  !



  !  local variables

  real*8 coord_current(3)
  real*8 dist_tab(n_centers_basis_integrals  , n_max_batch_size)
  real*8 dist_tab_sq(n_centers_basis_integrals, n_max_batch_size)
  real*8 i_r(n_centers_basis_integrals)
  real*8 dir_tab(3, n_centers_basis_integrals, n_max_batch_size)
  real*8 trigonom_tab(4, n_centers_basis_integrals)


  real*8,dimension(:),allocatable:: radial_wave
  real*8,dimension(:,:),allocatable:: wave
  complex*16,dimension(:,:),allocatable:: wave_c


  !     pruning of atoms, radial functions, and basis functions

  integer :: n_compute_c
  integer,dimension(:),allocatable :: i_basis

  integer :: n_compute_fns
  integer :: i_basis_fns(n_basis_fns*n_centers_basis_integrals)
  integer :: i_basis_fns_inv(n_basis_fns,n_centers)
  integer :: i_atom_fns(n_basis_fns*n_centers_basis_integrals)

  integer :: n_compute_atoms
  integer :: atom_index(n_centers_basis_integrals)
  integer :: atom_index_inv(n_centers)


  integer :: spline_array_start(n_centers_basis_integrals)
  integer :: spline_array_end(n_centers_basis_integrals)

  ! VB - renewed index infrastructure starts here

  real*8 one_over_dist_tab(n_max_compute_atoms)

  ! indices for basis functions that are nonzero at current point

  integer :: rad_index(n_max_compute_atoms)
  integer :: wave_index(n_max_compute_fns_dens)
  integer :: l_index(n_max_compute_fns_dens)
  integer :: l_count(n_max_compute_fns_dens)
  integer :: fn_atom(n_max_compute_fns_dens)

  ! indices for known zero basis functions at current point
  integer :: n_zero_compute
  integer :: zero_index_point(n_max_compute_dens)

  ! active atoms in current batch
  integer :: n_batch_centers
  integer :: batch_center(n_centers_integrals)

  !     other local variables
  integer, dimension(n_spin) :: max_occ_number
  real*8 :: occ_numbers_sqrt

  integer :: l_ylm_max
  integer :: n_points

  logical :: write_out


  real*8,     dimension(:,:),allocatable :: KS_vec_one
  complex*16, dimension(:,:),allocatable :: KS_vec_one_complex
  real*8,     dimension(:),allocatable :: KS_ev_compute_one
  complex*16, dimension(:),allocatable :: KS_ev_compute_one_complex
  real*8,     dimension(:),allocatable :: KS_orbital_one
  complex*16, dimension(:),allocatable :: KS_orbital_one_complex

  real*8,     dimension(:,:),allocatable :: KS_vec_two
  complex*16, dimension(:,:),allocatable :: KS_vec_two_complex
  real*8,     dimension(:),allocatable :: KS_ev_compute_two
  complex*16, dimension(:),allocatable :: KS_ev_compute_two_complex
  real*8,     dimension(:),allocatable :: KS_orbital_two
  complex*16, dimension(:),allocatable :: KS_orbital_two_complex

  real*8,     dimension(:,:,:),allocatable :: KS_vec_times_occ_sqrt
  real*8,     dimension(:,:),allocatable :: KS_ev_compute
  real*8,    dimension(:,:),allocatable :: KS_orbital
  complex*16,     dimension(:,:,:),allocatable :: KS_vec_times_occ_sqrt_complex
  complex*16,     dimension(:,:),allocatable :: KS_ev_compute_complex
  complex*16,    dimension(:,:),allocatable :: KS_orbital_complex

  real*8 temp_rho(n_max_batch_size,n_spin)


  integer, dimension(:,:), allocatable :: index_lm
  real*8, dimension(:,:), allocatable :: ylm_tab


  !     only allocated and referenced for gradient functionals
  real*8, dimension(:,:,:,:), allocatable :: KS_orbital_gradient
  complex*16, dimension(:,:,:,:), allocatable :: KS_orbital_gradient_complex


  !     counters

  integer i_l
  integer i_m
  integer :: i_state
  integer :: i_point
  integer :: i_bas
  integer :: i_k
  integer :: i_my_batch
  integer :: i_compute

  integer :: i_full_points
  integer :: i_full_points_2
  integer :: i_full_points_3

  ! i_spin for future use in a spin-polarized version
  integer :: i_spin = 1

  integer ::  i_index, i_k_point, i_k_point_group

  !     mpi

  !      integer, dimension(:,:), allocatable :: n_points_mpi
  integer :: info
  integer:: n_k_group, i_k_point_g


  real*8, external :: ddot
  !     begin work
  

  if(real_eigenvectors)then

     if (density_full)then
	if(.not. allocated( KS_vec_times_occ_sqrt))then
	    allocate( KS_vec_times_occ_sqrt(n_centers_basis_T,&
                                 (n_states*n_k_points_group), n_spin),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_vec_times_occ_sqrt'
	      stop
	    end if
	end if
	if(.not. allocated( KS_ev_compute))then
	    allocate( KS_ev_compute(n_states*n_k_points_group, &
                                                   n_centers_basis_T),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_ev_compute'
	      stop
	    end if
	end if

	if(.not. allocated( KS_orbital))then
	    allocate( KS_orbital(n_states*n_k_points_group,n_max_batch_size),&
                                                                      stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_orbital'
	      stop
	    end if
	end if
     else
	if(.not. allocated( KS_vec_one))then
	    allocate( KS_vec_one(n_centers_basis_T, n_spin),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_vec_one'
	      stop
	    end if
	end if

	if(.not. allocated( KS_vec_two))then
	    allocate( KS_vec_two(n_centers_basis_T, n_spin),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_vec_two'
	      stop
	    end if
	end if

	if(.not. allocated( KS_ev_compute_one))then
	    allocate( KS_ev_compute_one(n_centers_basis_T),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_ev_compute_one'
	      stop
	    end if
	end if

	if(.not. allocated( KS_ev_compute_two))then
	    allocate( KS_ev_compute_two(n_centers_basis_T),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_ev_compute_two'
	      stop
	    end if
	end if

	if(.not. allocated( KS_orbital_one))then
	    allocate( KS_orbital_one(n_max_batch_size),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_orbital_one'
	      stop
	    end if
	end if

	if(.not. allocated( KS_orbital_two))then
	    allocate( KS_orbital_two(n_max_batch_size),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_orbital_two'
	      stop
	    end if
	end if
     endif
  else
 
     if (density_full)then
	if(.not. allocated( KS_vec_times_occ_sqrt_complex))then
	    allocate( KS_vec_times_occ_sqrt_complex(n_centers_basis_T,&
                                 (n_states*n_k_points_group), n_spin),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_vec_times_occ_sqrt'
	      stop
	    end if
	end if
	if(.not. allocated( KS_ev_compute_complex))then
	    allocate( KS_ev_compute_complex(n_states*n_k_points_group,&
                                                   n_centers_basis_T),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_ev_compute'
	      stop
	    end if
	end if

	if(.not. allocated( KS_orbital_complex))then
	    allocate( KS_orbital_complex(n_states*n_k_points_group,&
                                                    n_max_batch_size),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_orbital'
	      stop
	    end if
	end if
     else
	if(.not. allocated( KS_vec_one_complex))then
	    allocate( KS_vec_one_complex(n_centers_basis_T, n_spin),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_vec_one_complex'
	      stop
	    end if
	end if

	if(.not. allocated( KS_vec_two_complex))then
	    allocate( KS_vec_two_complex(n_centers_basis_T, n_spin),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_vec_two_complex'
	      stop
	    end if
	end if

	if(.not. allocated( KS_ev_compute_one_complex))then
	    allocate( KS_ev_compute_one_complex(n_centers_basis_T),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_ev_compute_one_complex'
	      stop
	    end if
	end if

	if(.not. allocated( KS_ev_compute_two_complex))then
	    allocate( KS_ev_compute_two_complex(n_centers_basis_T),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_ev_compute_two_complex'
	      stop
	    end if
	end if

	if(.not. allocated( KS_orbital_one_complex))then
	    allocate( KS_orbital_one_complex(n_max_batch_size),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_orbital_one_complex'
	      stop
	    end if
	end if

	if(.not. allocated( KS_orbital_two_complex))then
	    allocate( KS_orbital_two_complex(n_max_batch_size),stat=info)
	    if(info/=0)then
	      write(use_unit,*)'Error in allocation: KS_orbital_two_complex'
	      stop
	    end if
	end if
     endif
  end if

  if(.not. allocated(i_basis))then
     allocate(i_basis(n_centers_basis_T),stat=info)
     if(info/=0)then
        write(use_unit,*)'Error in allocation: i_basis'
        stop
     end if
  end if

  if(.not.allocated(radial_wave))then
     allocate(radial_wave(n_max_compute_fns_dens),stat=info)
     if(info/=0)then
        write(use_unit,*)'Error in allocation: radial_wave'
        stop
     end if
  end if

  if(.not. allocated(wave))then
     allocate(wave(n_max_compute_dens, n_max_batch_size),stat=info)
     if(info/=0)then
        write(use_unit,*)'Error in allocation: wave'
        stop
     end if
  end if

  if(.not. allocated(wave_c))then
     allocate(wave_c(n_max_compute_dens, n_max_batch_size),stat=info)
     if(info/=0)then
        write(use_unit,*)'Error in allocation: wave_c'
        stop
     end if
  end if

  l_ylm_max = l_wave_max

  if(.not. allocated( ylm_tab))then
     allocate( ylm_tab( (l_ylm_max+1)**2,n_centers_basis_integrals),stat=info )
     if(info/=0)then
        write(use_unit,*)'Error in allocation: ylm_tab'
        stop
     end if
  end if


  if(.not. allocated( index_lm))then
     allocate( index_lm( -l_ylm_max:l_ylm_max, 0:l_ylm_max),stat=info) 
     if(info/=0)then
        write(use_unit,*)'Error in allocation: index_lm'
        stop
     end if
  end if


  if (.not.allocated(KS_orbital_gradient)) then
     if ( use_density_gradient .and. (real_eigenvectors) ) then
           allocate(KS_orbital_gradient(n_states*n_k_points_group,&
                    n_max_batch_size,3,n_spin),stat=info)
           if(info/=0)then
              write(use_unit,*)'Error in allocation: KS_orbital_gradient'
              stop
           end if
           KS_orbital_gradient = 0.d0

     else ! dummy allocation - needed for either spin channel
           allocate(KS_orbital_gradient(1,1,1,n_spin),stat=info)
     end if
  end if
  
  ! Since this routine is never used for periodic boundary conditions 
  ! in practice, the complex infrastructure is never presently used. Note that 
  !this could change if we were to introduce magnetic fields in the future.
  if (.not.allocated(KS_orbital_gradient_complex)) then
     if ( use_density_gradient .and. (.not.real_eigenvectors) ) then
        allocate(KS_orbital_gradient_complex(n_states*n_k_points_group,&
                 n_max_batch_size,3,n_spin),stat=info)
        if(info/=0)then
             write(use_unit,*)'Error in allocation: KS_orbital_gradient_complex'
             stop
        end if
           KS_orbital_gradient_complex = (0.d0,0.d0)
     else ! dummy allocation
        allocate(KS_orbital_gradient_complex(1,1,1,n_spin),stat=info)
     end if
  end if


  !     initialize index_lm
  i_index = 0
  do i_l = 0, l_ylm_max, 1
     do i_m = -i_l, i_l
        i_index = i_index + 1
        index_lm(i_m, i_l) = i_index
     enddo
  enddo


  !     initialize charge density convergence criterion
  delta_rho_KS = 0.d0

  if(real_eigenvectors)then
        do i_spin = 1, n_spin, 1

          if (density_full)then
            max_occ_number(i_spin) = 0
            i_index = 0
	    do i_state = 1, n_states, 1
	      if (occ_numbers(i_state,i_spin, i_k_point_one).gt.0.d0) then
		i_index = i_index + 1
		occ_numbers_sqrt =  sqrt(occ_numbers(i_state,i_spin,&
                                                          i_k_point_one))
	       	do i_bas = 1,  n_centers_basis_T, 1
		    KS_vec_times_occ_sqrt(i_bas,i_index,i_spin) =  &
		      KS_eigenvector(Cbasis_to_basis(i_bas),i_state,i_spin, &
                      i_k_point_one) * occ_numbers_sqrt * &
	              dble(k_phase(center_to_cell(Cbasis_to_center&
                             (  i_bas  )),i_k_point_one))
		end do
	      endif
	    enddo
            max_occ_number(i_spin) = i_index
         else
           do i_bas = 1,  n_centers_basis_T, 1
              KS_vec_one(i_bas,i_spin) =  &
                 KS_eigenvector(Cbasis_to_basis(i_bas),state_one,i_spin, &
                 i_k_point_one) *dble(k_phase(center_to_cell(Cbasis_to_center&
                 (  i_bas  )),i_k_point_one))
              KS_vec_two(i_bas,i_spin) =  KS_eigenvector(Cbasis_to_basis(i_bas)&
                 ,state_two,i_spin, i_k_point_two) * &
                  dble(k_phase(center_to_cell(Cbasis_to_center&
                  (  i_bas  )),i_k_point_two))
           end do
         endif
       end do


  else
    if(density_full)then
	n_k_group = ceiling(real(n_k_points)/real(n_k_points_group))
	do i_k_point_group = 1,n_k_group
	  do i_spin = 1, n_spin, 1

	    max_occ_number(i_spin) = 0
	    i_index = 0

	    do i_k_point_g = 1, n_k_points_group

		i_k_point = (i_k_point_group-1) * n_k_points_group + i_k_point_g

		if(i_k_point <= n_k_points)then

		  do i_state = 1, n_states, 1
		      if (occ_numbers(i_state,i_spin, i_k_point).gt.0.d0) then

			i_index = i_index + 1

			occ_numbers_sqrt =  sqrt(occ_numbers(i_state,i_spin,&
                                                 i_k_point))

			do i_bas = 1,  n_centers_basis_T, 1
			  KS_vec_times_occ_sqrt_complex(i_bas,i_index,i_spin) =&
		           KS_eigenvector_complex(Cbasis_to_basis(i_bas),&
                           i_state,i_spin, i_k_point) * occ_numbers_sqrt * &
			   dconjg(k_phase(center_to_cell(Cbasis_to_center&
                           (  i_bas  )),i_k_point))
			end do
		      end if
		  end do
		end if
	    end do
	    max_occ_number(i_spin) = i_index
	  end do
	enddo
    else
       i_k = 0
       do i_k_point = 1, n_k_points,1
             i_k = i_k + 1
             if (i_k_point==i_k_point_one)then                
		do i_spin = 1, n_spin, 1
		  do i_bas = 1,  n_centers_basis_T, 1
		      KS_vec_one_complex(i_bas,i_spin) =  &
			KS_eigenvector_complex(Cbasis_to_basis(i_bas),&
                          state_one,i_spin, i_k) * &
			  dconjg(k_phase(center_to_cell(Cbasis_to_center&
                          (  i_bas  )),i_k_point_one))
		  end do
		end do  
             endif
             if (i_k_point==i_k_point_two)then
		do i_spin = 1, n_spin, 1
		  do i_bas = 1,  n_centers_basis_T, 1
		      KS_vec_two_complex(i_bas,i_spin) =  &
			KS_eigenvector_complex(Cbasis_to_basis(i_bas),&
                          state_two,i_spin, i_k) * &
			  dconjg(k_phase(center_to_cell(Cbasis_to_center&
                          (  i_bas  )),i_k_point_two))
		  end do
		end do  
             endif
       enddo
      !call sync_vector_complex(KS_vec_one_complex,n_centers_basis_T*n_spin)
      !call sync_vector_complex(KS_vec_two_complex,n_centers_basis_T*n_spin)
    end if
  endif
     !     loop over integration grid

     i_basis_fns_inv = 0


     i_full_points = 0
     i_full_points_2 = 0
     i_full_points_3 = 0

     write_out = .false.

     do i_my_batch = 1, n_my_batches, 1

           n_compute_c = 0
           i_basis = 0

           i_point = 0

           ! loop over one batch
           do i_index = 1, batches(i_my_batch)%size, 1

              i_full_points = i_full_points + 1

              if (max(partition_tab(i_full_points),&
                   hartree_partition_tab(i_full_points)).gt.0.d0) then

                 i_point = i_point+1

                 !     get current integration point coordinate
                 coord_current(:) = &
                               batches(i_my_batch) % points(i_index) % coords(:)

                 if(n_periodic > 0)then
                    call map_to_center_cell(coord_current(1:3) )
                 end if

                 ! compute atom-centered coordinates of current integration 
                 ! point, as viewed from all atoms
                 call tab_atom_centered_coords_p0 &
                      ( coord_current,  &
                      dist_tab_sq(1,i_point),  &
                      dir_tab(1,1,i_point), &
                      n_centers_basis_integrals, centers_basis_integrals )


                 !     determine which basis functions are relevant at current 
                 !     integration point, and tabulate their indices

                 ! next, determine which basis functions u(r)/r*Y_lm(theta,phi) 
                 ! are actually needed
                 if (.not.prune_basis_once) then
                    call prune_basis_p2 &
                         ( dist_tab_sq(1,i_point), &
                         n_compute_c, i_basis,  &
                         n_centers_basis_T, n_centers_basis_integrals, &
                                                  inv_centers_basis_integrals  )
                 end if
              end if
           enddo ! end loop over the angular shell

           if (prune_basis_once) then
              n_compute_c = batches(i_my_batch)%batch_n_compute
              i_basis(1:n_compute_c) = batches(i_my_batch)%batch_i_basis
           end if

           ! from list of n_compute active basis functions in batch, collect 
           !all atoms that are ever needed in batch.           
           call collect_batch_centers_p2 &
                ( n_compute_c, i_basis, n_centers_basis_T, &
                  n_centers_basis_integrals, inv_centers_basis_integrals, &
                n_batch_centers, batch_center &
                )

           n_points = i_point
 
           if (n_compute_c.gt.0) then

              ! Determine all radial functions, ylm functions and their 
              !derivatives that are best evaluated strictly locally at each 
              ! individual grid point.
              i_point = 0
              do i_index = 1, batches(i_my_batch)%size, 1

                 i_full_points_2 = i_full_points_2 + 1

                 if (max(partition_tab(i_full_points_2),&
                      hartree_partition_tab(i_full_points_2)).gt.0.d0) then

                    i_point = i_point+1
                    n_compute_atoms = 0
                    n_compute_fns = 0

                    ! All radial functions (i.e. u(r), u''(r)+l(l+2)/r^2, u'(r) 
                    !if needed) Are stored in a compact spline array that can 
                    ! be accessed by spline_vector_waves, without any copying 
                    ! and without doing any unnecessary operations. 
                    ! The price is that the interface is no longer explicit in 
                    ! terms of physical objects. See shrink_fixed_basis() for
                    ! details regarding the reorganized spline arrays.

                    ! dir_tab(:,:,i_point,i_division) = 
                    !                dir_tab_full(:,:,i_angular)


                    call prune_radial_basis_p2 &
                         ( n_max_compute_atoms, n_max_compute_fns_dens, &
                         dist_tab_sq(1,i_point), dist_tab(1,i_point), &
                         dir_tab(1,1,i_point), &
                         n_compute_atoms, atom_index, atom_index_inv, &
                         n_compute_fns, i_basis_fns, i_basis_fns_inv, &
                         i_atom_fns, spline_array_start, spline_array_end, &
                         n_centers_basis_integrals, centers_basis_integrals,&
                         n_compute_c, i_basis, &
                         n_batch_centers, batch_center, &
                         one_over_dist_tab, rad_index, wave_index, l_index,&
                         l_count, &
                         fn_atom, n_zero_compute, zero_index_point &
                         )


                    ! Tabulate distances, unit vectors, and inverse logarithmic 
                    ! grid units for all atoms which are actually relevant
                    call tab_local_geometry_p2 &
                         ( n_compute_atoms, atom_index, &
                         dist_tab(1,i_point),  &
                         i_r &
                         )


                    ! Determine all needed radial functions from efficient 
                    ! splines

                    ! Now evaluate radial functions u(r) from the previously 
                    ! stored compressed  spline arrays  
                    call evaluate_radial_functions_p0 &
                         (   spline_array_start, spline_array_end, &
                         n_compute_atoms, n_compute_fns,  &
                         dist_tab(1,i_point), i_r(1), &
                         atom_index, i_basis_fns_inv, &
                         basis_wave_ordered, radial_wave(1), &
                         .false., n_compute_c, n_max_compute_fns_dens   &
                         )





                    !     Determine all needed ylm functions, and from these
                    !     * wave functions

                    ! compute trigonometric functions of spherical coordinate 
                    ! angles of current integration point, viewed from all atoms

                       call tab_trigonom_p0 &
                            ( n_compute_atoms, dir_tab(1,1,i_point),  &
                            trigonom_tab(1,1) &
                            )


                          ! tabulate distance and Ylm's w.r.t. other atoms            
                          call tab_wave_ylm_p0 &
                               ( n_compute_atoms, atom_index,  &
                               trigonom_tab(1,1), basis_l_max,  &
                               l_ylm_max, &
                               ylm_tab(1,1) )

                    ! tabulate total wave function value for each basis function 
                    ! in all cases -but only now are we sure that we have 
                    ! ylm_tab ...

                    ! tabulate total wave function value for each basis function
                    call evaluate_waves_p2  &
                         ( n_compute_c, n_compute_atoms, n_compute_fns, &
                         l_ylm_max, ylm_tab, one_over_dist_tab,   &
                         radial_wave, wave(1,i_point), &
                         rad_index, wave_index, l_index, l_count, fn_atom, &
                         n_zero_compute, zero_index_point &
                         )



                    ! reset i_basis_fns_inv
                    i_basis_fns_inv(:,atom_index(1:n_compute_atoms)) = 0

                 end if ! end if (partition_tab.gt.0)
              enddo ! end loop over one batch of the grid
              ! - all quantities that are evaluated pointwise are now known ...
              temp_rho = 0d0
              if (n_points.gt.0) then
                 ! Now perform all operations which are done across the entire 
                 ! integration
                 ! integration shell at once.
                 do i_spin = 1, n_spin, 1
                       ! New density is always evaluated
                       if(real_eigenvectors)then
                          if (density_full) then
                            do i_compute = 1, n_compute_c, 1
                              do i_state = 1, max_occ_number(i_spin), 1
                                 KS_ev_compute(i_state, i_compute) =  &
                                 KS_vec_times_occ_sqrt(i_basis(i_compute), &
                                 i_state ,i_spin)
                              enddo
                            enddo
                            call dgemm('N','N', max_occ_number(i_spin), &
                               n_points, n_compute_c, 1.0d0,  &
                               KS_ev_compute(1:max_occ_number(i_spin), &
                               1:n_compute_c), max_occ_number(i_spin), &
                               wave(1,1), n_max_compute_dens, &
                               0.0d0, KS_orbital(1:max_occ_number(i_spin), &
                               1:n_points), max_occ_number(i_spin))
                            do i_point = 1, n_points, 1
                              temp_rho(i_point, i_spin) =  &
                              ddot(max_occ_number(i_spin), &
                              KS_orbital(1,i_point), 1, &
                              KS_orbital(1,i_point), 1)
                            enddo
                          else
                            do i_compute = 1, n_compute_c, 1
                               KS_ev_compute_one(i_compute) =  &
                               KS_vec_one(i_basis(i_compute), i_spin)
                               KS_ev_compute_two(i_compute) =  &
                               KS_vec_two(i_basis(i_compute), i_spin)
                            enddo

                            call dgemm('N','N', 1, n_points, n_compute_c, &
                               1.0d0, KS_ev_compute_one(1:n_compute_c), 1, &
                               wave(1,1), n_max_compute_dens, &
                               0.0d0, KS_orbital_one(1:n_points), 1)
                            call dgemm('N','N', 1, n_points, n_compute_c, &
                               1.0d0, KS_ev_compute_two(1:n_compute_c), 1, &
                               wave(1,1), n_max_compute_dens, &
                               0.0d0, KS_orbital_two(1:n_points), 1)
                            do i_point = 1, n_points, 1
                              temp_rho(i_point, i_spin) =  ddot(1, &
                              KS_orbital_one(i_point), 1, &
                              KS_orbital_two(i_point), 1)
                            enddo
                          endif
                       else
                         if(density_full)then
                           do i_compute = 1, n_compute_c, 1
                             do i_state = 1, max_occ_number(i_spin), 1
                              KS_ev_compute_complex(i_state, i_compute) =  &
                              KS_vec_times_occ_sqrt_complex(i_basis(i_compute),&
                              i_state ,i_spin)
                             enddo
                           enddo
                           wave_c = dcmplx(wave)
                           call zgemm('N','N', max_occ_number(i_spin), &
                                n_points, n_compute_c, (1.0d0,0d0),  &
                                KS_ev_compute_complex, max_occ_number(i_spin), &
                                wave_c, n_max_compute_dens, &
                                (0.0d0,0d0), KS_orbital_complex, &
                                max_occ_number(i_spin))
                           do i_point = 1, n_points, 1
                             temp_rho(i_point, i_spin) = dble(dot_product(&
                             KS_orbital_complex(1:max_occ_number(i_spin),&
                             i_point), KS_orbital_complex(1:max_occ_number&
                             (i_spin),i_point)))
                          enddo
                         else
                           do i_compute = 1, n_compute_c, 1
                              KS_ev_compute_one_complex(i_compute) =  &
                              KS_vec_one_complex(i_basis(i_compute), i_spin)
                              KS_ev_compute_two_complex(i_compute) =  &
                              KS_vec_two_complex(i_basis(i_compute), i_spin)
                           enddo
                           wave_c = dcmplx(wave)
                           call zgemm('N','N', 1, n_points, n_compute_c, &
                            (1.0d0,0d0), KS_ev_compute_one_complex&
                            (1:n_compute_c), 1, wave_c(1,1), &
                             n_max_compute_dens, (0.0d0,0d0), &
                             KS_orbital_one_complex(1:n_points), 1)
                           call zgemm('N','N', 1, n_points, n_compute_c, &
                            (1.0d0,0d0),  &
                            KS_ev_compute_two_complex(1:n_compute_c), 1, &
                            wave_c(1,1), n_max_compute_dens, &
                            (0.0d0,0d0), KS_orbital_two_complex(1:n_points),&
                             1)
                           do i_point = 1, n_points, 1
                             temp_rho(i_point, i_spin) = &
                             dble(KS_orbital_one_complex(i_point)* &
                             (KS_orbital_two_complex(i_point)))
                           enddo
                         endif
                       end if
                 end do ! i_spin
              end if ! (n_points>0)

              ! set electron density
              i_point = 0
              do i_index = 1, batches(i_my_batch)%size, 1

                 i_full_points_3 = i_full_points_3 + 1

                 if (max(partition_tab(i_full_points_3),&
                      hartree_partition_tab(i_full_points_3)).gt.0) then

                    i_point = i_point + 1

                    if (spin_treatment.eq.0) then

                       delta_rho_KS(1,i_full_points_3) =   &
                            delta_rho_KS(1,i_full_points_3) & 
                            + temp_rho(i_point,1) 

                    elseif (spin_treatment.eq.1) then

                       delta_rho_KS(1,i_full_points_3) =   &
                            delta_rho_KS(1,i_full_points_3)   &
                            +( temp_rho(i_point, 1) +   &
                            temp_rho(i_point, 2) )

                       delta_rho_KS(2,i_full_points_3) =   &
                            delta_rho_KS(2,i_full_points_3)   &
                            +( temp_rho(i_point, 1) -   &
                            temp_rho(i_point, 2) )

                    end if

                    ! prepare charge density root-mean-square distance








                 endif
              end do

           else
              ! Even if n_compute .eq. 0 for the entire current batch of grid 
              ! points, we still need to make sure that the density _change_ at 
              ! this point is ( zero minus previous density ). This ensures 
              !that. even for a zero KS density, a potentially non-zero 
              !initialization density is subtracted to properly account for the 
              ! density change ....

              do i_index = 1, batches(i_my_batch)%size, 1

                 i_full_points_2 = i_full_points_2 + 1
                 i_full_points_3 = i_full_points_3 + 1

                 if (max(partition_tab(i_full_points_3),&
                      hartree_partition_tab(i_full_points_3)).gt.0.d0) then

                    if (spin_treatment.eq.0) then

                       delta_rho_KS(1,i_full_points_3) =   &
                            delta_rho_KS(1,i_full_points_3) +  &
                             temp_rho(i_point,1)

                    elseif (spin_treatment.eq.1) then

                       delta_rho_KS(1,i_full_points_3) =   &
                            delta_rho_KS(1,i_full_points_3)+  &
                             temp_rho(i_point,1)

                       delta_rho_KS(i_full_points_3,2) =   &
                            delta_rho_KS(2,i_full_points_3) + &
                             temp_rho(i_point, 2) 

                    end if

                 endif

              enddo
           end if  ! end if (n_compute.gt.0)
        ! end if ! end distribution over threads
     end do ! end loop over batches


  ! finally, deallocate stuff.

  if (allocated(ylm_tab)) then
     deallocate(ylm_tab)
  end if
  if (allocated(index_lm)) then
     deallocate(index_lm)
  end if

  if (allocated(KS_orbital_gradient)) then     
     deallocate(KS_orbital_gradient)
  end if

  if (allocated(KS_orbital_gradient_complex)) then     
     deallocate(KS_orbital_gradient_complex)
  end if

  if(allocated(radial_wave))then
     deallocate(radial_wave)
  end if
  if(allocated(wave))then
     deallocate(wave)
  end if
  if(allocated(wave_c))then
     deallocate(wave_c)
  end if
  if(allocated(i_basis))then
     deallocate(i_basis)
  end if
  if( allocated(KS_vec_one))then
     deallocate(KS_vec_one)
  end if
  if( allocated(KS_vec_two))then
     deallocate(KS_vec_two)
  end if
  if( allocated(KS_vec_one_complex))then
     deallocate(KS_vec_one_complex)
  end if
  if( allocated(KS_vec_two_complex))then
     deallocate(KS_vec_two_complex)
  end if
  if(allocated( KS_ev_compute_one))then
     deallocate( KS_ev_compute_one)
  end if
  if(allocated( KS_ev_compute_two))then
     deallocate( KS_ev_compute_two)
  end if
  if(allocated( KS_ev_compute_one_complex))then
     deallocate( KS_ev_compute_one_complex)
  end if
  if(allocated( KS_ev_compute_two_complex))then
     deallocate( KS_ev_compute_two_complex)
  end if
  if(allocated( KS_orbital_one))then
     deallocate( KS_orbital_one)
  end if
  if(allocated( KS_orbital_two))then
     deallocate( KS_orbital_two)
  end if
  if(allocated( KS_orbital_one_complex))then
     deallocate( KS_orbital_one_complex)
  end if
  if(allocated( KS_orbital_two_complex))then
     deallocate( KS_orbital_two_complex)
  end if
  if(allocated( KS_vec_times_occ_sqrt))then
     deallocate( KS_vec_times_occ_sqrt)
  end if
  if(allocated( KS_ev_compute))then
     deallocate( KS_ev_compute)
  end if
  if(allocated( KS_orbital))then
     deallocate( KS_orbital)
  end if
  if(allocated( KS_vec_times_occ_sqrt_complex))then
     deallocate( KS_vec_times_occ_sqrt_complex)
  end if
  if(allocated( KS_ev_compute_complex))then
     deallocate( KS_ev_compute_complex)
  end if
  if(allocated( KS_orbital_complex))then
     deallocate( KS_orbital_complex)
  end if

end subroutine get_transition_density
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_fit_pbc
!  NAME
!    esp_fit
!  SYNOPSIS
subroutine esp_fit_pbc(potential_std,partition_tab_std,esp_charges,esp_dipole,&
                   density_full,output_esp,rm,km,R_c,out_pot,use_dip_for_cube)

!  PURPOSE
!    Fits the esp charges centered at the atoms to the Hartree potential using
!    a least squares fit
!
!  USES
  use species_data,only:species_name
  use dimensions,only:n_atoms,n_k_points,esp_constraint
  use pbc_lists,only:k_point_list
  use runtime_choices,only:out_esp_full,calculate_work_function,&
                           calculate_work_function,dipole_correction_method,&
                           use_dipole_correction,vacuum_z_level,charge
  use load_balancing,only:use_batch_permutation,batch_perm,get_batch_weights
  use geometry,only:species,coords,cell_volume,recip_lattice_vector,&
                    lattice_vector
  use mpi_tasks,only:myid,mpi_wtime,check_allocation
  use synchronize_mpi_basic,only:sync_vector,sync_matrix,sync_real_number,&
                                 sync_integer
  use localorb_io,only:OL_norm,localorb_info
  use hartree_potential_recip,only:evaluate_dipole_correction_from_dipole,&
      evaluate_dipole_correction
  use esp_grids
  use constants,only: pi4,sqrt_pi,pi,bohr,hartree
  use arch_specific, only: arch_erfc
!  ARGUMENTS
!  INPUTS
!   o potential_std -- Hartree potential
!   o partition_tab_std -- partition tab
!   o output_esp -- trigger output
!  OUTPUT
!   o esp_charges -- esp charges fitted to potential
!   o esp_dipole --  dipole moment calculated from fitted charges
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none

  real*8, target, dimension(n_full_points_esp),INTENT(IN):: potential_std
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: partition_tab_std
  real*8, dimension(n_atoms),INTENT(OUT)        ::  esp_charges
  real*8, dimension(3) ,INTENT(OUT)       ::  esp_dipole
  logical,INTENT(IN)                                 :: density_full
  logical,INTENT(IN)                                 :: output_esp
  integer,INTENT(IN)                                :: rm
  integer,INTENT(IN)                                :: km
  real*8,INTENT(IN)                                :: R_c
  logical,INTENT(IN)                                 :: out_pot
  logical,INTENT(IN)                                 :: use_dip_for_cube

  integer :: info
  character*100 :: info_str

  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                         ! actually used
  ! Timing
  real*8, allocatable :: batch_times(:)
  real*8 time_start
  ! Pointers to the actually used array

  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)

  integer n_bp

  real*8,  dimension(:,:), allocatable :: a
  integer,  dimension(:), allocatable :: work
  real*8,  dimension(:), allocatable :: b
  real*8,  dimension(:), allocatable :: out_charge
  real*8,  dimension(:), allocatable :: pot_prime
  real*8  :: pot_sum
  real*8  :: di_r,di_k,di_r2,di_k2
  integer :: i_atom, j_atom
  integer :: all_points
  real*8  :: total_diff, diff, Vol, k_sum, k_sum_j
  real*8  :: coord_current(3)
  real*8  :: dir_r(3)
  real*8  :: dir_k(3)
  real*8  :: dir_r2(3)
  real*8  :: dir_k2(3)
  real*8  :: k_lattvec(3)
  real*8  :: r_lattvec(3)
  integer :: k_nvec(3)
  integer :: r_nvec(3)
  real*8  :: alpha,r_sum_1,r_sum_2, off_set
  !  counters
  integer i_batch
  integer i_index
  integer :: i_full_points
  integer :: n_x,n_y,n_z,rn_x,rn_y,rn_z

  real*8:: dip_gradient, dip_origin, dip_lenght, &
           dip_coord_current,dip_corr
  integer :: i_m
  real*8, save :: previous_average_delta_v_hartree_real = 0.d0
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

    partition_tab => batch_perm(n_bp)%partition_tab
    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
  endif

  if(get_batch_weights) then
    allocate(batch_times(n_my_batches_work))
    batch_times(:) = 0
  endif

  dip_corr = 0d0
  if((use_dipole_correction .or. calculate_work_function) .and. out_pot &
      .and. use_dip_for_cube) then
        if(dipole_correction_method=='potential') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via potential gradient'
             endif
             call evaluate_dipole_correction & 
            ( previous_average_delta_v_hartree_real, dip_gradient, dip_origin, &
              dip_lenght,.true. )
        elseif(dipole_correction_method=='dipole') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via slab dipole moment'
             endif
           call evaluate_dipole_correction_from_dipole &
           (previous_average_delta_v_hartree_real, dip_gradient,dip_origin,&
            dip_lenght,.true.)
        endif
   end if
  !ESP-charge fitting statrts here
  allocate(a(n_atoms+1,n_atoms+1),stat=info)
  call check_allocation(info, 'a')
  allocate(b(n_atoms+1),stat=info)
  call check_allocation(info, 'b')
  allocate(pot_prime(n_atoms),stat=info)
  call check_allocation(info, 'pot_prime')
  alpha = (sqrt_pi/(R_c))
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      all_points = 0
      a = 0d0
      b = 0d0
      pot_prime = 0d0
      pot_sum = 0d0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()
        ! loop over one batch
        Vol = 0
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            all_points = all_points + 1
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            do i_atom = 1, n_atoms, 1
                r_sum_1 = 0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r(1)=coord_current(1)-(coords(1,i_atom)+r_lattvec(1))
                  dir_r(2)=coord_current(2)-(coords(2,i_atom)+r_lattvec(2))
                  dir_r(3)=coord_current(3)-(coords(3,i_atom)+r_lattvec(3))
                  di_r=sqrt(sum(dir_r**2))
                  if (di_r.le.R_c)then
                    r_sum_1  = r_sum_1 + arch_erfc(alpha*&
                                         di_r)/di_r
                  endif

                enddo
                enddo
                enddo
                k_sum = 0
                dir_k(1)=coord_current(1)-coords(1,i_atom)
                dir_k(2)=coord_current(2)-coords(2,i_atom)
                dir_k(3)=coord_current(3)-coords(3,i_atom)
                di_k=sqrt(sum(dir_k**2))
                if (di_k.le.R_c)then
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                  k_nvec(1) = n_x
                  k_nvec(2) = n_y
                  k_nvec(3) = n_z
                  if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                     .ne.0)then
                  k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                   k_nvec(1:3))
                  k_sum =  k_sum + cos(dot_product(k_lattvec,dir_k))*&
                           (exp(-(sum(k_lattvec**2))/&
                           (4.*(alpha**2)))/sum(k_lattvec**2))
                  endif
                  enddo
                  enddo
                  enddo
                endif
                pot_prime(i_atom)  = pot_prime(i_atom) + r_sum_1+(pi4/&
                                     cell_volume)*k_sum
            enddo
            pot_sum = pot_sum + potential(i_full_points)
          !endif
        enddo
      enddo
      call sync_vector(pot_prime,n_atoms)
      call sync_real_number(pot_sum)
      call sync_integer(all_points)
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      a = 0
      b = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            do i_atom = 1, n_atoms, 1
                r_sum_1 = 0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r(1)=coord_current(1)-(coords(1,i_atom)+r_lattvec(1))
                  dir_r(2)=coord_current(2)-(coords(2,i_atom)+r_lattvec(2))
                  dir_r(3)=coord_current(3)-(coords(3,i_atom)+r_lattvec(3))
                  di_r=sqrt(sum(dir_r**2))
                  if (di_r.le.R_c)then
                    r_sum_1  = r_sum_1 + arch_erfc(alpha*&
                                         di_r)/di_r
                  endif
                enddo
                enddo
                enddo
                dir_k(1)=coord_current(1)-coords(1,i_atom)
                dir_k(2)=coord_current(2)-coords(2,i_atom)
                dir_k(3)=coord_current(3)-coords(3,i_atom)
                di_k=sqrt(sum(dir_k**2))
                k_sum = 0
                if (di_k.le.R_c)then
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                    k_nvec(1) = n_x
                    k_nvec(2) = n_y
                    k_nvec(3) = n_z
                    if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                      .ne.0)then
                    k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                     k_nvec(1:3))
                    k_sum =  k_sum + cos(dot_product(k_lattvec,dir_k))*&
                             (exp(-(sum(k_lattvec**2))/&
                             (4.*(alpha**2)))/sum(k_lattvec**2))
                    endif
                  enddo
                  enddo
                  enddo
                endif
!HO probably sign error in potential and thus pot_sum
                b(i_atom) = b(i_atom)-(potential(i_full_points)-&
                            pot_sum/all_points)*((r_sum_1+(pi4/cell_volume)*&
                            k_sum)-pot_prime(i_atom)/all_points)
              do j_atom = 1, n_atoms, 1
                r_sum_2  = 0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r2(1)=coord_current(1)-(coords(1,j_atom)+r_lattvec(1))
                  dir_r2(2)=coord_current(2)-(coords(2,j_atom)+r_lattvec(2))
                  dir_r2(3)=coord_current(3)-(coords(3,j_atom)+r_lattvec(3))
                  di_r2=sqrt(sum(dir_r2**2))
                  if (di_r2.le.R_c)then
                    r_sum_2  = r_sum_2 + arch_erfc(alpha*&
                                         di_r2)/di_r2
                  endif
                enddo
                enddo
                enddo
                dir_k2(1)=coord_current(1)-coords(1,j_atom)
                dir_k2(2)=coord_current(2)-coords(2,j_atom)
                dir_k2(3)=coord_current(3)-coords(3,j_atom)
                di_k2=sqrt(sum(dir_k2**2))
                if (di_k2.le.R_c)then
                  k_sum_j = 0
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                    k_nvec(1) = n_x
                    k_nvec(2) = n_y
                    k_nvec(3) = n_z
                    if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                       .ne.0)then
                    k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                     k_nvec(1:3))
                    k_sum_j =  k_sum_j + cos(dot_product(k_lattvec,dir_k2))*&
                             (exp(-(sum(k_lattvec**2))/&
                             (4.*(alpha**2)))/sum(k_lattvec**2))
                    endif
                  enddo
                  enddo
                  enddo
                endif
                a(i_atom,j_atom) = a(i_atom,j_atom) + &
                                   ((r_sum_1+(pi4/cell_volume)*&
                                   k_sum)- pot_prime(i_atom)/all_points)*&
                                   ((r_sum_2+(pi4/&
                                   cell_volume)*k_sum_j)- pot_prime(j_atom)/&
                                   all_points)
              enddo
            enddo
          !endif
        enddo
      enddo
      call sync_vector(b,n_atoms+1)
      call sync_matrix(a,n_atoms+1 ,n_atoms+1 )
      a(n_atoms+1,1:n_atoms) = 1d0
      a(1:n_atoms,n_atoms+1) = 1d0
      a(n_atoms+1,n_atoms+1) = 0d0
      b(n_atoms+1) = charge
      if (esp_constraint.eq.1)then
        write(use_unit,'(2X,A)') &
                       "| Using supplied constraints for method 1"
        do i_atom = 1, n_atoms
          a(i_atom,i_atom) = a(i_atom,i_atom) + &
                             (esp_atom_constraint(1,i_atom)/2.0)*&
                              esp_atom_constraint(2,i_atom)
          b(i_atom) = b(i_atom) + &
                             (esp_atom_constraint(1,i_atom)/2.0)*&
                              esp_atom_constraint(3,i_atom)
        enddo
      endif
      allocate(work(n_atoms+1),stat=info)
      call check_allocation(info, 'work')
      if (myid==0)then
!debug        call print_matrix( 'a', n_atoms+2, n_atoms+2, a, n_atoms+2 )
!debug        call print_matrix( 'b', n_atoms+2, 1, b, n_atoms+2 )
        call dgesv(n_atoms+1, 1, a, n_atoms+1, work, b, n_atoms+1 ,info)
      if( info.gt.0 ) then
         write(use_unit,'(2X,A)')'The diagonal element of the triangular factor of A,'
         write(use_unit,'(2X,A,I8,A,I8,A)')'U(',info,',',info,') is zero, so that'
         write(use_unit,'(2X,A)')'A is singular; the solution could not be computed.'
      endif
!debug        call print_matrix( 'Solution', n_atoms+2, 1, b, n_atoms+2 )
!debug        call print_matrix( 'Details of LU factorization', n_atoms+2, &
!debug                            n_atoms+2, a, n_atoms+2 )
!debug        call print_int_vector( 'Pivot indices', n_atoms+2, work )     
      endif
      allocate(out_charge(n_atoms),stat=info)
      call check_allocation(info, 'out_charge')
      out_charge = 0d0
      if (myid==0)then
         do i_atom = 1, n_atoms, 1
            out_charge(i_atom)=b(i_atom)
         enddo
      endif
      call sync_vector(out_charge,n_atoms)
      if (myid==0.and.output_esp)then
            write(info_str,'(2X,A)') "| "
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A)') "| ESP charges fitted to the electrostatic potential : "
	    call localorb_info(info_str,use_unit,'(A)')
	    do i_atom = 1, n_atoms, 1
		write(info_str,'(2X,A)') "| "
		call localorb_info(info_str,use_unit,'(A)')
		write(info_str,'(2X,A,1X,I8,A,A,A,F15.8)') '| Atom ', i_atom&
                   , ": ",trim(species_name(species(i_atom))),", ESP charge: ",&
                   out_charge(i_atom)
		call localorb_info(info_str,use_unit,'(A)')
	    end do
	    write(info_str,'(2X,A)') " "
	    call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
      !Calculate RMS for fitted charges
      total_diff = 0
      off_set = 0
      pot_sum = 0
      i_full_points = 0
      dip_corr = 0d0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the entire 
          ! grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            if( use_dipole_correction.and. out_pot &
                .and. use_dip_for_cube)then

                i_m = int(floor((coord_current(3) - vacuum_z_level)/dip_lenght))
                dip_coord_current = coord_current(3) - i_m * dip_lenght
                  
                if( dip_coord_current <  vacuum_z_level)then
                  dip_coord_current = dip_coord_current + dip_lenght
                end if
      
                dip_corr = (dip_coord_current-dip_origin) * dip_gradient

            end if
            diff = 0
            do i_atom = 1, n_atoms, 1
                r_sum_1 = 0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r(1)=coord_current(1)-coords(1,i_atom)+r_lattvec(1)
                  dir_r(2)=coord_current(2)-coords(2,i_atom)+r_lattvec(2)
                  dir_r(3)=coord_current(3)-coords(3,i_atom)+r_lattvec(3)
                  di_r=sqrt(sum(dir_r**2))
                  if (di_r.le.R_c)then
                    r_sum_1  = r_sum_1 + arch_erfc(alpha*&
                                         di_r)/di_r
                  endif
                enddo
                enddo
                enddo
                k_sum = 0
                dir_k(1)=coord_current(1)-coords(1,i_atom)
                dir_k(2)=coord_current(2)-coords(2,i_atom)
                dir_k(3)=coord_current(3)-coords(3,i_atom)
                di_k=sqrt(sum(dir_k**2))
                if (di_k.le.R_c)then
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                  k_nvec(1) = n_x
                  k_nvec(2) = n_y
                  k_nvec(3) = n_z
                  if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                     .ne.0)then
                  k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                   k_nvec(1:3))
                  k_sum =  k_sum + cos(dot_product(k_lattvec,dir_k))*&
                           (exp(-(sum(k_lattvec**2))/&
                           (4.*alpha**2))/sum(k_lattvec**2))
                  endif
                  enddo
                  enddo
                  enddo
                endif
                if( use_dipole_correction.and. out_pot &
                    .and. use_dip_for_cube)then
                  diff = diff +  (r_sum_1+(pi4/&
                                     cell_volume)*k_sum)*out_charge(i_atom)&
                              - dip_corr
                else
                  diff = diff +  (r_sum_1+(pi4/&
                                     cell_volume)*k_sum)*out_charge(i_atom)
                endif
            enddo
!HO probably sign error in potential
            total_diff = total_diff + (potential(i_full_points) - diff)**2
                off_set = off_set +(-potential(i_full_points) - diff)
                pot_sum = pot_sum + potential(i_full_points)**2
                if (out_pot) then
                   potential(i_full_points) = diff
                endif
          !endif
        enddo
      enddo
      call sync_real_number(off_set)
      call sync_real_number(total_diff)
      call sync_real_number(pot_sum)
      endif
      do i_atom = 1, n_atoms, 1
       esp_charges(i_atom) = out_charge(i_atom)
      enddo
      if ((.not.density_full.and.output_esp).or.out_esp_full) then
        esp_dipole = 0
        do i_atom = 1, n_atoms, 1
              esp_dipole(1)= esp_dipole(1) - out_charge(i_atom)*&
                             (coords(1,i_atom))
              esp_dipole(2)= esp_dipole(2) - out_charge(i_atom)*&
                             (coords(2,i_atom))
              esp_dipole(3)= esp_dipole(3) - out_charge(i_atom)*&
                             (coords(3,i_atom))
        enddo
        write(info_str,'(2X,A,1X,F15.8,F15.8,F15.8)') "Dipole matrix element: "&
                                   , esp_dipole(1), esp_dipole(2), esp_dipole(3)
        call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
        write(info_str,'(2X,A,1X,F15.8)') "RRMS: ", sqrt(total_diff/pot_sum)
        call localorb_info(info_str,use_unit,'(A)')
	write(info_str,'(2X,A)') "| "
	call localorb_info(info_str,use_unit,'(A)')
	write(info_str,'(2X,A,1X,F15.8,A)') '| Potential offset: ', &
           (off_set/all_points)*hartree, ' eV'
	call localorb_info(info_str,use_unit,'(A)')
        ! Write one last line to bound output
        write(info_str,*) ' '
        call localorb_info(info_str,use_unit,'(A)',OL_norm)
      endif
  if(allocated(a)) deallocate(a)
  if(allocated(b)) deallocate(b)
  if(allocated(work)) deallocate(work)
  if(allocated(out_charge)) deallocate(out_charge)
  if(allocated(pot_prime)) deallocate(pot_prime)
end subroutine esp_fit_pbc
!******	
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_fit_pbc_two
!  NAME
!    esp_fit_pbc_two
!  SYNOPSIS
subroutine esp_fit_pbc_two(potential_std,partition_tab_std,esp_charges,&
                           esp_dipole,density_full,output_esp,rm,km,R_c,&
                           out_pot, use_dip_for_cube)
!  PURPOSE
!    Fits the esp charges centered at the atoms to the Hartree potential using
!    a least squares fit
!
!  USES
  use species_data,only:species_name
  use dimensions,only:n_atoms,n_k_points,esp_constraint
  use pbc_lists,only:k_point_list
  use runtime_choices,only:out_esp_full,calculate_work_function,&
                           calculate_work_function,dipole_correction_method,&
                           use_dipole_correction,vacuum_z_level,charge
  use load_balancing,only:use_batch_permutation,batch_perm,get_batch_weights
  use geometry,only:species,coords,cell_volume,recip_lattice_vector,&
                    lattice_vector
  use mpi_tasks,only:myid,mpi_wtime,check_allocation,mpi_comm_global
  use synchronize_mpi_basic,only:sync_vector,sync_matrix,sync_real_number,&
                                 sync_integer
  use hartree_potential_recip,only:evaluate_dipole_correction_from_dipole,&
      evaluate_dipole_correction
  use localorb_io,only:OL_norm,localorb_info
  use esp_grids
  use constants,only: pi4,sqrt_pi,pi,bohr,hartree
  use arch_specific, only: arch_erfc
!  ARGUMENTS
!  INPUTS
!   o potential_std -- Hartree potential
!   o partition_tab_std -- partition tab
!   o output_esp -- trigger output
!  OUTPUT
!   o esp_charges -- esp charges fitted to potential
!   o esp_dipole --  dipole moment calculated from fitted charges
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none

  real*8, target, dimension(n_full_points_esp),INTENT(INOUT):: potential_std
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: partition_tab_std
  real*8, dimension(n_atoms),INTENT(OUT)        ::  esp_charges
  real*8, dimension(3) ,INTENT(OUT)       ::  esp_dipole
  logical,INTENT(IN)                                 :: density_full
  logical,INTENT(IN)                                 :: output_esp
  integer,INTENT(IN)                                :: rm
  integer,INTENT(IN)                                :: km
  real*8                                :: R_c
  integer :: info
  character*100 :: info_str
  logical,INTENT(IN)                                 :: out_pot
  logical,INTENT(IN)                                 :: use_dip_for_cube
  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                         ! actually used
  ! Timing
  real*8, allocatable :: batch_times(:)
  real*8 time_start
  ! Pointers to the actually used array

  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)

  integer n_bp

  real*8,  dimension(:,:), allocatable :: a
  integer,  dimension(:), allocatable :: work
  real*8,  dimension(:), allocatable :: b
  real*8,  dimension(:), allocatable :: out_charge
  real*8  :: pot_sum
  real*8  :: di_r,di_r2,di_k,di_k2
  integer :: i_atom, j_atom
  integer :: all_points
  real*8  :: total_diff, diff, k_sum_1, k_sum_2
  real*8  :: coord_current(3)
  real*8  :: dir_r(3), dir_r2(3)
  real*8  :: dir_k(3), dir_k2(3)
  real*8  :: k_lattvec(3)
  real*8  :: r_lattvec(3)

  integer :: k_nvec(3)
  integer :: r_nvec(3)
  real*8  :: alpha,r_sum_1,r_sum_2
  !  counters

  integer i_batch
  integer i_index
  integer :: i_full_points
  integer :: i_points,n_x,n_y,n_z,rn_x,rn_y,rn_z

  real*8:: dip_gradient, dip_origin, dip_lenght, &
           dip_coord_current,dip_corr
  integer :: i_m
  real*8, save :: previous_average_delta_v_hartree_real = 0.d0
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

    partition_tab => batch_perm(n_bp)%partition_tab
    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
  endif

  if(get_batch_weights) then
    allocate(batch_times(n_my_batches_work))
    batch_times(:) = 0
  endif
  dip_corr = 0d0
  if((use_dipole_correction .or. calculate_work_function) .and. out_pot &
      .and. use_dip_for_cube) then
        if(dipole_correction_method=='potential') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via potential gradient'
             endif
             call evaluate_dipole_correction & 
            ( previous_average_delta_v_hartree_real, dip_gradient, dip_origin, &
              dip_lenght,.true. )
        elseif(dipole_correction_method=='dipole') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via slab dipole moment'
             endif
           call evaluate_dipole_correction_from_dipole &
           (previous_average_delta_v_hartree_real, dip_gradient,dip_origin,&
            dip_lenght,.true.)
        endif
   end if
  !ESP-charge fitting statrts here
  allocate(a(n_atoms+2,n_atoms+2),stat=info)
  call check_allocation(info, 'a')
  allocate(b(n_atoms+2),stat=info)
  call check_allocation(info, 'b')
  !alpha = (sqrt_pi/R_c)
  alpha = (3.2/(R_c))**2
      pot_sum = 0d0
      all_points = 0
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      a = 0d0
      b = 0d0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            all_points = all_points + 1
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            pot_sum = pot_sum - potential(i_full_points)
            do i_atom = 1, n_atoms, 1
                r_sum_1 = 0d0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r(1)=coord_current(1)-(coords(1,i_atom)+r_lattvec(1))
                  dir_r(2)=coord_current(2)-(coords(2,i_atom)+r_lattvec(2))
                  dir_r(3)=coord_current(3)-(coords(3,i_atom)+r_lattvec(3))
                  di_r=sqrt(sum(dir_r**2))
                  if (di_r.le.R_c)then
                    r_sum_1  = r_sum_1 + arch_erfc((sqrt(alpha))*&
                                         di_r)/di_r
                    !r_sum_1  = r_sum_1 + arch_erfc((alpha)*&
                    !                     di_r)/di_r
                  endif
                enddo
                enddo
                enddo
                dir_k(1)=coord_current(1)-coords(1,i_atom)
                dir_k(2)=coord_current(2)-coords(2,i_atom)
                dir_k(3)=coord_current(3)-coords(3,i_atom)
                di_k=sqrt(sum(dir_k**2))
                k_sum_1 = 0d0
                if (di_k.le.R_c)then
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                    k_nvec(1) = n_x
                    k_nvec(2) = n_y
                    k_nvec(3) = n_z
                    if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                      .ne.0)then
                      k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                     k_nvec(1:3))
                      k_sum_1 =  k_sum_1 + cos(dot_product(k_lattvec,dir_k))*&
                             !(exp(-(sum(k_lattvec**2))/&
                             !(4.*alpha**2))/sum(k_lattvec**2))
                             (exp(-(sum(k_lattvec**2)*pi**2)/&
                             ((alpha)))/sum(k_lattvec**2))
                    endif
                  enddo
                  enddo
                  enddo
                endif
!HO here the sign error in the potential was already fixed
                b(i_atom) = b(i_atom)+((-potential(i_full_points))&
                            *(r_sum_1+(pi4/cell_volume)*&
                            k_sum_1))
                a(n_atoms+1,i_atom) = a(n_atoms+1,i_atom) +&
                                    (r_sum_1+(pi4/cell_volume)*k_sum_1)
                a(i_atom,n_atoms+1) = a(i_atom,n_atoms+1) + &
                                    (r_sum_1+(pi4/cell_volume)*k_sum_1)
              do j_atom = 1, n_atoms, 1
                r_sum_2  = 0d0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r2(1)=coord_current(1)-(coords(1,j_atom)+r_lattvec(1))
                  dir_r2(2)=coord_current(2)-(coords(2,j_atom)+r_lattvec(2))
                  dir_r2(3)=coord_current(3)-(coords(3,j_atom)+r_lattvec(3))
                  di_r2=sqrt(sum(dir_r2**2))
                  if (di_r2.le.R_c)then
                    !r_sum_2  = r_sum_2 + arch_erfc((alpha)*&
                    !                     di_r2)/di_r2
                    r_sum_2  = r_sum_2 + arch_erfc(sqrt(alpha)*&
                                         di_r2)/di_r2
                  endif
                enddo
                enddo
                enddo
                dir_k2(1)=coord_current(1)-coords(1,j_atom)
                dir_k2(2)=coord_current(2)-coords(2,j_atom)
                dir_k2(3)=coord_current(3)-coords(3,j_atom)
                di_k2=sqrt(sum(dir_k2**2))
                k_sum_2 = 0
                if (di_k2.le.R_c)then
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                    k_nvec(1) = n_x
                    k_nvec(2) = n_y
                    k_nvec(3) = n_z
                    if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                       .ne.0)then
                      k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                     k_nvec(1:3))
                      k_sum_2 = k_sum_2 + cos(dot_product(k_lattvec,dir_k2))*&
                             !(exp(-(sum(k_lattvec**2))/&
                             !(4.*alpha**2))/sum(k_lattvec**2))
                             (exp(-(sum(k_lattvec**2)*pi**2)/&
                             (alpha))/sum(k_lattvec**2))
                    endif
                  enddo
                  enddo
                  enddo
                endif
                a(i_atom,j_atom) = a(i_atom,j_atom) + &
                                   (r_sum_1+(pi4/cell_volume)*k_sum_1)*&
                                   (r_sum_2+(pi4/cell_volume)*k_sum_2)
              enddo
            enddo
          !endif
        enddo
      enddo
      a(n_atoms+1,n_atoms+1) = dble(all_points)
      b(n_atoms+1) = pot_sum
      call sync_vector(b,n_atoms+2)
      call sync_matrix(a,n_atoms+2 ,n_atoms+2 )
      a(n_atoms+2,1:n_atoms) = 1d0
      a(1:n_atoms,n_atoms+2) = 1d0
      a(n_atoms+1,n_atoms+2) = 0d0
      a(n_atoms+2,n_atoms+1) = 0d0
      a(n_atoms+2,n_atoms+2) = 0d0
      b(n_atoms+2) = charge
      if (esp_constraint.eq.2)then
        write(use_unit,'(2X,A)') &
                       "| Using supplied constraints for method 2"
        do i_atom = 1, n_atoms
          a(i_atom,i_atom) = a(i_atom,i_atom) + &
                             esp_atom_constraint(1,i_atom)
          b(i_atom) = b(i_atom) + &
                              esp_atom_constraint(1,i_atom)*&
                              esp_atom_constraint(2,i_atom)
        enddo
      endif
      allocate(work(n_atoms+2),stat=info)
      call check_allocation(info, 'work')
      if (myid==0)then
!debug        call print_matrix( 'a', n_atoms+2, n_atoms+2, a, n_atoms+2 )
!debug        call print_matrix( 'b', n_atoms+2, 1, b, n_atoms+2 )
        call dgesv(n_atoms+2, 1, a, n_atoms+2, work, b, n_atoms+2 ,info)
      if( info.gt.0 ) then
         write(use_unit,'(2X,A)')'The diagonal element of the triangular factor of A,'
         write(use_unit,'(2X,A,I8,A,I8,A)')'U(',info,',',info,') is zero, so that'
         write(use_unit,'(2X,A)')'A is singular; the solution could not be computed.'
      endif
!debug        call print_matrix( 'Solution', n_atoms+2, 1, b, n_atoms+2 )
!debug        call print_matrix( 'Details of LU factorization', n_atoms+2, &
!debug                            n_atoms+2, a, n_atoms+2 )
!debug        call print_int_vector( 'Pivot indices', n_atoms+2, work )     
      endif
      allocate(out_charge(n_atoms+1),stat=info)
      call check_allocation(info, 'out_charge')
      out_charge = 0d0
      if (myid==0)then
         do i_atom = 1, n_atoms, 1
            out_charge(i_atom)=b(i_atom)
         enddo
         out_charge(n_atoms+1)=b(n_atoms+1)
      endif
      call sync_vector(out_charge,n_atoms+1)
      if (myid==0.and.output_esp)then
            write(info_str,'(2X,A)') "| "
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A)') "| ESP charges fitted to the electrostatic potential : "
	    call localorb_info(info_str,use_unit,'(A)')
	    do i_atom = 1, n_atoms, 1
		write(info_str,'(2X,A)') "| "
		call localorb_info(info_str,use_unit,'(A)')
		write(info_str,'(2X,A,1X,I8,A,A,A,F15.8,F15.8,F15.8,F15.8)') '| Atom ', i_atom&
                   , ": ",trim(species_name(species(i_atom))),", ESP charge: ",&
                   out_charge(i_atom)
		call localorb_info(info_str,use_unit,'(A)')
	    end do
	    write(info_str,'(2X,A)') "| "
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A,1X,F15.8,A)') '| Potential offset: ', &
                   out_charge(n_atoms+1)*hartree, ' eV'
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A)') " "
	    call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
      !Calculate RMS for fitted charges
      total_diff = 0
      i_full_points = 0
      i_points = 0
      pot_sum = 0
      dip_corr = 0d0
      
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the entire 
          ! grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            i_points = i_points + 1
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            if( use_dipole_correction.and. out_pot &
                .and. use_dip_for_cube)then

                i_m = int(floor((coord_current(3) - vacuum_z_level)/dip_lenght))
                dip_coord_current = coord_current(3) - i_m * dip_lenght
                  
                if( dip_coord_current <  vacuum_z_level)then
                  dip_coord_current = dip_coord_current + dip_lenght
                end if
      
                dip_corr = (dip_coord_current-dip_origin) * dip_gradient

            end if
            diff = 0d0
            do i_atom = 1, n_atoms, 1
                r_sum_1 = 0
                do rn_x=-rm,rm,1
                do rn_y=-rm,rm,1
                do rn_z=-rm,rm,1
                  r_nvec(1) = rn_x
                  r_nvec(2) = rn_y
                  r_nvec(3) = rn_z
                  r_lattvec(1:3) = matmul(lattice_vector,&
                                   r_nvec(1:3))
                  dir_r(1)=coord_current(1)-(coords(1,i_atom)+r_lattvec(1))
                  dir_r(2)=coord_current(2)-(coords(2,i_atom)+r_lattvec(2))
                  dir_r(3)=coord_current(3)-(coords(3,i_atom)+r_lattvec(3))
                  di_r=sqrt(sum(dir_r**2))
                  if (di_r.le.R_c)then
                    !r_sum_1  = r_sum_1 + arch_erfc((alpha)*&
                    !                     di_r)/di_r
                    r_sum_1  = r_sum_1 + arch_erfc(sqrt(alpha)*&
                                         di_r)/di_r
                  endif
                enddo
                enddo
                enddo
                dir_k(1)=coord_current(1)-coords(1,i_atom)
                dir_k(2)=coord_current(2)-coords(2,i_atom)
                dir_k(3)=coord_current(3)-coords(3,i_atom)
                di_k=sqrt(sum(dir_k**2))
                k_sum_1 = 0
                if (di_k.le.R_c)then
                  do n_x=-km,km,1
                  do n_y=-km,km,1
                  do n_z=-km,km,1
                    k_nvec(1) = n_x
                    k_nvec(2) = n_y
                    k_nvec(3) = n_z
                  if (sqrt(sum(k_nvec**2.)).le.km.and.sqrt(sum(k_nvec**2.))&
                     .ne.0)then
                      k_lattvec(1:3) = matmul(recip_lattice_vector,&
                                   k_nvec(1:3))
                      k_sum_1 =  k_sum_1 + cos(dot_product(k_lattvec,dir_k))*&
!                           (exp(-(sum(k_lattvec**2))/&
!                           (4.*alpha**2))/sum(k_lattvec**2))
                           (exp(-(sum(k_lattvec**2))*pi**2/&
                           (alpha))/sum(k_lattvec**2))
                    endif
                  enddo
                  enddo
                  enddo
                endif
                if( use_dipole_correction.and. out_pot &
                    .and. use_dip_for_cube)then
                  diff = diff +  (r_sum_1+(pi4/&
                                     cell_volume)*k_sum_1)*out_charge(i_atom)&
                              - dip_corr
                else
                  diff = diff +  (r_sum_1+(pi4/&
                                     cell_volume)*k_sum_1)*out_charge(i_atom)
                endif
            enddo
                total_diff = total_diff + ((diff + out_charge(n_atoms+1)) +  &
                                         potential(i_full_points))**2
                pot_sum = pot_sum + potential(i_full_points)**2
                if (out_pot) then
                   potential(i_full_points) = diff + out_charge(n_atoms+1)
                endif
          !endif
        enddo
      enddo
      call sync_real_number(total_diff)
      call sync_real_number(pot_sum)
      endif

      do i_atom = 1, n_atoms, 1
       esp_charges(i_atom) = out_charge(i_atom)
      enddo
      if ((.not.density_full.and.output_esp).or.out_esp_full) then
        esp_dipole = 0
        do i_atom = 1, n_atoms, 1
              esp_dipole(1)= esp_dipole(1) - out_charge(i_atom)*&
                             (coords(1,i_atom))
              esp_dipole(2)= esp_dipole(2) - out_charge(i_atom)*&
                             (coords(2,i_atom))
              esp_dipole(3)= esp_dipole(3) - out_charge(i_atom)*&
                             (coords(3,i_atom))
        enddo
        write(info_str,'(2X,A,1X,F15.8,F15.8,F15.8)') "Dipole matrix element: "&
                                   , esp_dipole(1), esp_dipole(2), esp_dipole(3)
        call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
        write(info_str,'(2X,A,1X,F15.8,F15.8,F15.8)')  "RRMS: ", &
                                                   sqrt(total_diff/pot_sum)
        call localorb_info(info_str,use_unit,'(A)')
        ! Write one last line to bound output
        write(info_str,*) ' '
        call localorb_info(info_str,use_unit,'(A)',OL_norm)
      endif
  if(allocated(a)) deallocate(a)
  if(allocated(b)) deallocate(b)
  if(allocated(work)) deallocate(work)
  if(allocated(out_charge)) deallocate(out_charge)

end subroutine esp_fit_pbc_two
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_fit_pbc_three
!  NAME
!    esp_fit
!  SYNOPSIS
subroutine esp_fit_pbc_three(potential_std,partition_tab_std,esp_charges,&
                             esp_dipole,density_full,output_esp,R_c,&
                             out_pot,use_dip_for_cube)
!  PURPOSE
!    Fits the esp charges centered at the atoms to the Hartree potential using
!    a least squares fit
!
!  USES
  use species_data,only:species_name
  use dimensions,only:n_atoms,n_k_points,esp_constraint
  use pbc_lists,only:k_point_list
  use runtime_choices,only:out_esp_full,calculate_work_function,&
                           calculate_work_function,dipole_correction_method,&
                           use_dipole_correction,vacuum_z_level,charge
  use load_balancing,only:use_batch_permutation,batch_perm,get_batch_weights
  use geometry,only:species,coords,cell_volume,recip_lattice_vector,&
                    lattice_vector
  use mpi_tasks,only:myid,mpi_wtime,check_allocation
  use synchronize_mpi_basic,only:sync_vector,sync_matrix,sync_real_number,&
                                 sync_integer
  use localorb_io,only:OL_norm,localorb_info
  use hartree_potential_recip,only:evaluate_dipole_correction_from_dipole,&
      evaluate_dipole_correction
  use esp_grids
  use constants,only: pi4,sqrt_pi,pi,bohr,hartree
  use arch_specific, only: arch_erfc
!  ARGUMENTS
!  INPUTS
!   o potential_std -- Hartree potential
!   o partition_tab_std -- partition tab
!   o output_esp -- trigger output
!  OUTPUT
!   o esp_charges -- esp charges fitted to potential
!   o esp_dipole --  dipole moment calculated from fitted charges
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none

  real*8, target, dimension(n_full_points_esp),INTENT(IN):: potential_std
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: partition_tab_std
  real*8, dimension(n_atoms),INTENT(OUT)        ::  esp_charges
  real*8, dimension(3) ,INTENT(OUT)       ::  esp_dipole
  logical,INTENT(IN)                                 :: density_full
  logical,INTENT(IN)                                 :: output_esp
  real*8,INTENT(IN)                                :: R_c
  logical,INTENT(IN)                                 :: out_pot
  logical,INTENT(IN)                                 :: use_dip_for_cube
  integer :: info
  character*100 :: info_str

  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                         ! actually used
  ! Timing
  real*8, allocatable :: batch_times(:)
  real*8 time_start
  ! Pointers to the actually used array

  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)

  integer n_bp

  real*8,  dimension(:,:), allocatable :: a
  integer,  dimension(:), allocatable :: work
  real*8,  dimension(:), allocatable :: b
  real*8,  dimension(:), allocatable :: out_charge
  real*8  :: pot_sum
  real*8  :: di_r,di_r2
  integer :: i_atom, j_atom
  integer :: all_points
  real*8  :: total_diff, diff
  real*8  :: coord_current(3)
  real*8  :: dir_r(3)
  real*8  :: dir_r2(3)
  real*8  :: alpha
  !  counters

  integer i_batch
  integer i_index
  integer :: i_full_points
  integer :: i_points

  real*8:: dip_gradient, dip_origin, dip_lenght, &
           dip_coord_current,dip_corr
  integer :: i_m
  real*8, save :: previous_average_delta_v_hartree_real = 0.d0
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

    partition_tab => batch_perm(n_bp)%partition_tab
    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
  endif

  if(get_batch_weights) then
    allocate(batch_times(n_my_batches_work))
    batch_times(:) = 0
  endif
  dip_corr = 0d0
  if((use_dipole_correction .or. calculate_work_function) .and. out_pot &
      .and. use_dip_for_cube) then
        if(dipole_correction_method=='potential') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via potential gradient'
             endif
             call evaluate_dipole_correction & 
            ( previous_average_delta_v_hartree_real, dip_gradient, dip_origin, &
              dip_lenght,.true. )
        elseif(dipole_correction_method=='dipole') then
             if (output_esp) then
               write(info_str,*) 'Calculating dipole correction via slab dipole moment'
             endif
           call evaluate_dipole_correction_from_dipole &
           (previous_average_delta_v_hartree_real, dip_gradient,dip_origin,&
            dip_lenght,.true.)
        endif
   end if
  !ESP-charge fitting statrts here
  allocate(a(n_atoms+2,n_atoms+2),stat=info)
  call check_allocation(info, 'a')
  allocate(b(n_atoms+2),stat=info)
  call check_allocation(info, 'b')
  !alpha = (sqrt_pi/R_c)
  alpha = (2.3/R_c)**2
      pot_sum = 0d0
      all_points = 0
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      a = 0d0
      b = 0d0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            all_points = all_points + 1
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            pot_sum = pot_sum + potential(i_full_points)
            do i_atom = 1, n_atoms, 1
                dir_r(1)=coord_current(1)-coords(1,i_atom)
                dir_r(2)=coord_current(2)-coords(2,i_atom)
                dir_r(3)=coord_current(3)-coords(3,i_atom)
                di_r=sqrt(sum(dir_r**2))
              if (di_r.le.R_c)then
!HO probably sign error in potential
                b(i_atom) = b(i_atom)+(-potential(i_full_points)&
                            *(arch_erfc(sqrt(alpha)*&
                                     di_r)/di_r-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     exp(-(alpha)*&
                                     R_c**2)/R_c)*(di_r-R_c)))
                a(n_atoms+1,i_atom) = a(n_atoms+1,i_atom) +&
                                    (arch_erfc(sqrt(alpha)*&
                                     di_r)/di_r-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     exp(-(alpha)*&
                                     R_c**2)/R_c)*(di_r-R_c))
                a(i_atom,n_atoms+1) = a(i_atom,n_atoms+1) + &
                                    (arch_erfc(sqrt(alpha)*&
                                     di_r)/di_r-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     exp(-(alpha)*&
                                     R_c**2)/R_c)*(di_r-R_c))

                do j_atom = 1, n_atoms, 1
                  dir_r2(1)=coord_current(1)-coords(1,j_atom)
                  dir_r2(2)=coord_current(2)-coords(2,j_atom)
                  dir_r2(3)=coord_current(3)-coords(3,j_atom)
                  di_r2=sqrt(sum(dir_r2**2))
                  if (di_r2.le.R_c)then
                    a(i_atom,j_atom) = a(i_atom,j_atom) + &
                                   (arch_erfc((alpha)*&
                                     di_r)/di_r-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     arch_erfc(-(alpha)*&
                                     R_c**2)/R_c)*(di_r-R_c))*&
                                   (arch_erfc((alpha)*&
                                     di_r2)/di_r2-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     exp(-(alpha)*&
                                     R_c**2)/R_c)*(di_r2-R_c))
                  endif
                enddo
              endif
            enddo
          !endif
        enddo
      enddo
      call sync_integer(all_points)
      call sync_real_number(pot_sum)
      call sync_vector(b,n_atoms+2)
      call sync_matrix(a,n_atoms+2 ,n_atoms+2 )
      a(n_atoms+1,n_atoms+1) = dble(all_points)
      a(n_atoms+2,1:n_atoms) = 1d0
      a(1:n_atoms,n_atoms+2) = 1d0
      a(n_atoms+1,n_atoms+2) = 0d0
      a(n_atoms+2,n_atoms+1) = 0d0
      a(n_atoms+2,n_atoms+2) = 0d0
      b(n_atoms+1) = pot_sum
      b(n_atoms+2) = charge
      if (esp_constraint.eq.2)then
        write(use_unit,'(2X,A)') &
                       "| Using supplied constraints for method 2"
        do i_atom = 1, n_atoms
          a(i_atom,i_atom) = a(i_atom,i_atom) + &
                             esp_atom_constraint(1,i_atom)
          b(i_atom) = b(i_atom) + &
                              esp_atom_constraint(1,i_atom)*&
                              esp_atom_constraint(2,i_atom)
        enddo
      endif
      allocate(work(n_atoms+2),stat=info)
      call check_allocation(info, 'work')
      if (myid==0)then
!debug        call print_matrix( 'a', n_atoms+2, n_atoms+2, a, n_atoms+2 )
!debug        call print_matrix( 'b', n_atoms+2, 1, b, n_atoms+2 )
        call dgesv(n_atoms+2, 1, a, n_atoms+2, work, b, n_atoms+2 ,info)
      if( info.gt.0 ) then
         write(use_unit,'(2X,A)')'The diagonal element of the triangular factor of A,'
         write(use_unit,'(2X,A,I8,A,I8,A)')'U(',info,',',info,') is zero, so that'
         write(use_unit,'(2X,A)')'A is singular; the solution could not be computed.'
      endif
!debug        call print_matrix( 'Solution', n_atoms+2, 1, b, n_atoms+2 )
!debug        call print_matrix( 'Details of LU factorization', n_atoms+2, &
!debug                            n_atoms+2, a, n_atoms+2 )
!debug        call print_int_vector( 'Pivot indices', n_atoms+2, work )     
      endif
      allocate(out_charge(n_atoms+1),stat=info)
      call check_allocation(info, 'out_charge')
      out_charge = 0d0
      if (myid==0)then
         do i_atom = 1, n_atoms, 1
            out_charge(i_atom)=b(i_atom)
         enddo
         out_charge(n_atoms+1)=b(n_atoms+1)
      endif
      call sync_vector(out_charge,n_atoms+1)
      if (myid==0.and.output_esp)then
            write(info_str,'(2X,A)') "| "
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A)') "| ESP charges fitted to the electrostatic potential : "
	    call localorb_info(info_str,use_unit,'(A)')
	    do i_atom = 1, n_atoms, 1
		write(info_str,'(2X,A)') "| "
		call localorb_info(info_str,use_unit,'(A)')
		write(info_str,'(2X,A,1X,I8,A,A,A,F15.8)') '| Atom ', i_atom&
                   , ": ",trim(species_name(species(i_atom))),", ESP charge: ",&
                   out_charge(i_atom)
		call localorb_info(info_str,use_unit,'(A)')
	    end do
	    write(info_str,'(2X,A)') "| "
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A,1X,F15.8,A)') '| Potential offset: ', &
                   out_charge(n_atoms+1)*hartree, ' eV'
	    call localorb_info(info_str,use_unit,'(A)')
	    write(info_str,'(2X,A)') " "
	    call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
      !Calculate RMS for fitted charges
      total_diff = 0d0
      pot_sum = 0d0
      i_full_points = 0
      i_points = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the entire 
          ! grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          !if (partition_tab(i_full_points).gt.0.d0) then
            i_points = i_points + 1
            ! get current integration point coordinate
            coord_current(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            if( use_dipole_correction.and. out_pot &
                .and. use_dip_for_cube)then

                i_m = int(floor((coord_current(3) - vacuum_z_level)/dip_lenght))
                dip_coord_current = coord_current(3) - i_m * dip_lenght
                  
                if( dip_coord_current <  vacuum_z_level)then
                  dip_coord_current = dip_coord_current + dip_lenght
                end if
      
                dip_corr = (dip_coord_current-dip_origin) * dip_gradient

            end if
            diff = 0
            do i_atom = 1, n_atoms, 1
                dir_r(1)=coord_current(1)-coords(1,i_atom)
                dir_r(2)=coord_current(2)-coords(2,i_atom)
                dir_r(3)=coord_current(3)-coords(3,i_atom)
                di_r=sqrt(sum(dir_r**2))
              if (di_r.le.R_c)then
                if( use_dipole_correction.and. out_pot &
                    .and. use_dip_for_cube)then
                  diff = diff +  (arch_erfc(sqrt(alpha)*&
                                     di_r)/di_r-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     exp(-(alpha)*&
                                     R_c**2)/R_c)*(di_r-R_c))*&
                                     out_charge(i_atom)- dip_corr
                else
                  diff = diff +  (arch_erfc((alpha)*&
                                     di_r)/di_r-arch_erfc(sqrt(alpha)*&
                                     R_c)/R_c+(arch_erfc(sqrt(alpha)*&
                                     R_c)/(R_c**2)+(2.0*sqrt(alpha))/sqrt_pi*&
                                     exp(-(alpha)*&
                                     R_c**2)/R_c)*(di_r-R_c))*&
                                     out_charge(i_atom)
                endif
              endif
            enddo
!HO probably sign error in potential
               total_diff = total_diff + ((diff+out_charge(n_atoms+1))+&
                                         potential(i_full_points))**2
               pot_sum = pot_sum + potential(i_full_points)**2
               if (out_pot) then
                   potential(i_full_points) = diff + out_charge(n_atoms+1)
               endif
          !endif
        enddo
      enddo
      call sync_real_number(total_diff)
      call sync_real_number(pot_sum)
      endif
      do i_atom = 1, n_atoms, 1
       esp_charges(i_atom) = b(i_atom)
      enddo
      if ((.not.density_full.and.output_esp).or.out_esp_full) then
        esp_dipole = 0
        do i_atom = 1, n_atoms, 1
              esp_dipole(1)= esp_dipole(1) - out_charge(i_atom)*&
                             (coords(1,i_atom))
              esp_dipole(2)= esp_dipole(2) - out_charge(i_atom)*&
                             (coords(2,i_atom))
              esp_dipole(3)= esp_dipole(3) - out_charge(i_atom)*&
                             (coords(3,i_atom))
        enddo
        write(info_str,'(2X,A,1X,F15.8,F15.8,F15.8)') "Dipole matrix element: "&
                                 , esp_dipole(1), esp_dipole(2), esp_dipole(3)
        call localorb_info(info_str,use_unit,'(A)')
      endif
      if (output_esp) then
        write(info_str,'(2X,A,1X,F15.8)')  "RRMS: ", &
                                           sqrt(total_diff/pot_sum)
        call localorb_info(info_str,use_unit,'(A)')
        ! Write one last line to bound output
        write(info_str,*) ' '
        call localorb_info(info_str,use_unit,'(A)',OL_norm)
      endif
  if(allocated(a)) deallocate(a)
  if(allocated(b)) deallocate(b)
  if(allocated(work)) deallocate(work)
  if(allocated(out_charge)) deallocate(out_charge)
end subroutine esp_fit_pbc_three
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_output_cube
!  NAME
!    esp_output_cube
!  SYNOPSIS
subroutine esp_output_cubefile(cube_output,coord_output,n_x,n_y,n_z,&
                               radius_esp_max_new,i_esp,cube_rad_grid,output_select)
!  PURPOSE
!    Output the potential into a cube file
!
!  USES
  use species_data,only:species_z
  use dimensions,only:n_atoms,n_periodic,n_species
  use runtime_choices,only:output_in_original_unit_cell
  use geometry,only:species,coords,lattice_vector,frac2cart,cart2frac,&
                    periodic_unit_cell_translations
  use mpi_utilities, only:myid
  use constants,only:bohr
  use hartree_potential_recip,only:update_hartree_potential_recip
  use pbc_lists, only: coords_center
  implicit none

  real*8,     dimension(n_x*n_y*n_z) :: cube_output
  real*8,  dimension(n_x*n_y*n_z,3) :: coord_output
  integer  :: n_x,n_y,n_z
  real*8,  dimension(n_species) :: radius_esp_max_new
  integer  :: i_esp
  integer  :: output_select
  logical  :: cube_rad_grid
!  ARGUMENTS
!  INPUTS
!   o cube_output -- Array with potential data
!   o n_x, n_y, n_z -- number of points in x, y, z
!  OUTPUT
!   o cube file "potential_esp.cube"
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  real*8  local_offset(3)
  real*8  coords_temp(3,n_atoms)
  real*8, dimension(3,n_atoms) :: periodic_unit_cell_translations_cart
  integer :: descriptor
  character(LEN=15):: file_format
  character(LEN=25):: filename_esp
  integer :: blocksize, i_point, i_atom,i_coord, n_points
  real*8, dimension(3) :: d_r
  real*8, dimension(3) :: cube_coord_min
  real*8, dimension(3) :: cube_coord_max
  CHARACTER(len=1) :: valuek1
  CHARACTER(len=2) :: valuek2
  CHARACTER(len=3) :: valuek3
  CHARACTER(len=4) :: valuek4
  CHARACTER(len=5) :: valuek5
  real*8  :: lattice_vector_new(3,3)
  real*8  :: off
  integer :: i_x,i_y,i_z
  if (n_periodic.eq.0)then
    cube_coord_min = 0d0
    cube_coord_max = 0d0
    do i_atom = 1, n_atoms, 1
       if(i_atom.eq.1)then
         cube_coord_min(1) = coords_center( 1,i_atom )-&
                                  radius_esp_max_new(species(i_atom))
         cube_coord_min(2) = coords_center( 2,i_atom )-&
                                  radius_esp_max_new(species(i_atom))
         cube_coord_min(3) = coords_center( 3,i_atom )-&
                                  radius_esp_max_new(species(i_atom))
         cube_coord_max(1) = coords_center( 1,i_atom )+&
                                  radius_esp_max_new(species(i_atom))
         cube_coord_max(2) = coords_center( 2,i_atom )+&
                                  radius_esp_max_new(species(i_atom))
         cube_coord_max(3) = coords_center( 3,i_atom )+&
                                  radius_esp_max_new(species(i_atom))
       else
         cube_coord_min(1) = min(cube_coord_min(1),coords_center( 1,i_atom )-&
                                  radius_esp_max_new(species(i_atom)))
         cube_coord_min(2) = min(cube_coord_min(2),coords_center( 2,i_atom )-&
                                  radius_esp_max_new(species(i_atom)))
         cube_coord_min(3) = min(cube_coord_min(3),coords_center( 3,i_atom )-&
                                  radius_esp_max_new(species(i_atom)))
         cube_coord_max(1) = max(cube_coord_max(1),coords_center( 1,i_atom )+&
                                  radius_esp_max_new(species(i_atom)))
         cube_coord_max(2) = max(cube_coord_max(2),coords_center( 2,i_atom )+&
                                  radius_esp_max_new(species(i_atom)))
         cube_coord_max(3) = max(cube_coord_max(3),coords_center( 3,i_atom )+&
                                  radius_esp_max_new(species(i_atom)))
       endif
    enddo
  endif
  if ((n_periodic.ne.0.and.cube_rad_grid))then
   lattice_vector_new = lattice_vector
   lattice_vector_new(3,3) = maxval(coords_center( 3,1:n_atoms ))+&
                             maxval(radius_esp_max_new)
   off=minval(coords_center( 3,1:n_atoms ))-maxval(radius_esp_max_new)
   lattice_vector_new(3,3) = abs(lattice_vector_new(3,3) - off)
   if (abs(lattice_vector_new(3,3))>lattice_vector(3,3))then
     off = -0.5*lattice_vector(3,3)
     lattice_vector_new(3,3) = lattice_vector(3,3)
   endif
  endif
     if (myid.eq.0) then
       if (i_esp<=9) then
	  write(valuek1,'(I1)') i_esp
	  filename_esp = 'potential_esp_'//trim(valuek1)//'.cube'
       elseif (i_esp<=99) then
	  write(valuek2,'(I2)') i_esp
	  filename_esp = 'potential_esp_'//trim(valuek2)//'.cube'
       elseif (i_esp<=999) then
	  write(valuek3,'(I3)') i_esp
	  filename_esp = 'potential_esp_'//trim(valuek3)//'.cube'
       elseif (i_esp<=9999) then
	  write(valuek4,'(I4)') i_esp
	  filename_esp = 'potential_esp_'//trim(valuek4)//'.cube'
       elseif (i_esp<=99999) then
	  write(valuek5,'(I5)') i_esp
	  filename_esp = 'potential_esp_'//trim(valuek5)//'.cube'
       endif
       !Open file
       descriptor = 11+i_esp
       open(unit= descriptor, file=filename_esp, ACTION='WRITE')
       n_points = n_x*n_y*n_z
       !Write the header
       coords_temp = coords
       if ((output_in_original_unit_cell).and.(n_periodic>0)) then

          call frac2cart (lattice_vector,                    &
                       periodic_unit_cell_translations,   &
                       periodic_unit_cell_translations_cart)

          !coords_temp(:,:) = coords(:,:)+periodic_unit_cell_translations_cart(:,:)

          call cart2frac (lattice_vector,                       &
                       periodic_unit_cell_translations_cart, &
                       periodic_unit_cell_translations)
       end if


       write (descriptor,*) "CUBE FILE written by FHI-AIMS"
       write (descriptor,*) "*****************************"

       if(n_periodic.eq.0)then
       local_offset(1) = cube_coord_min(1)
       local_offset(2) = cube_coord_min(2)
       local_offset(3) = cube_coord_min(3)
       write (descriptor,fmt='(1X,I4,3F12.6)') &
              n_atoms, (local_offset(i_coord), i_coord=1,3,1)
       d_r(1) = (dble(1)/n_x)*(cube_coord_max(1)-cube_coord_min(1))
       d_r(2) = 0d0
       d_r(3) = 0d0
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_x, d_r(1:3)
       d_r(1) = 0d0
       d_r(2) = (dble(1)/n_y)*(cube_coord_max(2)-cube_coord_min(2))
       d_r(3) = 0d0
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_y, d_r(1:3)
       d_r(1) = 0d0
       d_r(2) = 0d0
       d_r(3) = (dble(1)/n_z)*(cube_coord_max(3)-cube_coord_min(3))
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_z, d_r(1:3)
       elseif (cube_rad_grid.and.n_periodic.ne.0) then
       local_offset(1) = (maxval(coords_temp(1,:))+minval(coords_temp(1,:)))*0.5 - sum(lattice_vector(1,:))*0.5
       local_offset(2) = (maxval(coords_temp(2,:))+minval(coords_temp(2,:)))*0.5 - sum(lattice_vector(2,:))*0.5
       local_offset(3) = (maxval(coords_temp(3,:))+minval(coords_temp(3,:)))*0.5 + off
       write (descriptor,fmt='(1X,I4,3F12.6)') &
              n_atoms, (local_offset(i_coord), i_coord=1,3,1)
       d_r(1) = dble(1)/n_x
       d_r(2) = 0d0
       d_r(3) = 0d0
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_x, matmul(lattice_vector_new,d_r(1:3))
       d_r(1) = 0d0
       d_r(2) = dble(1)/n_y
       d_r(3) = 0d0
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_y, matmul(lattice_vector_new,d_r(1:3))
       d_r(1) = 0d0
       d_r(2) = 0d0
       d_r(3) = dble(1)/n_z
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_z, matmul(lattice_vector_new,d_r(1:3))
       else
       local_offset(1) = (maxval(coords_temp(1,:))+minval(coords_temp(1,:)))*0.5 - sum(lattice_vector(1,:))*0.5
       local_offset(2) = (maxval(coords_temp(2,:))+minval(coords_temp(2,:)))*0.5 - sum(lattice_vector(2,:))*0.5
       local_offset(3) = (maxval(coords_temp(3,:))+minval(coords_temp(3,:)))*0.5 - sum(lattice_vector(3,:))*0.5
       write (descriptor,fmt='(1X,I4,3F12.6)') &
              n_atoms, (local_offset(i_coord), i_coord=1,3,1)
       d_r(1) = dble(1)/n_x
       d_r(2) = 0d0
       d_r(3) = 0d0
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_x, matmul(lattice_vector,d_r(1:3))
       d_r(1) = 0d0
       d_r(2) = dble(1)/n_y
       d_r(3) = 0d0
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_y, matmul(lattice_vector,d_r(1:3))
       d_r(1) = 0d0
       d_r(2) = 0d0
       d_r(3) = dble(1)/n_z
       write(descriptor,fmt='(1X,I4,3F12.6)') &
           n_z, matmul(lattice_vector,d_r(1:3))
       endif
       do i_atom = 1,n_atoms,1
         write (descriptor,fmt='(1X,I4,4F12.6)') &
         nint(species_z(species(i_atom))), 0.0d0, &
         (coords_temp(i_coord,i_atom),i_coord=1,3,1)
       enddo
       write(file_format,'(A)') '(1E13.5, $)'
       blocksize = 1 
       i_point = 0
	do i_x = 0, n_x-1, 1
	do i_y = 0, n_y-1, 1
	do i_z = 0, n_z-1, 1
         i_point = i_point +1
                  if (output_select.lt.6) then
                      write (descriptor,fmt=file_format) &
                           cube_output(i_point)
                  elseif (output_select.eq.6) then
                      write (descriptor,fmt='(1X,1E13.5,3F12.6)') &
                           cube_output(i_point),coord_output(i_point,:)
                  end if
                  if(mod(blocksize,6).eq.0) then
                      write(descriptor,*) ""
                      blocksize=0
                  elseif(mod(i_point,n_z).eq.0) then
                      write(descriptor,*) ""
                      blocksize=0
                  endif
                  blocksize= blocksize+1
         enddo
         enddo
         enddo
      ! Add a carriage return at the end
      write(descriptor,*) ""
      !close the file
      close(descriptor)
  endif !myid = 0
end subroutine esp_output_cubefile
!---------------------------------------------------------------------------
!****s* esp_cahrges/esp_fit_pbc_three
!  NAME
!    esp_fit
!  SYNOPSIS
subroutine esp_collect_pot(potential_std,partition_tab_std,esp_cube_output,&
                          n_x,n_y,n_z )
!  PURPOSE
!    Collect the potential into one large array
!
!  USES
  use load_balancing,only:use_batch_permutation,batch_perm,get_batch_weights
  use mpi_tasks,only:mpi_wtime
  use synchronize_mpi_basic,only:sync_vector
  use esp_grids
  use hartree_potential_recip,only:update_hartree_potential_recip
!  ARGUMENTS
!  INPUTS
!   o potential_std -- Hartree potential
!   o n_points -- Total number of points in cube file
!  OUTPUT
!   o esp_cube_output -- output array
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: potential_std
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: partition_tab_std
  real*8, target, dimension(n_x*n_y*n_z),INTENT(OUT):: esp_cube_output
  integer  :: n_x,n_y,n_z
  
  ! Timing
  real*8 time_start

  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                         ! actually used
  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)
  integer n_bp



  !  counters

  integer i_batch
  integer i_index
  integer :: i_full_points
  integer :: ind
  integer  :: coord_current(3)
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

    partition_tab => batch_perm(n_bp)%partition_tab
    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
  endif
      esp_cube_output = 0d0
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          if (partition_tab(i_full_points).gt.0.d0) then
            ! get current integration point coordinate
            coord_current(1) = batches_work(i_batch) % points_esp(i_index) % &
                               index_atom_esp
            coord_current(2) = batches_work(i_batch) % points_esp(i_index) % &
                               index_radial_esp
            coord_current(3) = batches_work(i_batch) % points_esp(i_index) % &
                               index_angular_esp
            ind = (coord_current(1))*(n_y*n_z) + (coord_current(2))*n_z + &
                   coord_current(3)+1
            esp_cube_output(ind) = potential(i_full_points)
          endif
        enddo
      enddo
      call sync_vector(esp_cube_output,n_x*n_y*n_z)
end subroutine esp_collect_pot

!****s* FHI-aims/esp_collect_pot_Hu
!  NAME
!    esp_collect_pot_Hu
!  SYNOPSIS
subroutine esp_collect_pot_Hu(potential_std,partition_tab_std,esp_cube_output,&
                          n_x,n_y,n_z,esp_cube_coord )
!  PURPOSE
!    Collect the potential into one large array and coordinates 
!
!  USES
  use load_balancing,only:use_batch_permutation,batch_perm,get_batch_weights
  use mpi_tasks,only:mpi_wtime
  use synchronize_mpi_basic,only:sync_vector,sync_matrix
  use esp_grids
  use hartree_potential_recip,only:update_hartree_potential_recip
  use geometry,only:species,coords
!  ARGUMENTS
!  INPUTS
!   o potential_std -- Hartree potential
!   o n_points -- Total number of points in cube file
!  OUTPUT
!   o esp_cube_output -- output array
!   
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
  implicit none
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: potential_std
  real*8, target, dimension(n_full_points_esp),INTENT(IN):: partition_tab_std
  real*8, target, dimension(n_x*n_y*n_z),INTENT(OUT):: esp_cube_output
  real*8, target, dimension(n_x*n_y*n_z,3),INTENT(OUT) :: esp_cube_coord
  integer  :: n_x,n_y,n_z
  
  ! Timing
  real*8 time_start

  ! Load balancing stuff

  integer n_my_batches_work  ! Number of batches actually used
  integer n_full_points_work ! Number of integration points actually used
  type (batch_of_points_esp), pointer :: batches_work(:) ! Pointer to batches 
                                                         ! actually used
  real*8, pointer :: partition_tab(:)
  real*8, pointer :: potential(:)
  integer n_bp



  !  counters

  integer i_batch
  integer i_index
  integer :: i_full_points
  integer :: ind
  integer  :: coord_current(3)
  real*8 ::	coord_cube(3)
  !-----------------------------------------------------------------------------

  ! Initialize load balancing:
  ! Set pointers either to permuted batches / arrays over integration points 
  ! (for load balancing) or to standard batches / arrays (no load balancing)

  n_bp = use_batch_permutation
  if(use_batch_permutation > 0) then

    n_my_batches_work = batch_perm(n_bp)%n_my_batches
    n_full_points_work = batch_perm(n_bp)%n_full_points

    partition_tab => batch_perm(n_bp)%partition_tab
    allocate(potential(n_full_points_work))

  else

    n_my_batches_work = n_my_batches_esp
    n_full_points_work = n_full_points_esp
    batches_work => batches_esp
    partition_tab => partition_tab_std
    potential => potential_std
  endif
      esp_cube_output = 0d0
      esp_cube_coord = 0d0
      ! Reset grid counter for current Hartree potential center
      i_full_points = 0
      do i_batch = 1, n_my_batches_work

        if(get_batch_weights) time_start = mpi_wtime()

        ! loop over one batch
        do i_index = 1, batches_work(i_batch)%size_esp, 1

          ! i_full_points is the index that indicates where we are in the 
          ! entire grid (for external quanities like rho, potential, ...)
          i_full_points = i_full_points + 1

          ! Only execute if partition_tab is .gt. zero, else
          ! we can run into 1/0 !!!
          if (partition_tab(i_full_points).gt.0.d0) then
            ! get current integration point coordinate
            coord_current(1) = batches_work(i_batch) % points_esp(i_index) % &
                               index_atom_esp
            coord_current(2) = batches_work(i_batch) % points_esp(i_index) % &
                               index_radial_esp
            coord_current(3) = batches_work(i_batch) % points_esp(i_index) % &
                               index_angular_esp
            coord_cube(:) = batches_work(i_batch) % points_esp(i_index) % &
                               coords_esp(:)
            ind = (coord_current(1))*(n_y*n_z) + (coord_current(2))*n_z + &
                   coord_current(3)+1
            esp_cube_output(ind) = potential(i_full_points)
            esp_cube_coord(ind,:) = coord_cube(:)
          endif
        enddo
      enddo
      call sync_vector(esp_cube_output,n_x*n_y*n_z)
      call sync_matrix(esp_cube_coord,n_x*n_y*n_z,3)
end subroutine esp_collect_pot_Hu

!****s* FHI-aims/evaluate_wave_hessian_cartesian_esp
!  NAME
!    evaluate_wave_hessian_hessian_esp
!  SYNOPSIS

subroutine evaluate_wave_hessian_cartesian_esp & 
     ( dist_tab, i_r, dir_tab, index_lm, l_ylm_max, n_compute, i_basis, atom_index_inv, i_basis_fns_inv, &
       radial_wave, radial_wave_deriv, radial_wave_2nd_deriv, ylm_tab, &
       sum_gradient, sum_hessian, hessian, n_atoms_compute,&
       n_max_compute_fns_dens,n_max_compute_dens)

!  PURPOSE
!     Subroutine evaluate_wave_hessian_cartesian provides the hessian of each basis
!     function at a given integration point based on the expansion of ylm-functions
!     into cartesian terms.
!
!
!  USES

  use dimensions,only:n_basis_fns,n_centers_basis_T,l_wave_max,n_centers
  use pbc_lists,only:Cbasis_to_basis,Cbasis_to_center
  use basis,only:basis_m, basis_l,basis_fn
  implicit none

!  ARGUMENTS 

  integer:: n_atoms_compute, n_max_compute_fns_dens, n_max_compute_dens

  real*8, dimension(n_atoms_compute), intent(in) :: dist_tab
  real*8, dimension(n_atoms_compute), intent(in) :: i_r
  real*8, dimension(3, n_atoms_compute), intent(in) :: dir_tab
  integer :: l_ylm_max
  integer, intent(in) :: n_compute
  integer, dimension(n_centers_basis_T), intent(in) :: i_basis
  integer, dimension(n_centers), intent(in) :: atom_index_inv
  integer, dimension(n_basis_fns, n_centers), intent(in) :: i_basis_fns_inv
  real*8, dimension(n_max_compute_fns_dens) :: radial_wave
  real*8, dimension(n_max_compute_fns_dens) :: radial_wave_deriv
  real*8, dimension(n_max_compute_fns_dens) :: radial_wave_2nd_deriv
  integer, intent(in) :: index_lm(-l_wave_max:l_wave_max, 0:l_wave_max )
  real*8, dimension((l_wave_max+1) ** 2, n_atoms_compute) :: ylm_tab
  real*8, dimension(3, (l_wave_max+1) ** 2, n_atoms_compute) :: sum_gradient
  real*8, dimension(6, (l_wave_max+1) ** 2, n_atoms_compute) :: sum_hessian

  real*8, dimension(n_max_compute_dens, 6) :: hessian


!  INPUTS
!   o n_atoms_compute -- number of relevant atoms
!   o dist_tab -- distance to atoms
!   o i_r --  Not used anymore
!   o dir_tab -- direction to relevant atoms
!   o l_ylm_max --  Not used anymore
!   o n_compute -- number of relevant basis functions
!   o i_basis -- list of relevant basis functions
!   o atom_index_inv -- inverse of atom index list
!   o i_basis_fns_inv -- inverse of basis fns list
!   o radial_wave -- radial basis functions
!   o radial_wave_deriv -- derivative of radial basis functions
!   o radial_wave_2nd_deriv -- 2nd derivative of radial basis functions
!   o index_lm -- order of l and m index pairs
!   o ylm_tab -- Y_lm functions
!   o sum_gradient -- ????????
!   o sum_hessian -- ?????????
!
!  OUTPUT
!   o hessian - ??????????
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
!

	









  ! local variables
  real*8 :: radial_wave_scaled
  real*8 :: radial_wave_deriv_scaled
  real*8 :: radial_wave_2nd_deriv_scaled
  real*8 :: prefactor_one
  real*8 :: prefactor_two
  real*8 :: prefactor_two_diag
  real*8 :: prefactor_three
  integer :: lm_index
  real*8 :: temp_inv_dist
  real*8 :: one_over_dist
  integer :: current_basis, current_basis_atom
  integer :: compute_atom
  integer :: current_fn

  ! counters

  integer :: i_atom
  integer :: i_compute
  integer :: i_coords
  integer :: i_coords_2
  integer :: i_counter

  ! begin work

  ! tabulate hessian (wave function) for each basis function
  ! assuming that cartesians and gradient_terms are already evaluated !!!
  ! -> subroutine evaluate_cartesians()
  ! -> subroutine evaluate_gradient_and_hessian_terms()
  ! (module cartesian_ylm)

  do i_compute = 1, n_compute, 1
     current_basis = i_basis(i_compute)
     compute_atom = atom_index_inv(Cbasis_to_center(current_basis))
     current_basis_atom = Cbasis_to_basis(current_basis)

     current_fn = i_basis_fns_inv(basis_fn(current_basis_atom), Cbasis_to_center(current_basis))
     lm_index = index_lm(basis_m(current_basis_atom), basis_l(current_basis_atom))


     if (current_fn .gt. 0) then

           ! first scale radial terms in the appropriate way (u/r^3, u'/r^2, u''/r)
           one_over_dist = 1. / dist_tab(compute_atom)
           radial_wave_2nd_deriv_scaled = radial_wave_2nd_deriv(current_fn) * one_over_dist
           
           temp_inv_dist = one_over_dist * one_over_dist
           radial_wave_deriv_scaled = radial_wave_deriv(current_fn) * temp_inv_dist
           
           temp_inv_dist = temp_inv_dist * one_over_dist
           radial_wave_scaled = radial_wave(current_fn) * temp_inv_dist
           
           prefactor_one   = radial_wave_2nd_deriv_scaled - (2 * basis_l(current_basis_atom) + 3) * radial_wave_deriv_scaled + &
                (basis_l(current_basis_atom) + 1) * (basis_l(current_basis_atom) + 3) * radial_wave_scaled

           prefactor_one = prefactor_one * ylm_tab(lm_index, compute_atom)

           prefactor_two   = radial_wave_deriv_scaled - (basis_l(current_basis_atom) + 1) * radial_wave_scaled

           prefactor_two_diag = prefactor_two * ylm_tab(lm_index, compute_atom)

           prefactor_three = radial_wave_scaled
           
           i_counter = 0
           do i_coords = 1, 3, 1
              ! diagonal elements
              i_counter = i_counter + 1
              hessian(i_compute, i_counter) = dir_tab(i_coords, compute_atom) * (dir_tab(i_coords, compute_atom) * &
                   prefactor_one + 2 * prefactor_two * &
                   sum_gradient(i_coords, lm_index, compute_atom)) + & 
                   prefactor_two_diag + &
                   prefactor_three * sum_hessian(i_counter, lm_index, compute_atom)
              
              ! non-diagonal elements
              do i_coords_2 = i_coords + 1, 3, 1
                 i_counter = i_counter + 1
                 hessian(i_compute, i_counter) = dir_tab(i_coords, compute_atom) * (dir_tab(i_coords_2, compute_atom) * &
                      prefactor_one + & 
                      prefactor_two * sum_gradient(i_coords_2, lm_index, compute_atom))  &
                      + prefactor_two * dir_tab(i_coords_2, compute_atom) & 
                        * sum_gradient(i_coords, lm_index, compute_atom) + &
                      prefactor_three * sum_hessian(i_counter, lm_index, compute_atom)
              end do
           end do
     else
           hessian(i_compute, :) = 0.d0
     end if
        

  end do
  
end subroutine evaluate_wave_hessian_cartesian_esp
!---------------------------------------------------------------------
!******
!****s* FHI-aims/evaluate_wave_gradient_cartesian_esp
!  NAME
!    evaluate_wave_gradient_cartesian_esp
!  SYNOPSIS

subroutine evaluate_wave_gradient_cartesian_esp &
     (dist_tab, i_r, dir_tab, index_lm, l_ylm_max, n_compute, i_basis, atom_index_inv, i_basis_fns_inv, &
     radial_wave, radial_wave_deriv, ylm_tab, sum_gradient, gradient, n_compute_atoms, &
     n_max_compute_fns_dens,n_max_compute_dens)

!  PURPOSE
!     Subroutine evaluate_wave_gradient_cartesian provides the gradient of each basis
!     function for a given integration point based on the expansion of ylm-functions
!     into cartesian terms
!     
!     The subroutine assumes that cartesians and gradient_terms are already evaluated:
!     * subroutine evaluate_cartesians()
!     * subroutine evaluate_gradient_terms() or subroutine evaluate_gradient_and_hessian_terms()
!     (module cartesian_ylm)
!
!  USES

  use dimensions,only:n_basis_fns,n_centers_basis_T,l_wave_max,n_centers
  use pbc_lists,only:Cbasis_to_basis,Cbasis_to_center
  use basis,only:basis_m, basis_l,basis_fn
  implicit none


!  ARGUMENTS


  integer, intent(in) :: n_compute, n_compute_atoms, n_max_compute_fns_dens,&
                         n_max_compute_dens
  real*8, dimension(n_compute_atoms), intent(in) :: dist_tab
  real*8, dimension(n_compute_atoms), intent(in) :: i_r
  real*8, dimension(3, n_compute_atoms), intent(in) :: dir_tab
  integer :: l_ylm_max

  integer, dimension(n_centers_basis_T), intent(in) :: i_basis
  integer, dimension(n_centers), intent(in) :: atom_index_inv
  integer, dimension(n_basis_fns, n_centers), intent(in) :: i_basis_fns_inv
  real*8, dimension(n_max_compute_fns_dens) :: radial_wave
  real*8, dimension(n_max_compute_fns_dens) :: radial_wave_deriv

  integer, intent(in) :: index_lm(-l_wave_max:l_wave_max, 0:l_wave_max )
  real*8, dimension((l_wave_max+1) ** 2, n_compute_atoms) :: ylm_tab
  real*8, dimension(3, (l_wave_max+1) ** 2, n_compute_atoms) :: sum_gradient

  real*8, dimension(n_max_compute_dens, 3) :: gradient

!  INPUTS
!  o n_compute -- number of relevant basis functions
!  o n_compute_atoms -- number of relevant atoms
!  o dist_tab -- distance to relevant atoms
!  o i_r -- the distance from current integration point to all atoms
!         in units of the logarithmic grid, i(r)
!  o dir_tab -- direction to relevant atoms
!  o l_ylm_max -- maximum l component
!  o i_basis  -- non-zero  basis functions
!  o atom_index_inv -- inverse citing list of relevant atoms
!  o i_basis_fns_inv -- inverse citing list of radial functions
!  o radial_wave -- radial basis functions
!  o radial_wave_deriv -- derivative of radial basis functions
!  o index_lm -- order of l and m indexis
!  o ylm_tab -- Y_lm functions
!  o sum_gradient -- ????????
!
!  OUTPUT
!  o gradient --  gradient of each basis function for a given integration point
!
!  AUTHOR
!    FHI-aims team, Fritz-Haber Institute of the Max-Planck-Society
!  SEE ALSO
!    Volker Blum, Ralf Gehrke, Felix Hanke, Paula Havu, Ville Havu,
!    Xinguo Ren, Karsten Reuter, and Matthias Scheffler,
!    "Ab initio simulations with Numeric Atom-Centered Orbitals: FHI-aims",
!    Computer Physics Communications (2008), submitted.
!  COPYRIGHT
!   Max-Planck-Gesellschaft zur Foerderung der Wissenschaften
!   e.V. Please note that any use of the "FHI-aims-Software" is subject to
!   the terms and conditions of the respective license agreement."
!  HISTORY
!    Release version, FHI-aims (2008).
!  SOURCE
!

	



  ! local variables
  real*8 :: radial_wave_scaled
  real*8 :: radial_wave_deriv_scaled
  real*8 :: prefactor_one
  real*8 :: prefactor_two
  integer :: lm_index
  real*8 :: one_over_dist
  integer :: current_basis, current_basis_atom
  integer :: compute_atom
  integer :: current_fn

  ! counters

  integer :: i_atom
  integer :: i_compute
  integer :: i_coords

  ! begin work





  do i_compute = 1, n_compute, 1
     current_basis = i_basis(i_compute)
     current_basis_atom = Cbasis_to_basis(current_basis)

     compute_atom = atom_index_inv(Cbasis_to_center(current_basis))

     current_fn = i_basis_fns_inv(basis_fn(Cbasis_to_basis(current_basis)), &
          Cbasis_to_center(current_basis))

!   current_fn = i_basis_fns_inv(basis_fn(current_basis_atom), Cbasis_to_atom(current_basis))
     lm_index = index_lm(basis_m(current_basis_atom), basis_l(current_basis_atom))



     if (current_fn .gt. 0 .and. compute_atom > 0) then

        if (dist_tab(compute_atom) .gt. 0.d0) then
           
           ! first scale radial terms in the appropriate way (u/r^2, u'/r)
           one_over_dist = 1. / dist_tab(compute_atom)
           radial_wave_deriv_scaled = radial_wave_deriv(current_fn) * one_over_dist
           radial_wave_scaled = radial_wave(current_fn) * one_over_dist * one_over_dist
           
           prefactor_one = radial_wave_deriv_scaled - (basis_l(current_basis_atom) + 1) * radial_wave_scaled
           prefactor_two = radial_wave_scaled
           do i_coords = 1, 3, 1
              gradient(i_compute, i_coords) = dir_tab(i_coords, compute_atom) * &
                   prefactor_one * ylm_tab(lm_index, compute_atom) + &
                   prefactor_two * sum_gradient(i_coords, lm_index, compute_atom)
           end do
           
        else
           gradient(i_compute, :) = 0.d0
        end if
        
     else
        ! wave function is zero anyway; set gradient to zero explicitly to be sure.
        gradient(i_compute, :) = 0.d0
     end if
     

  end do
end subroutine evaluate_wave_gradient_cartesian_esp

!---------------------------------------------------------------------
!******	

!debug subroutine print_matrix( desc, M, N, A, LDA )
!debug       character(*)   ::    desc
!debug       integer        ::    M, N, LDA
!debug       real*8         ::    A( LDA, * )
!debug 
!debug       integer        ::    i, j

!debug       write(use_unit,*)
!debug       write(use_unit,*) desc
!debug       do i = 1, M
!debug          write(use_unit,9998) ( A( i, j ), j = 1, N )
!debug       enddo

!debug  9998 FORMAT( 11(:,1X,F15.8) )
!debug       return
!debug end subroutine

!debug subroutine print_int_vector( desc, N, A )
!debug       character(*)   ::    desc
!debug       integer        ::    N
!debug       integer        ::    A( N )

!debug       integer        ::    i

!debug       write(use_unit,*)
!debug       write(use_unit,*) DESC
!debug       write(use_unit,9999) ( A( i ), i = 1, N )

!debug  9999 FORMAT( 11(:,1X,I6) )
!debug       return
!debug end subroutine
endmodule esp_charges
